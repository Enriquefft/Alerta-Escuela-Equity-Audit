\documentclass[acmsmall,nonacm]{acmart}

\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}

\sisetup{
  round-mode=places,
  round-precision=3,
  table-format=1.3
}

% Arrow macros for direction column
\newcommand{\up}{$\uparrow$}
\newcommand{\down}{$\downarrow$}

\title{Equity Audit of Peru's Alerta Escuela Early Warning System: Who Gets Missed?}

\author{Enrique Francisco Flores Teniente}
\affiliation{%
  \institution{Universidad de Ingeniería y Tecnología (UTEC)}
  \city{Lima}
  \country{Peru}
}
\affiliation{%
  \institution{Genera}
  \city{Lima}
  \country{Peru}
}
\email{enrique.flores@utec.edu.pe}

\begin{abstract}
Peru's Alerta Escuela early warning system aims to identify students at risk of dropping out, yet its algorithmic fairness properties remain unexamined. Using six years of nationally representative ENAHO survey data (2018--2023, $N=150{,}135$), we replicate an Alerta Escuela--style dropout prediction model and conduct a comprehensive equity audit across language, geography, poverty, and sex dimensions. Our LightGBM model achieves a calibrated test PR-AUC of 0.236, with a false negative rate (FNR) of 63\% for Spanish-speaking students but only 22\% for indigenous-language speakers---revealing that the system over-flags indigenous students (surveillance bias) while missing the majority of Spanish-speaking dropouts (invisibility bias). SHAP analysis shows the model predicts through spatial-structural features (age, nightlights, literacy rates, poverty) rather than identity features directly. The starkest disparity emerges at the intersection of language and urbanicity: urban indigenous students face a 75\% FNR, making them the most invisible group. These findings demonstrate that dropout prediction systems can encode systematic inequities even without explicitly discriminatory intent.
\end{abstract}

\keywords{educational equity, dropout prediction, algorithmic fairness, Peru, ENAHO, early warning system}

\begin{document}
\maketitle

% =============================================================================
\section{Introduction}
% =============================================================================

Educational early warning systems (EWS) are proliferating across Latin America as governments seek data-driven approaches to reduce school dropout. Peru's Alerta Escuela, operated by the Ministry of Education (MINEDU), flags students at risk of leaving school using administrative data from the SIAGIE system \cite{minedu2023alerta}. Such systems promise efficiency and early intervention, but their algorithmic fairness properties remain almost entirely unaudited. A growing body of research has demonstrated that predictive models can systematically disadvantage marginalized groups---encoding structural inequities into automated decisions that affect millions of students \cite{barocas2016big,mitchell2021algorithmic}.

Despite the expanding algorithmic fairness literature, few studies audit deployed educational prediction systems in developing countries. Most fairness work focuses on US and European contexts, examines race and gender as primary dimensions, and does not incorporate survey weights or intersectional analysis \cite{kizilcec2020algorithmic,baker2022algorithmic,gardner2024debiasing}. This gap is particularly consequential in countries like Peru, where the axes of disadvantage---mother tongue, geography, poverty---differ fundamentally from those studied in the Global North.

Peru is a multilingual country where approximately 16\% of the population speaks an indigenous language as their mother tongue. Indigenous-language speakers face persistent educational inequities rooted in colonial legacies, geographic isolation, and inadequate bilingual education---only 37\% of indigenous students attend schools with bilingual instruction \cite{cueto2009explaining,cueto2016education}. Peru's Encuesta Nacional de Hogares (ENAHO), conducted annually by the Instituto Nacional de Estad\'{i}stica e Inform\'{a}tica (INEI), provides nationally representative data that enables analysis of these disparities \cite{inei2023enaho}. Because the actual SIAGIE administrative data used by Alerta Escuela is not publicly accessible, we construct a proxy replication of an Alerta Escuela--style dropout prediction model using ENAHO survey data spanning 150,135 student-year observations across six years (2018--2023).

This paper addresses three research questions:
\begin{enumerate}
  \item \textbf{RQ1:} What disparities exist in dropout prediction accuracy across demographic groups defined by language, geography, poverty, and sex?
  \item \textbf{RQ2:} How does the model encode these disparities---through identity features directly or through structural proxies?
  \item \textbf{RQ3:} How do intersections of demographic dimensions (e.g., language $\times$ geography) amplify prediction errors beyond what single-axis analysis reveals?
\end{enumerate}

To answer these questions, we train a LightGBM model with Platt calibration \cite{ke2017lightgbm,platt1999probabilistic}, evaluate fairness across seven demographic dimensions and three intersections using the fairlearn framework \cite{bird2020fairlearn}, and apply SHAP TreeExplainer to decompose predictions into feature-level contributions \cite{lundberg2017unified}. We use a temporal train/validation/test split (2018--2021/2022/2023) that mirrors real-world deployment, and incorporate ENAHO survey weights (FACTOR07) throughout all metrics to ensure nationally representative estimates.

Our analysis reveals a surveillance--invisibility axis across language groups: the model achieves a false negative rate (FNR) of only 0.22 for indigenous-language speakers but with a false positive rate (FPR) of 0.52---over-flagging that constitutes surveillance bias. Conversely, Spanish-speaking students face an FNR of 0.63 with FPR of only 0.18, meaning the system misses the majority of their dropouts---invisibility bias. The starkest disparity emerges at the intersection of language and urbanicity: urban indigenous students face an FNR of 0.753, meaning the model misses three out of four of their dropouts. SHAP analysis shows the model predicts through spatial-structural features (age, nightlight intensity, literacy rates, poverty index) rather than identity features directly, and algorithm-independence is confirmed across LightGBM and XGBoost (PR-AUC ratio of 1.0006).

Our contributions are:
\begin{itemize}
  \item One of few comprehensive fairness audits of an educational EWS in a developing country, spanning seven demographic dimensions and three intersections with survey-weighted metrics throughout.
  \item A demonstration that intersectional analysis reveals disparities hidden by single-axis evaluation---urban indigenous students are invisible in both language-only and geography-only analyses but emerge as the most systematically missed group when dimensions are crossed \cite{crenshaw1989demarginalizing,buolamwini2018gender}.
  \item Evidence that dropout prediction models encode structural inequities through spatial-structural proxy features rather than through explicit use of protected attributes, with implications for fairness interventions.
  \item An open-source, replicable audit framework---code, data pipeline, and analysis are publicly available to enable similar audits of educational EWS in other contexts.
\end{itemize}

% =============================================================================
\section{Related Work}
% =============================================================================

\subsection{Early Warning Systems in Education}

Dropout early warning systems have evolved substantially over the past two decades. Bowers~\cite{bowers2010grades} established foundational indicators---grades, GPA, and course failures---as predictors of dropout risk in a longitudinal study of US high school students. As machine learning methods matured, researchers developed increasingly sophisticated systems: Lakkaraju et al.~\cite{lakkaraju2015machine} introduced an ML framework for identifying at-risk K-12 students, demonstrating that ensemble methods could substantially outperform traditional indicator thresholds. Knowles~\cite{knowles2015needles} deployed the first statewide ML-based dropout EWS in Wisconsin, covering over 225,000 students with administrative data---a scale comparable to Peru's Alerta Escuela.

Subsequent work has expanded both the methods and the contexts. Baker et al.~\cite{baker2020predicting} applied logistic regression to attendance, grades, and disciplinary data in diverse US school districts, while Lee and Chung~\cite{lee2019machine} established the pattern of temporal train-test splits that mirrors real-world deployment---a design we adopt. In the developing-country context most relevant to our work, Adelman et al.~\cite{adelman2018predicting} used administrative data to predict dropout in Guatemala and Honduras, correctly identifying 80\% of eventual dropouts.

Two recent contributions provide critical framing for our study. McMahon et al.~\cite{mcmahon2020reenvisioning} argue that EWS should shift from pure identification to meaningful prediction-plus-intervention, questioning whether flagging students as ``at-risk'' without adequate support mechanisms constitutes a net benefit. Most provocatively, Perdomo et al.~\cite{perdomo2023difficult} evaluate Wisconsin's deployed EWS over a decade and argue that structural features predict dropout as well as individual risk scores---a finding our SHAP analysis directly corroborates. Our paper extends this critical tradition by auditing an EWS-style model in a context where deployment occurs but fairness evaluation does not.

\subsection{Algorithmic Fairness in Education}

Kizilcec and Lee~\cite{kizilcec2020algorithmic} identified a critical gap in their framework paper on algorithmic fairness in education: while fairness audits were becoming standard in criminal justice and hiring, they remained rare in deployed educational systems. Subsequent work has begun to address this gap, but important blind spots persist.

Baker and Hawn~\cite{baker2022algorithmic} provided the most comprehensive review of algorithmic bias in education to date, cataloguing known biases across applications from automated essay scoring to dropout prediction and introducing ``slice analysis'' as a methodology for disaggregated fairness evaluation. Mehrabi et al.~\cite{mehrabi2021survey} offered a broader taxonomy of fairness definitions and bias sources in ML, situating educational applications within the larger landscape of fairness criteria---including the impossibility results of Chouldechova~\cite{chouldechova2017fair} that prove no classifier can simultaneously satisfy calibration, equal FNR, and equal FPR across groups with different base rates.

Applied studies have examined specific educational domains. Loukina, Madnani, and Zechner~\cite{loukina2019many} demonstrated that different fairness definitions require different solutions in automated essay scoring, showing that total fairness may be unachievable---a finding that parallels the FNR-FPR trade-off we document. Pan and Zhang~\cite{pan2024examining} examined algorithmic fairness specifically in high school dropout prediction using HSLS:09 data, finding model-specific biases across threshold ranges; however, their analysis was limited to a US context without survey weights or intersectional analysis. Karimi-Haghighi et al.~\cite{karimihag2021predicting} combined calibration and fairness evaluation in dropout prediction, evaluating across nationality, gender, and school type, but without the survey-weighted methodology our ENAHO analysis requires.

On the intervention side, Saleiro et al.~\cite{saleiro2018aequitas} developed the Aequitas bias audit toolkit that operationalizes fairness evaluation, and Gardner, Brooks, and Baker~\cite{gardner2024debiasing} systematically reviewed debiasing strategies in educational ML, finding that most studies focus on gender and race in US and European contexts. Our paper fills the gap identified by Kizilcec and Lee: a comprehensive fairness audit in a developing-country, multilingual context using survey-weighted analysis across seven dimensions and three intersections.

\subsection{Fairness in Latin American Educational AI}

Latin American education systems face structural inequalities rooted in colonial histories, geographic barriers, and linguistic diversity \cite{unesco2022lac}. In Peru specifically, indigenous-language speakers experience persistent educational disadvantage. Cueto et al.~\cite{cueto2009explaining} documented how ethnic and language minorities in Peru are systematically marginalized in education, finding that indigenous-language students receive lower-quality instruction and face cultural barriers to school engagement. Cueto, Miranda, and Le\'{o}n~\cite{cueto2016education} traced education trajectories from early childhood through adolescence using Young Lives longitudinal data, reporting that only 37\% of indigenous students attend bilingual schools despite legal mandates for intercultural bilingual education.

Machine learning is increasingly applied to dropout prediction in the region. Adelman et al.~\cite{adelman2018predicting} demonstrated effective dropout prediction in Guatemala and Honduras using administrative data, and Villegas-Ch et al.~\cite{villegas2023supporting} evaluated ML approaches for higher education dropout in a Latin American context, finding that socioeconomic variables dominate prediction. Zawacki-Richter et al.~\cite{zawacki2022artificial} surveyed AI applications in Latin American higher education, identifying growing adoption but limited resources and unequal socioeconomic contexts as persistent challenges. Notably, none of these studies conducted fairness audits of their prediction systems. Our paper provides the first such audit for a Peruvian educational prediction system, examining whether the demographic disparities documented by Cueto and colleagues are reproduced---or amplified---by algorithmic prediction.

\subsection{Intersectionality in ML Fairness}

Crenshaw~\cite{crenshaw1989demarginalizing} established the foundational insight that single-axis analysis of disadvantage systematically misses the experiences of those who face compound marginalization. Originally articulated in the context of Black women's experiences under US antidiscrimination law, the intersectionality framework has proven broadly applicable to algorithmic systems.

Buolamwini and Gebru~\cite{buolamwini2018gender} demonstrated this insight computationally in their landmark Gender Shades study: commercial facial recognition systems showed error rates of 34.7\% for darker-skinned females compared to 0.8\% for lighter-skinned males---a disparity invisible in either gender-only or skin-type-only analysis. This finding catalyzed a body of work formalizing intersectional fairness in ML. Kearns et al.~\cite{kearns2018preventing} proved that a classifier can appear fair on individual protected groups while violating fairness on their intersections, formalizing the concept of subgroup fairness and demonstrating that auditing individual dimensions is provably insufficient. H\'{e}bert-Johnson et al.~\cite{hebertjohnson2018multicalibration} extended this reasoning to calibration, introducing multicalibration---the requirement that predicted probabilities be well-calibrated not just overall but for all computationally identifiable subgroups.

Our intersectional analysis directly operationalizes these frameworks in an educational context where no prior intersectional audit exists. The language $\times$ geography intersection that reveals urban indigenous students' FNR of 0.753 parallels the Gender Shades finding: a group invisible to single-axis analysis that faces the most extreme prediction errors. Where Buolamwini and Gebru examined race $\times$ gender in facial recognition, we examine language $\times$ urbanicity in dropout prediction---demonstrating that the intersectionality imperative extends to educational EWS in developing countries.

% =============================================================================
\section{Data}
% =============================================================================

Peru has approximately 8 million school-age children, and dropout remains a persistent challenge---particularly in rural areas and among indigenous-language communities. The Ministry of Education (MINEDU) operates Alerta Escuela, which uses data from the Sistema de Informaci\'{o}n de Apoyo a la Gesti\'{o}n de la Instituci\'{o}n Educativa (SIAGIE) to flag students at risk of dropout \cite{minedu2023alerta,minedu2022estadistica}. Because SIAGIE administrative records are not publicly accessible, we use ENAHO survey data as a proxy to construct and audit an Alerta Escuela--style prediction model.

We use Peru's Encuesta Nacional de Hogares (ENAHO), a nationally representative household survey conducted annually by the Instituto Nacional de Estad\'{i}stica e Inform\'{a}tica (INEI) \cite{inei2023enaho}. ENAHO employs a multi-stage, stratified sampling design covering all 25 departments of Peru, with survey weights (FACTOR07) that account for the complex sampling structure and enable nationally representative inference. We extract data from Module 200 (demographic characteristics) and Module 300 (education), joining them by household and person identifiers.

Our analysis pools six annual waves (2018--2023) covering school-age children aged 6--17, yielding 150,135 individual-year observations after data cleaning. The 2020 wave is notably affected by the COVID-19 pandemic: INEI conducted phone interviews rather than in-person visits, resulting in a reduced sample of approximately 13,755 observations (compared to approximately 25,000 in a typical year) and 52\% null values in the education attendance variable (P303), which were dropped. We define dropout as a binary outcome: a child of school age who was enrolled in the previous academic year but is not currently attending, following MINEDU's operational definition.

\begin{table}[htbp]
  \caption{Sample Description by Demographic Dimensions}
  \label{tab:sample}
  \input{tables/table_01_sample.tex}
\end{table}

Table~\ref{tab:sample} summarizes the sample across key demographic dimensions. The sample is predominantly Spanish-speaking (approximately 84\%), with indigenous-language speakers comprising Quechua, Aymara, and other indigenous groups. Urban residents constitute approximately 65\% of observations. The sample spans all three major geographic regions: Costa (coast), Sierra (highlands), and Selva (Amazon lowlands).

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Language Group}
  \label{tab:language}
  \input{tables/table_02_language.tex}
\end{table}

Table~\ref{tab:language} reveals substantial disparities in weighted dropout rates across language groups. Indigenous-language speakers face rates 34\% higher than Spanish speakers on average. The Awajun group exhibits the highest dropout rate at 0.2047, compared to 0.1526 for Castellano speakers---a gap that persists even after accounting for geographic and socioeconomic differences.

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Region and Poverty Quintile}
  \label{tab:region_poverty}
  \input{tables/table_03_region_poverty.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig04_dropout_heatmap.pdf}
  \caption{Dropout rates by language group and rurality. Each cell shows the weighted dropout rate for the intersection of language and urban/rural geography.}
  \label{fig:dropout_heatmap}
\end{figure}

Table~\ref{tab:region_poverty} and Figure~\ref{fig:dropout_heatmap} show how dropout rates vary by region, poverty level, and their interactions with language. The Sierra and Selva regions exhibit higher dropout rates than the Costa, and a monotonic poverty gradient is visible: the poorest quintile has substantially higher dropout rates than the wealthiest. Figure~\ref{fig:dropout_heatmap} further reveals that the interaction of language and rurality produces disparities that exceed what either dimension alone would suggest.

In addition to individual and household variables from ENAHO, we merge district-level spatial features to capture contextual effects. Census data provides district population and literacy rate z-scores. Nightlight intensity, measured by satellite remote sensing, serves as a proxy for local economic activity and infrastructure. Administrative records from MINEDU provide district-level primaria (primary) and secundaria (secondary) completion rates. Merge rates are high: 100\% for administrative and census data, and 95.9\% for nightlight data, with 44 districts (1.53\%) having primaria but no secundaria administrative records.

% =============================================================================
\section{Methods}
% =============================================================================

\subsection{Feature Engineering}

We engineer 25 features organized into three categories. \emph{Individual demographics} (8 features) include age, sex, nationality (Peruvian/foreign), and mother tongue encoded as binary indicators for Quechua, Aymara, other indigenous languages, and foreign languages, with Castellano as the reference category. \emph{Household characteristics} (8 features) include parent education (maximum years of schooling between household head and spouse, mapped from ENAHO's P301A categorical variable), poverty index z-score, poverty quintile (constructed using FACTOR07-weighted quantiles to ensure each quintile represents exactly 20\% of the population), working status, household size, and a binary indicator for whether the child's birthplace matches their current residence. Note that ENAHO's birthplace question (P209) is asked only of respondents aged 12 and above; children aged 6--11 are assigned the default of Peruvian-born. \emph{District-level spatial indicators} (9 features) include nightlight intensity z-score (a satellite-derived proxy for local economic activity), census population and literacy rate z-scores, administrative primaria and secundaria completion rates, and the district's historical dropout rate computed from ENAHO itself. Nightlight z-score nulls (4.1\% of observations) are imputed with 0.0, representing the distribution mean \cite{macnell2023implementing}.

\subsection{Model Selection and Training}

We compare three model families chosen for complementary purposes. \emph{Logistic regression} provides interpretable coefficients and odds ratios; we fit both a scikit-learn implementation for prediction and a statsmodels GLM with Binomial family for statistical inference (standard errors, p-values, confidence intervals). \emph{LightGBM} \cite{ke2017lightgbm} serves as the primary predictive model, leveraging gradient boosting's ability to capture nonlinearities and feature interactions without explicit specification. \emph{XGBoost} \cite{chen2016xgboost} serves as an algorithm-independence check: if fairness findings hold across both gradient boosting implementations, they reflect data structure rather than algorithmic artifacts.

Models are trained on 2018--2021 data ($n=98{,}023$), validated on 2022 ($n=26{,}477$), and tested on 2023 ($n=25{,}635$). This temporal split mirrors real-world deployment, where models trained on historical data must predict future cohorts. LightGBM hyperparameters are tuned via Optuna \cite{akiba2019optuna} with 100 trials, using early stopping on validation average precision (PR-AUC). All models incorporate ENAHO survey weights (FACTOR07) during training and evaluation to ensure nationally representative estimates.

\subsection{Calibration}

Gradient boosted trees produce probability estimates that are often poorly calibrated, particularly when class weighting is applied to handle imbalanced outcomes \cite{niculescumizil2005predicting}. We apply Platt scaling \cite{platt1999probabilistic} to the LightGBM model's raw probability outputs, fitting a sigmoid function on the validation set. This reduces the Brier score from 0.212 to 0.116---a 45\% improvement---confirming that calibration is critical for models with scale\_pos\_weight adjustments. The Platt scaling parameters ($A=-6.236$, $B=4.443$) compress the raw probability range, with calibrated probabilities reaching a maximum of approximately 0.43.

\subsection{Fairness Evaluation Framework}

Fairness evaluation uses the fairlearn framework \cite{bird2020fairlearn} to compute disaggregated metrics across seven demographic dimensions (language, natural region, rurality, poverty quintile, sex, nationality, age group) and three intersections (language $\times$ rurality, language $\times$ poverty quintile, language $\times$ region). For each subgroup, we compute four metrics: false negative rate (FNR, the proportion of actual dropouts missed by the model), false positive rate (FPR, the proportion of non-dropouts incorrectly flagged), precision, and PR-AUC. All metrics are computed with survey weights. This framework operationalizes the ``slice analysis'' approach advocated by Baker and Hawn \cite{baker2022algorithmic} and aligns with the audit methodology of Saleiro et al. \cite{saleiro2018aequitas}.

The choice of FNR as a primary fairness metric reflects its direct operational interpretation: a high FNR means the system fails to identify students who will drop out. From an equity perspective, FNR disparities indicate which populations are systematically rendered invisible to the early warning system. We complement FNR with FPR to capture the surveillance--invisibility trade-off that Chouldechova's \cite{chouldechova2017fair} impossibility theorem predicts will arise when base rates differ across groups.

SHAP TreeExplainer \cite{lundberg2020local} provides feature-level interpretability, decomposing each prediction into additive feature contributions. We compute SHAP values on the raw (uncalibrated) LightGBM model, as TreeExplainer requires direct access to the tree structure. SHAP interaction values are computed on a 1,000-row subsample of the test set.

% =============================================================================
\section{Results}
% =============================================================================

\begin{table}[htbp]
  \caption{Model Performance Comparison (Survey-Weighted Metrics)}
  \label{tab:models}
  \input{tables/table_04_models.tex}
\end{table}

Table~\ref{tab:models} compares the three model families on the 2022 validation set. LightGBM and XGBoost achieve near-identical validation PR-AUC (0.262 vs. 0.263), yielding an algorithm-independence ratio of 1.0006. This near-perfect agreement is significant for our fairness analysis: it means that any disparities we identify reflect the structure of the data and prediction task, not artifacts of a particular gradient boosting implementation. The calibrated LightGBM model achieves a test PR-AUC of 0.236, with a validation-test gap of 0.023 (well within the 0.07 threshold that would indicate concerning generalization failure). The calibrated Brier score improves from 0.212 to 0.116---a reduction that confirms calibration is essential for interpreting predicted probabilities as actual dropout risks.

\begin{table}[htbp]
  \caption{Logistic Regression Coefficients (All 25 Features)}
  \label{tab:lr_coefficients}
  \input{tables/table_05_lr_coefficients.tex}
\end{table}

Table~\ref{tab:lr_coefficients} shows the logistic regression coefficients for all 25 features. Indigenous language variables dominate the linear model: the ``other indigenous'' group has the highest odds ratio (2.20), followed by Quechua and Aymara. This pattern---where identity features rank highest in a linear model---contrasts sharply with the SHAP analysis of the tree-based models presented in Section~6, where spatial-structural features dominate. The paradigm difference between linear and tree-based models (zero overlap in top-5 features) is itself a finding: it demonstrates that the ``importance'' of identity features depends entirely on the model family, and that tree-based models can achieve similar or better predictive accuracy by routing through correlated spatial-structural features instead.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig01_pr_curves.pdf}
  \caption{Precision-Recall curves for all three model families on the 2022 validation set. LightGBM and XGBoost curves largely overlap, confirming algorithm independence.}
  \label{fig:pr_curves}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig02_calibration.pdf}
  \caption{Calibration plot comparing uncalibrated and Platt-calibrated LightGBM probabilities. Platt scaling reduces Brier score by 38\%.}
  \label{fig:calibration}
\end{figure}

Figure~\ref{fig:pr_curves} visually confirms the algorithm independence: LightGBM and XGBoost PR curves are nearly superimposed across the full recall range, while logistic regression shows lower precision at equivalent recall levels. Figure~\ref{fig:calibration} demonstrates the calibration improvement. The uncalibrated model systematically overestimates risk for the highest-probability predictions---a known consequence of the scale\_pos\_weight parameter used to handle class imbalance. Platt scaling corrects this distortion, producing probabilities that more closely match observed dropout rates. This calibration step is particularly important for fairness: uncalibrated probabilities systematically overestimate risk for groups with high base rates (indigenous-language speakers), and calibration helps ensure that a predicted probability of, say, 0.3 corresponds to an approximately 30\% actual dropout rate regardless of group membership.

% =============================================================================
\section{Fairness Analysis}
% =============================================================================

\begin{table}[htbp]
  \caption{Fairness Metrics by Language Group}
  \label{tab:fairness_language}
  \input{tables/table_06_fairness_language.tex}
\end{table}

\subsection{Language Dimension: The Surveillance--Invisibility Axis}

Table~\ref{tab:fairness_language} reveals a fundamental FNR--FPR trade-off across language groups. The model achieves low FNR for indigenous-language speakers (0.22 for other indigenous languages) but at the cost of high FPR (0.52)---a pattern we term ``surveillance bias,'' where the system correctly identifies most indigenous-language dropouts but also incorrectly flags many non-dropouts. Conversely, Spanish speakers face high FNR (0.63) with low FPR (0.18)---``invisibility bias'' where the majority of actual dropouts are missed by the system.

This inverse FNR-FPR relationship is not a model bug but the mathematical consequence of Chouldechova's~\cite{chouldechova2017fair} impossibility result applied to groups with different base rates. Indigenous-language speakers have higher baseline dropout rates, so a model trained to minimize overall prediction error will flag them more aggressively. The result is a systematic redistribution of prediction errors: indigenous communities bear the burden of false alarms (surveillance) while Spanish-speaking dropouts bear the burden of being missed (invisibility).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig03_fnr_fpr_language.pdf}
  \caption{FNR and FPR by language group. The inverse relationship between FNR and FPR reveals the surveillance--invisibility trade-off.}
  \label{fig:fnr_fpr}
\end{figure}

Figure~\ref{fig:fnr_fpr} visualizes this trade-off. The inverse relationship between FNR and FPR across language groups forms a clear axis: as FNR decreases (better detection), FPR increases (more false alarms), with indigenous-language groups clustered at the high-detection/high-surveillance end and Spanish speakers at the low-detection/low-surveillance end.

\subsection{Other Demographic Dimensions}

The fairness analysis extends beyond language to six additional dimensions. \emph{Region:} The Selva (Amazon) and Sierra (highlands) regions show lower FNR than the Costa (coast)---the model detects rural and remote dropouts more effectively, likely because these students match the spatial profile most strongly associated with dropout risk. However, a calibration gap exists: students predicted as ``high risk'' in the Selva have a 28.1\% actual dropout rate, compared to 38.9\% in the Sierra, meaning the same risk score carries different meaning across regions.

\emph{Poverty:} A monotonic relationship emerges across poverty quintiles---students in poorer quintiles are flagged more frequently and have higher base dropout rates. This alignment between base rates and flagging rates means poverty-based disparities are partially expected, though the magnitude of the FNR gap between the poorest and wealthiest quintiles warrants attention.

\emph{Sex:} The gender gap is minimal, with an FNR difference of only 0.026 between male and female students. Sex is not a major axis of disparity in this model, consistent with the relatively small gender gap in Peruvian school enrollment at the primary and secondary levels.

\emph{Nationality:} With only 27 non-Peruvian students in the test set, this dimension is unusable for reliable fairness inference. We report it for completeness but note that any metrics computed on such a small sample are unreliable.

\emph{Age:} Older students (ages 15--17) are flagged more accurately than younger students (ages 6--11), reflecting both higher base dropout rates among older students and the model's heavy reliance on age as a predictive feature.

\subsection{Intersectional Analysis}

\begin{table}[htbp]
  \caption{Intersection Analysis: Language $\times$ Rurality}
  \label{tab:intersection}
  \input{tables/table_07_intersection.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig05_fnr_heatmap.pdf}
  \caption{FNR heatmap by language and rurality intersection. The darkest cell (other indigenous, urban) represents the group most missed by the model.}
  \label{fig:fnr_heatmap}
\end{figure}

Table~\ref{tab:intersection} and Figure~\ref{fig:fnr_heatmap} present the paper's starkest finding. Urban indigenous students face an FNR of 0.753---the model misses three out of four of their dropouts. This intersection group is invisible in both language-only analysis (where other-indigenous FNR is 0.22, driven by rural indigenous students) and geography-only analysis (where urban FNR is moderate). Only by crossing language and urbanicity does this extreme disparity emerge, directly demonstrating the intersectionality imperative articulated by Crenshaw~\cite{crenshaw1989demarginalizing} and operationalized computationally by Buolamwini and Gebru~\cite{buolamwini2018gender}.

The mechanism behind this disparity is interpretable: the model predicts dropout primarily through spatial-structural features---nightlight intensity, district historical dropout rates, census literacy rates---that code indigenous communities as rural. Urban indigenous students ``break the spatial profile'': they live in urban areas with higher nightlight intensity and lower district-level dropout rates, but face the same educational barriers (language, cultural mismatch, discrimination) as their rural counterparts. The model has no pathway to identify them because the features that capture indigenous disadvantage in rural settings do not activate in urban ones. We note the sample caveat: $n=89$ for urban other-indigenous students in the test set, which is sufficient for estimating proportions but should be interpreted with appropriate caution.

\subsection{SHAP Interpretability}

\begin{table}[htbp]
  \caption{SHAP Feature Importance (Top 15)}
  \label{tab:shap}
  \input{tables/table_08_shap.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig06_shap_bar.pdf}
  \caption{Mean absolute SHAP values for the top 15 features. Age and spatial-structural features dominate, while identity features (language, sex) have minimal direct importance.}
  \label{fig:shap_bar}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig07_shap_beeswarm.pdf}
  \caption{SHAP beeswarm plot showing feature value distributions and their impact on predictions. Red indicates high feature values; blue indicates low values.}
  \label{fig:shap_beeswarm}
\end{figure}

Table~\ref{tab:shap} and Figures~\ref{fig:shap_bar}--\ref{fig:shap_beeswarm} reveal how the model makes predictions, directly addressing RQ2. The top five SHAP features---age, nightlight z-score, working status, census literacy z-score, and poverty index z-score---are all spatial-structural variables. Identity features contribute minimally: the sex indicator (es\_mujer) ranks 16th out of 25 features with a mean absolute SHAP value of only 0.003, and the nationality indicator (es\_peruano) ranks 25th with effectively zero contribution, consistent with the $n=27$ non-Peruvian sample producing no learnable signal.

Critically, the top 5 SHAP features have zero overlap with the top 5 logistic regression features (which are dominated by indigenous language dummies). This paradigm difference reflects how tree-based models route predictions differently from linear models: where logistic regression must assign large coefficients to identity features to capture group-level differences, LightGBM can achieve similar discrimination through the continuous spatial-structural features that correlate with those identity categories. The fairness implications are significant: the model encodes structural inequities without using identity features directly. Removing protected attributes from the feature set would not mitigate the disparities documented above, because the model already operates through proxy features that carry the same information.

% =============================================================================
\section{Discussion}
% =============================================================================

\subsection{Summary of Findings}

Our equity audit of an Alerta Escuela--style dropout prediction model reveals three principal findings, each corresponding to a research question.

In response to \textbf{RQ1} (what disparities exist in prediction accuracy across demographic groups), we document a surveillance--invisibility axis across language groups: indigenous-language speakers experience low FNR (0.22) but high FPR (0.52), constituting surveillance bias, while Spanish speakers experience high FNR (0.63) but low FPR (0.18), constituting invisibility bias. This systematic redistribution of prediction errors follows directly from Chouldechova's \cite{chouldechova2017fair} impossibility result applied to groups with heterogeneous base rates.

In response to \textbf{RQ2} (how the model encodes disparities), SHAP analysis reveals that the model predicts through spatial-structural proxy features---age, nightlight intensity, census literacy rates, poverty index---rather than through identity features directly. Indigenous language indicators, which dominate the logistic regression model, contribute minimally to LightGBM predictions (zero overlap in top-5 features between models). This means the model encodes structural inequities without explicit use of protected attributes.

In response to \textbf{RQ3} (how intersections amplify prediction errors), the language $\times$ urbanicity intersection reveals urban indigenous students' FNR of 0.753---a disparity completely invisible in single-axis analysis of either language or geography alone. This finding directly operationalizes the intersectionality frameworks of Crenshaw~\cite{crenshaw1989demarginalizing} and the computational demonstrations of Buolamwini and Gebru~\cite{buolamwini2018gender} in an educational context.

\subsection{The Spatial Proxy Mechanism}

Our SHAP analysis reveals that the model essentially uses geography as a proxy for demographic risk. Nightlight intensity, district-level dropout rates, census literacy rates, and rurality indicators collectively encode the spatial concentration of disadvantage. This spatial proxy mechanism is efficient for prediction---it explains why LightGBM and logistic regression achieve comparable PR-AUC despite using features very differently---but it creates systematic blind spots for populations that do not match spatial stereotypes.

Urban indigenous students exemplify this failure mode: they reside in areas with higher nightlight intensity, lower district dropout rates, and more urban infrastructure, yet face educational barriers (language, cultural mismatch, limited bilingual instruction) comparable to their rural counterparts. The model has no pathway to identify their risk because the features that capture indigenous disadvantage in rural settings do not activate in urban ones. This finding resonates with Perdomo et al.'s~\cite{perdomo2023difficult} argument that structural features predict dropout as well as individual risk scores---and extends it by showing that reliance on structural features creates predictable fairness failures at demographic intersections.

\subsection{Considerations for EWS Operators}

Our findings raise several considerations for operators of educational early warning systems:

\begin{itemize}
  \item Group-specific threshold adjustment could equalize FNR across language groups, reducing invisibility bias for Spanish speakers without necessarily increasing overall error. However, threshold adjustment redistributes errors rather than eliminating them---equalizing FNR would increase FPR for Spanish speakers---and the appropriate trade-off depends on the relative costs of missed dropouts versus false alarms in specific operational contexts.
  \item Supplementary identification mechanisms for urban indigenous students could address the intersection-level blind spot our analysis reveals. However, designing such mechanisms without creating additional surveillance of already-marginalized communities requires careful consideration of community perspectives and consent \cite{mcmahon2020reenvisioning}.
  \item Regular fairness auditing, conducted across multiple demographic dimensions and their intersections, could detect disparities before they become entrenched. The question of who should conduct such audits---system operators, independent researchers, affected communities, or regulatory bodies---remains open.
\end{itemize}

\subsection{Generalizability}

Our findings likely apply to similar EWS systems across Latin America and other developing regions that use administrative or spatial features for dropout prediction. The surveillance--invisibility dynamic may emerge wherever prediction models operate on populations with heterogeneous base rates and correlated spatial-demographic structure. Adelman et al.'s~\cite{adelman2018predicting} work in Guatemala and Honduras and Villegas-Ch et al.'s~\cite{villegas2023supporting} analysis in Latin American higher education suggest that the structural features driving our model's predictions---poverty, geography, household characteristics---are similarly dominant across the region. Whether the specific intersection-level failures we document (urban indigenous invisibility) generalize depends on the degree to which indigenous populations in other countries exhibit similar rural-urban migration patterns.

% =============================================================================
\section{Limitations}
% =============================================================================

Several limitations qualify the interpretation of our findings.

First, we audit an Alerta Escuela--style model built on ENAHO survey data, not the actual Alerta Escuela system, which uses SIAGIE administrative records that are not publicly accessible. Our findings demonstrate what disparities \emph{can} emerge in dropout prediction using nationally representative data, not what disparities definitively exist in the deployed system. The feature sets, training data, and modeling choices of the actual system may differ from our proxy replication.

Second, ENAHO's mother tongue variable (P300) captures language by self-report. Bilingual speakers may report Spanish as their mother tongue, potentially undercounting indigenous-language prevalence and understating the disparities we document. The true magnitude of language-based prediction disparities may be larger than our estimates.

Third, the 2020 wave is affected by the COVID-19 pandemic. INEI conducted phone interviews rather than in-person household visits, producing a reduced sample (approximately 13,755 observations versus approximately 25,000 in typical years) with 52\% null values in the education attendance variable. While we include 2020 in the training data after dropping null records, this year may not represent the same population as in-person survey years.

Fourth, some intersectional subgroups have small samples: $n=89$ for urban other-indigenous students (our starkest finding) and $n=27$ for non-Peruvian nationality (rendering this dimension unusable for reliable inference). While $n=89$ is sufficient for point estimates of proportions, the associated confidence intervals are wide, and results for this group should be interpreted with appropriate caution.

Fifth, while we incorporate FACTOR07 survey weights throughout training and evaluation, the theoretical properties of survey-weighted gradient boosting are not fully established. MacNell et al.~\cite{macnell2023implementing} found that ignoring survey weights in gradient boosting can affect both prediction accuracy and feature importance rankings, supporting our decision to incorporate weights, but the formal statistical guarantees of weighted ML estimators under complex survey designs remain an active area of research.

% =============================================================================
\section{Ethical Considerations}
% =============================================================================

\paragraph{Positionality.} The author is Peruvian, a computer science self-learner at Universidad de Ingenier\'{i}a y Tecnolog\'{i}a (UTEC), and co-founder of Genera, an educational technology startup. This positionality shapes the work in important ways: as someone who believes AI can address education's one-size-fits-all problem, I undertook this audit precisely because systems deployed without scrutiny risk deepening the inequities I aim to address. I am not from the indigenous communities most affected by the disparities documented here, and this work should be understood as an outsider's technical audit rather than a community-centered assessment. A more complete evaluation would incorporate the perspectives of indigenous educators, families, and students directly affected by early warning systems.

\paragraph{Generative AI Disclosure.} Portions of the data pipeline code and manuscript preparation were assisted by generative AI tools (Claude, Anthropic). The author was responsible for all research design, methodological decisions, result interpretation, and substantive writing. AI assistance was used for code implementation and editorial refinement.

\paragraph{Data Ethics.} This study uses publicly available, de-identified survey data (ENAHO) released by INEI for research purposes. No individual students can be identified from the analysis, and no direct human subjects interaction was involved. The analysis operates exclusively on aggregate patterns in survey-weighted data.

% =============================================================================
\section{Conclusion}
% =============================================================================

This paper presents a comprehensive equity audit of an Alerta Escuela--style dropout prediction model using six years of nationally representative ENAHO survey data (2018--2023, $N=150{,}135$). Our audit evaluates fairness across seven demographic dimensions and three intersections, using survey-weighted metrics throughout and applying SHAP interpretability analysis to understand how the model encodes disparities.

Our findings answer three research questions. \textbf{RQ1:} Substantial disparities exist in prediction accuracy across demographic groups, organized along a surveillance--invisibility axis---indigenous-language speakers are over-flagged (FNR=0.22, FPR=0.52) while Spanish-speaking dropouts are systematically missed (FNR=0.63, FPR=0.18). \textbf{RQ2:} The model encodes these disparities through spatial-structural proxy features (age, nightlight intensity, census literacy rates, poverty index) rather than through identity features directly, as revealed by SHAP analysis showing zero overlap between the top-5 features of linear and tree-based models. \textbf{RQ3:} Intersectional analysis reveals that urban indigenous students face an FNR of 0.753---a disparity completely invisible to single-axis evaluation and the most extreme prediction failure in our analysis.

As educational early warning systems proliferate across Latin America and globally, fairness auditing must become standard practice. Our analysis demonstrates that single-axis evaluation is insufficient: the populations most systematically missed by prediction models---those at the intersection of multiple dimensions of disadvantage---can only be identified through intersectional analysis. The evidence presented here demonstrates that dropout prediction systems can encode systematic inequities even without explicitly discriminatory intent. Understanding who gets missed is a prerequisite for building systems that serve all students equitably.

% =============================================================================
% Appendix
% =============================================================================
\appendix
\section{Supplementary Tables}

Additional disaggregated fairness metrics, regional SHAP decompositions, and model hyperparameter details are available in the supplementary materials.

% =============================================================================
% Bibliography
% =============================================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
