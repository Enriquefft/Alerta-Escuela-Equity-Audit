\documentclass[acmsmall,nonacm]{acmart}

\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}

\sisetup{
  round-mode=places,
  round-precision=3,
  table-format=1.3
}

% Arrow macros for direction column
\newcommand{\up}{$\uparrow$}
\newcommand{\down}{$\downarrow$}

\title{Equity Audit of Peru's Alerta Escuela Early Warning System: Who Gets Missed?}

\author{Enrique Francisco Flores Teniente}
\affiliation{%
  \institution{Universidad de Ingeniería y Tecnología (UTEC)}
  \city{Lima}
  \country{Peru}
}
\affiliation{%
  \institution{Genera}
  \city{Lima}
  \country{Peru}
}
\email{enrique.flores@utec.edu.pe}

\begin{abstract}
Peru's Alerta Escuela early warning system aims to identify students at risk of dropping out, yet its algorithmic fairness properties remain unexamined. Using six years of nationally representative ENAHO survey data (2018--2023, $N=150{,}135$), we replicate an Alerta Escuela--style dropout prediction model and conduct a comprehensive equity audit across language, geography, poverty, and sex dimensions. Our LightGBM model achieves a calibrated test PR-AUC of 0.236, with a false negative rate (FNR) of 63\% for Spanish-speaking students but only 22\% for indigenous-language speakers---revealing that the system over-flags indigenous students (surveillance bias) while missing the majority of Spanish-speaking dropouts (invisibility bias). SHAP analysis shows the model predicts through spatial-structural features (age, nightlights, literacy rates, poverty) rather than identity features directly. The starkest disparity emerges at the intersection of language and urbanicity: urban indigenous students face a 75\% FNR, making them the most invisible group. These findings demonstrate that dropout prediction systems can encode systematic inequities even without explicitly discriminatory intent.
\end{abstract}

\keywords{educational equity, dropout prediction, algorithmic fairness, Peru, ENAHO, early warning system}

\begin{document}
\maketitle

% =============================================================================
\section{Introduction}
% =============================================================================

Educational early warning systems (EWS) are proliferating across Latin America as governments seek data-driven approaches to reduce school dropout. Peru's Alerta Escuela, operated by the Ministry of Education (MINEDU), flags students at risk of leaving school using administrative data from the SIAGIE system \cite{minedu2023alerta}. Such systems promise efficiency and early intervention, but their algorithmic fairness properties remain almost entirely unaudited. A growing body of research has demonstrated that predictive models can systematically disadvantage marginalized groups---encoding structural inequities into automated decisions that affect millions of students \cite{barocas2016big,mitchell2021algorithmic}.

Despite the expanding algorithmic fairness literature, few studies audit deployed educational prediction systems in developing countries. Most fairness work focuses on US and European contexts, examines race and gender as primary dimensions, and does not incorporate survey weights or intersectional analysis \cite{kizilcec2020algorithmic,baker2022algorithmic,gardner2024debiasing}. This gap is particularly consequential in countries like Peru, where the axes of disadvantage---mother tongue, geography, poverty---differ fundamentally from those studied in the Global North.

Peru is a multilingual country where approximately 16\% of the population speaks an indigenous language as their mother tongue. Indigenous-language speakers face persistent educational inequities rooted in colonial legacies, geographic isolation, and inadequate bilingual education---only 37\% of indigenous students attend schools with bilingual instruction \cite{cueto2009explaining,cueto2016education}. Peru's Encuesta Nacional de Hogares (ENAHO), conducted annually by the Instituto Nacional de Estad\'{i}stica e Inform\'{a}tica (INEI), provides nationally representative data that enables analysis of these disparities \cite{inei2023enaho}. Because the actual SIAGIE administrative data used by Alerta Escuela is not publicly accessible, we construct a proxy replication of an Alerta Escuela--style dropout prediction model using ENAHO survey data spanning 150,135 student-year observations across six years (2018--2023).

This paper addresses three research questions:
\begin{enumerate}
  \item \textbf{RQ1:} What disparities exist in dropout prediction accuracy across demographic groups defined by language, geography, poverty, and sex?
  \item \textbf{RQ2:} How does the model encode these disparities---through identity features directly or through structural proxies?
  \item \textbf{RQ3:} How do intersections of demographic dimensions (e.g., language $\times$ geography) amplify prediction errors beyond what single-axis analysis reveals?
\end{enumerate}

To answer these questions, we train a LightGBM model with Platt calibration \cite{ke2017lightgbm,platt1999probabilistic}, evaluate fairness across seven demographic dimensions and three intersections using the fairlearn framework \cite{bird2020fairlearn}, and apply SHAP TreeExplainer to decompose predictions into feature-level contributions \cite{lundberg2017unified}. We use a temporal train/validation/test split (2018--2021/2022/2023) that mirrors real-world deployment, and incorporate ENAHO survey weights (FACTOR07) throughout all metrics to ensure nationally representative estimates.

Our analysis reveals a surveillance--invisibility axis across language groups: the model achieves a false negative rate (FNR) of only 0.22 for indigenous-language speakers but with a false positive rate (FPR) of 0.52---over-flagging that constitutes surveillance bias. Conversely, Spanish-speaking students face an FNR of 0.63 with FPR of only 0.18, meaning the system misses the majority of their dropouts---invisibility bias. The starkest disparity emerges at the intersection of language and urbanicity: urban indigenous students face an FNR of 0.753, meaning the model misses three out of four of their dropouts. SHAP analysis shows the model predicts through spatial-structural features (age, nightlight intensity, literacy rates, poverty index) rather than identity features directly, and algorithm-independence is confirmed across LightGBM and XGBoost (PR-AUC ratio of 1.0006).

Our contributions are:
\begin{itemize}
  \item One of few comprehensive fairness audits of an educational EWS in a developing country, spanning seven demographic dimensions and three intersections with survey-weighted metrics throughout.
  \item A demonstration that intersectional analysis reveals disparities hidden by single-axis evaluation---urban indigenous students are invisible in both language-only and geography-only analyses but emerge as the most systematically missed group when dimensions are crossed \cite{crenshaw1989demarginalizing,buolamwini2018gender}.
  \item Evidence that dropout prediction models encode structural inequities through spatial-structural proxy features rather than through explicit use of protected attributes, with implications for fairness interventions.
  \item An open-source, replicable audit framework---code, data pipeline, and analysis are publicly available to enable similar audits of educational EWS in other contexts.
\end{itemize}

% =============================================================================
\section{Related Work}
% =============================================================================

\subsection{Early Warning Systems in Education}

Dropout early warning systems have evolved substantially over the past two decades. Bowers~\cite{bowers2010grades} established foundational indicators---grades, GPA, and course failures---as predictors of dropout risk in a longitudinal study of US high school students. As machine learning methods matured, researchers developed increasingly sophisticated systems: Lakkaraju et al.~\cite{lakkaraju2015machine} introduced an ML framework for identifying at-risk K-12 students, demonstrating that ensemble methods could substantially outperform traditional indicator thresholds. Knowles~\cite{knowles2015needles} deployed the first statewide ML-based dropout EWS in Wisconsin, covering over 225,000 students with administrative data---a scale comparable to Peru's Alerta Escuela.

Subsequent work has expanded both the methods and the contexts. Baker et al.~\cite{baker2020predicting} applied logistic regression to attendance, grades, and disciplinary data in diverse US school districts, while Lee and Chung~\cite{lee2019machine} established the pattern of temporal train-test splits that mirrors real-world deployment---a design we adopt. In the developing-country context most relevant to our work, Adelman et al.~\cite{adelman2018predicting} used administrative data to predict dropout in Guatemala and Honduras, correctly identifying 80\% of eventual dropouts.

Two recent contributions provide critical framing for our study. McMahon et al.~\cite{mcmahon2020reenvisioning} argue that EWS should shift from pure identification to meaningful prediction-plus-intervention, questioning whether flagging students as ``at-risk'' without adequate support mechanisms constitutes a net benefit. Most provocatively, Perdomo et al.~\cite{perdomo2023difficult} evaluate Wisconsin's deployed EWS over a decade and argue that structural features predict dropout as well as individual risk scores---a finding our SHAP analysis directly corroborates. Our paper extends this critical tradition by auditing an EWS-style model in a context where deployment occurs but fairness evaluation does not.

\subsection{Algorithmic Fairness in Education}

Kizilcec and Lee~\cite{kizilcec2020algorithmic} identified a critical gap in their framework paper on algorithmic fairness in education: while fairness audits were becoming standard in criminal justice and hiring, they remained rare in deployed educational systems. Subsequent work has begun to address this gap, but important blind spots persist.

Baker and Hawn~\cite{baker2022algorithmic} provided the most comprehensive review of algorithmic bias in education to date, cataloguing known biases across applications from automated essay scoring to dropout prediction and introducing ``slice analysis'' as a methodology for disaggregated fairness evaluation. Mehrabi et al.~\cite{mehrabi2021survey} offered a broader taxonomy of fairness definitions and bias sources in ML, situating educational applications within the larger landscape of fairness criteria---including the impossibility results of Chouldechova~\cite{chouldechova2017fair} that prove no classifier can simultaneously satisfy calibration, equal FNR, and equal FPR across groups with different base rates.

Applied studies have examined specific educational domains. Loukina, Madnani, and Zechner~\cite{loukina2019many} demonstrated that different fairness definitions require different solutions in automated essay scoring, showing that total fairness may be unachievable---a finding that parallels the FNR-FPR trade-off we document. Pan and Zhang~\cite{pan2024examining} examined algorithmic fairness specifically in high school dropout prediction using HSLS:09 data, finding model-specific biases across threshold ranges; however, their analysis was limited to a US context without survey weights or intersectional analysis. Karimi-Haghighi et al.~\cite{karimihag2021predicting} combined calibration and fairness evaluation in dropout prediction, evaluating across nationality, gender, and school type, but without the survey-weighted methodology our ENAHO analysis requires.

On the intervention side, Saleiro et al.~\cite{saleiro2018aequitas} developed the Aequitas bias audit toolkit that operationalizes fairness evaluation, and Gardner, Brooks, and Baker~\cite{gardner2024debiasing} systematically reviewed debiasing strategies in educational ML, finding that most studies focus on gender and race in US and European contexts. Our paper fills the gap identified by Kizilcec and Lee: a comprehensive fairness audit in a developing-country, multilingual context using survey-weighted analysis across seven dimensions and three intersections.

\subsection{Fairness in Latin American Educational AI}

Latin American education systems face structural inequalities rooted in colonial histories, geographic barriers, and linguistic diversity \cite{unesco2022lac}. In Peru specifically, indigenous-language speakers experience persistent educational disadvantage. Cueto et al.~\cite{cueto2009explaining} documented how ethnic and language minorities in Peru are systematically marginalized in education, finding that indigenous-language students receive lower-quality instruction and face cultural barriers to school engagement. Cueto, Miranda, and Le\'{o}n~\cite{cueto2016education} traced education trajectories from early childhood through adolescence using Young Lives longitudinal data, reporting that only 37\% of indigenous students attend bilingual schools despite legal mandates for intercultural bilingual education.

Machine learning is increasingly applied to dropout prediction in the region. Adelman et al.~\cite{adelman2018predicting} demonstrated effective dropout prediction in Guatemala and Honduras using administrative data, and Villegas-Ch et al.~\cite{villegas2023supporting} evaluated ML approaches for higher education dropout in a Latin American context, finding that socioeconomic variables dominate prediction. Zawacki-Richter et al.~\cite{zawacki2022artificial} surveyed AI applications in Latin American higher education, identifying growing adoption but limited resources and unequal socioeconomic contexts as persistent challenges. Notably, none of these studies conducted fairness audits of their prediction systems. Our paper provides the first such audit for a Peruvian educational prediction system, examining whether the demographic disparities documented by Cueto and colleagues are reproduced---or amplified---by algorithmic prediction.

\subsection{Intersectionality in ML Fairness}

Crenshaw~\cite{crenshaw1989demarginalizing} established the foundational insight that single-axis analysis of disadvantage systematically misses the experiences of those who face compound marginalization. Originally articulated in the context of Black women's experiences under US antidiscrimination law, the intersectionality framework has proven broadly applicable to algorithmic systems.

Buolamwini and Gebru~\cite{buolamwini2018gender} demonstrated this insight computationally in their landmark Gender Shades study: commercial facial recognition systems showed error rates of 34.7\% for darker-skinned females compared to 0.8\% for lighter-skinned males---a disparity invisible in either gender-only or skin-type-only analysis. This finding catalyzed a body of work formalizing intersectional fairness in ML. Kearns et al.~\cite{kearns2018preventing} proved that a classifier can appear fair on individual protected groups while violating fairness on their intersections, formalizing the concept of subgroup fairness and demonstrating that auditing individual dimensions is provably insufficient. H\'{e}bert-Johnson et al.~\cite{hebertjohnson2018multicalibration} extended this reasoning to calibration, introducing multicalibration---the requirement that predicted probabilities be well-calibrated not just overall but for all computationally identifiable subgroups.

Our intersectional analysis directly operationalizes these frameworks in an educational context where no prior intersectional audit exists. The language $\times$ geography intersection that reveals urban indigenous students' FNR of 0.753 parallels the Gender Shades finding: a group invisible to single-axis analysis that faces the most extreme prediction errors. Where Buolamwini and Gebru examined race $\times$ gender in facial recognition, we examine language $\times$ urbanicity in dropout prediction---demonstrating that the intersectionality imperative extends to educational EWS in developing countries.

% =============================================================================
\section{Data}
% =============================================================================

We use Peru's Encuesta Nacional de Hogares (ENAHO), a nationally representative household survey conducted annually by the Instituto Nacional de Estad\'istica e Inform\'atica (INEI) \cite{inei2023enaho}. Our analysis pools six waves (2018--2023) covering school-age children (6--17 years), yielding 150,135 individual-year observations.

\begin{table}[htbp]
  \caption{Sample Description by Demographic Dimensions}
  \label{tab:sample}
  \input{tables/table_01_sample.tex}
\end{table}

Table~\ref{tab:sample} summarizes the sample across key demographic dimensions. Dropout is defined as a child of school age who was enrolled in the previous year but is not currently attending, following MINEDU's operational definition.

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Language Group}
  \label{tab:language}
  \input{tables/table_02_language.tex}
\end{table}

Table~\ref{tab:language} reveals substantial disparities in dropout rates across language groups. Indigenous-language speakers face rates 34\% higher than Spanish speakers on average.

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Region and Poverty Quintile}
  \label{tab:region_poverty}
  \input{tables/table_03_region_poverty.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig04_dropout_heatmap.pdf}
  \caption{Dropout rates by language group and rurality. Each cell shows the weighted dropout rate for the intersection of language and urban/rural geography.}
  \label{fig:dropout_heatmap}
\end{figure}

Table~\ref{tab:region_poverty} and Figure~\ref{fig:dropout_heatmap} show how dropout rates vary by region and poverty level, with the Sierra and Selva regions and the poorest quintile exhibiting the highest rates.

% =============================================================================
\section{Methods}
% =============================================================================

We engineer 25 features spanning individual demographics, household characteristics, and district-level spatial indicators from census and administrative data. Models are trained on 2018--2021 data ($n=98{,}023$), validated on 2022 ($n=26{,}477$), and tested on 2023 ($n=25{,}635$), following a temporal split that mirrors real-world deployment.

We compare three model families: logistic regression (for interpretability), LightGBM \cite{ke2017lightgbm} (for predictive performance), and XGBoost (for algorithm-independence verification). LightGBM hyperparameters are tuned via Optuna with 100 trials. Platt scaling calibrates the final model's probability estimates. All metrics are computed with ENAHO survey weights (FACTOR07).

Fairness evaluation uses the fairlearn framework \cite{bird2020fairlearn} to compute false negative rates (FNR), false positive rates (FPR), precision, and PR-AUC across seven demographic dimensions and three intersections. SHAP TreeExplainer \cite{lundberg2020local} provides feature-level interpretability.

% =============================================================================
\section{Results}
% =============================================================================

\begin{table}[htbp]
  \caption{Model Performance Comparison (Survey-Weighted Metrics)}
  \label{tab:models}
  \input{tables/table_04_models.tex}
\end{table}

Table~\ref{tab:models} compares the three model families. LightGBM and XGBoost achieve near-identical validation PR-AUC (0.262 vs. 0.263), confirming algorithm independence of our fairness findings. The calibrated LightGBM model achieves a test PR-AUC of 0.236 with substantially improved Brier score (0.112 vs. 0.186 uncalibrated).

\begin{table}[htbp]
  \caption{Logistic Regression Coefficients (All 25 Features)}
  \label{tab:lr_coefficients}
  \input{tables/table_05_lr_coefficients.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig01_pr_curves.pdf}
  \caption{Precision-Recall curves for all three model families on the 2022 validation set. LightGBM and XGBoost curves largely overlap, confirming algorithm independence.}
  \label{fig:pr_curves}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig02_calibration.pdf}
  \caption{Calibration plot comparing uncalibrated and Platt-calibrated LightGBM probabilities. Platt scaling reduces Brier score by 38\%.}
  \label{fig:calibration}
\end{figure}

Table~\ref{tab:lr_coefficients} shows the logistic regression coefficients. Indigenous language variables dominate the linear model, with the ``other indigenous'' group having the highest odds ratio (2.20). Figure~\ref{fig:pr_curves} compares PR curves across models, and Figure~\ref{fig:calibration} demonstrates the calibration improvement.

% =============================================================================
\section{Fairness Analysis}
% =============================================================================

\begin{table}[htbp]
  \caption{Fairness Metrics by Language Group}
  \label{tab:fairness_language}
  \input{tables/table_06_fairness_language.tex}
\end{table}

Table~\ref{tab:fairness_language} reveals a fundamental FNR--FPR trade-off across language groups. The model achieves low FNR for indigenous-language speakers (0.22 for other indigenous) but at the cost of high FPR (0.52)---a pattern we term ``surveillance bias.'' Conversely, Spanish speakers face high FNR (0.63) with low FPR (0.18)---``invisibility bias'' where the majority of actual dropouts are missed.

\begin{table}[htbp]
  \caption{Intersection Analysis: Language $\times$ Rurality}
  \label{tab:intersection}
  \input{tables/table_07_intersection.tex}
\end{table}

\begin{table}[htbp]
  \caption{SHAP Feature Importance (Top 15)}
  \label{tab:shap}
  \input{tables/table_08_shap.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig03_fnr_fpr_language.pdf}
  \caption{FNR and FPR by language group. The inverse relationship between FNR and FPR reveals the surveillance--invisibility trade-off.}
  \label{fig:fnr_fpr}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig05_fnr_heatmap.pdf}
  \caption{FNR heatmap by language and rurality intersection. The darkest cell (other indigenous, urban) represents the group most missed by the model.}
  \label{fig:fnr_heatmap}
\end{figure}

Table~\ref{tab:intersection} shows the starkest finding: urban indigenous students face an FNR of 0.753---the model misses three-quarters of their dropouts. This intersection group, invisible in both language-only and geography-only analyses, represents the most systematically missed population.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig06_shap_bar.pdf}
  \caption{Mean absolute SHAP values for the top 15 features. Age and spatial-structural features dominate, while identity features (language, sex) have minimal direct importance.}
  \label{fig:shap_bar}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig07_shap_beeswarm.pdf}
  \caption{SHAP beeswarm plot showing feature value distributions and their impact on predictions. Red indicates high feature values; blue indicates low values.}
  \label{fig:shap_beeswarm}
\end{figure}

Table~\ref{tab:shap} and Figures~\ref{fig:shap_bar}--\ref{fig:shap_beeswarm} reveal that the model predicts through spatial-structural features (age, nightlight intensity, working status, census indicators) rather than identity features directly. The top 5 SHAP features have zero overlap with the top 5 logistic regression features, reflecting the paradigm difference between linear and tree-based models.

% =============================================================================
\section{Discussion}
% =============================================================================

Our equity audit reveals three key findings. First, the FNR--FPR trade-off across language groups represents a systematic redistribution of prediction errors: indigenous students are over-flagged (surveillance) while Spanish-speaking dropouts are missed (invisibility). Second, intersection analysis reveals that urban indigenous students---who defy the spatial profile associated with indigenous languages---are the most invisible group. Third, SHAP analysis shows the model operates through spatial-structural proxies rather than direct identity features, suggesting that even ``fair'' feature sets can encode structural inequities.

These findings have direct policy implications for Alerta Escuela and similar systems across Latin America. We recommend: (1) group-specific threshold adjustment to equalize FNR across language groups; (2) supplementary urban indigenous identification mechanisms; and (3) regular fairness auditing as a deployment requirement.

% =============================================================================
\section{Conclusion}
% =============================================================================

This paper demonstrates that dropout prediction systems can systematically disadvantage the groups they aim to serve. Our comprehensive equity audit of an Alerta Escuela--style model reveals a surveillance--invisibility axis across language groups and identifies urban indigenous students as the most missed population. These findings underscore the need for mandatory fairness auditing of educational early warning systems.

% =============================================================================
% Appendix
% =============================================================================
\appendix
\section{Supplementary Tables}

Additional disaggregated fairness metrics, regional SHAP decompositions, and model hyperparameter details are available in the supplementary materials.

% =============================================================================
% Bibliography
% =============================================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
