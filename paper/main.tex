\documentclass[acmsmall,nonacm]{acmart}

\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}

\sisetup{
  round-mode=places,
  round-precision=3,
  table-format=1.3
}

% Arrow macros for direction column
\newcommand{\up}{$\uparrow$}
\newcommand{\down}{$\downarrow$}

\title{Who Gets Missed? A Proxy Equity Audit of Survey-Derived Dropout Risk in Peru}

\author{Enrique Francisco Flores Teniente}
\affiliation{%
  \institution{Universidad de Ingeniería y Tecnología (UTEC)}
  \city{Lima}
  \country{Peru}
}
\affiliation{%
  \institution{Genera}
  \city{Lima}
  \country{Peru}
}
\email{enrique.flores@utec.edu.pe}

\begin{abstract}
Dropout prediction systems are proliferating across Latin America, yet their fairness properties remain unaudited. We construct a proxy dropout prediction model from publicly available ENAHO survey data (2018--2023, $N=150{,}135$) targeting the same school-age population as Peru's Alerta Escuela early warning system. We have not accessed Alerta Escuela's predictions, training data, or operational feature set; our findings characterize disparities in survey-derived dropout risk modeling, not the deployed system itself. Training five model families (logistic regression, LightGBM, XGBoost, random forest, and MLP) and auditing across language, geography, poverty, and sex dimensions, we find that the calibrated LightGBM model (test PR-AUC = 0.236, top-decile lift = 2.54$\times$) exhibits a false negative rate (FNR) of 63.3\% for Spanish-speaking students [95\% CI: 0.608, 0.656] but only 21.6\% for indigenous-language speakers [0.137, 0.310]---the majority of Spanish-speaking dropouts are missed while indigenous students are over-flagged. This FNR rank order holds across all five model families. SHAP analysis shows the model predicts through spatial-structural features rather than identity features directly. Intersectional analysis identifies urban indigenous students as a potentially high-FNR subgroup (pooled estimate 0.69, $n=167$), though small sample size limits precision (95\% CI [0.39, 0.98]); a power analysis shows that survey-based intersectional auditing requires approximately 8 ENAHO years to confirm whether FNR exceeds 0.50 for this group, demonstrating a methodological ceiling that argues for opening administrative data. Our contributions are: (1) a proxy audit framework demonstrating that independent algorithmic accountability is achievable using only public data, and (2) empirical documentation that Spanish-speaking dropouts---the demographic majority---are the group most systematically missed by survey-derived prediction.
\end{abstract}

\keywords{educational equity, dropout prediction, algorithmic fairness, proxy audit, Peru, ENAHO, early warning system}

\begin{document}
\maketitle

% =============================================================================
\section{Introduction}
% =============================================================================

Educational early warning systems (EWS) are proliferating across Latin America as governments seek data-driven approaches to reduce school dropout. Peru's Alerta Escuela, operated by the Ministry of Education (MINEDU), flags students at risk of leaving school using administrative data from the SIAGIE system \cite{minedu2023alerta}. Such systems promise efficiency and early intervention, but their algorithmic fairness properties remain almost entirely unaudited. Research demonstrates that predictive models can systematically disadvantage marginalized groups---encoding structural inequities into automated decisions that affect millions of students \cite{barocas2016big,mitchell2021algorithmic}.

Despite the expanding algorithmic fairness literature, few studies audit deployed educational prediction systems in developing countries. Most fairness work focuses on US and European contexts, examines race and gender as primary dimensions, and does not incorporate survey weights or intersectional analysis \cite{kizilcec2020algorithmic,baker2022algorithmic,gardner2024debiasing}. This gap is consequential in countries like Peru, where the axes of disadvantage---mother tongue, geography, poverty---differ from those studied in the Global North.

Peru is a multilingual country where approximately 16\% of the population speaks an indigenous language as their mother tongue. Indigenous-language speakers face persistent educational inequities rooted in colonial legacies, geographic isolation, and inadequate bilingual education---only 37\% of indigenous students attend schools with bilingual instruction \cite{cueto2009explaining,cueto2016education}. Peru's Encuesta Nacional de Hogares (ENAHO), conducted annually by the Instituto Nacional de Estad\'{i}stica e Inform\'{a}tica (INEI), provides nationally representative data that enables analysis of these disparities \cite{inei2023enaho}. Because the actual SIAGIE administrative data used by Alerta Escuela is not publicly accessible, we construct a proxy replication of an Alerta Escuela--style dropout prediction model using ENAHO survey data spanning 150,135 student-year observations across six years (2018--2023).

This paper addresses three research questions:
\begin{enumerate}
  \item \textbf{RQ1:} What disparities exist in dropout prediction accuracy across demographic groups defined by language, geography, poverty, and sex?
  \item \textbf{RQ2:} How does the model encode these disparities---through identity features directly or through structural proxies?
  \item \textbf{RQ3:} How do intersections of demographic dimensions (e.g., language $\times$ geography) amplify prediction errors beyond what single-axis analysis reveals?
\end{enumerate}

To answer these questions, we train a LightGBM model with Platt calibration \cite{ke2017lightgbm,platt1999probabilistic}, evaluate fairness across seven demographic dimensions and three intersections using the fairlearn framework \cite{bird2020fairlearn}, and apply SHAP TreeExplainer to decompose predictions into feature-level contributions \cite{lundberg2017unified}. We use a temporal train/validation/test split (2018--2021/2022/2023) that mirrors real-world deployment, and incorporate ENAHO survey weights (FACTOR07) throughout all metrics to ensure nationally representative estimates.

Our analysis reveals a surveillance--invisibility axis: the model over-flags indigenous-language students (low false negative rate but high false positive rate) while missing the majority of Spanish-speaking dropouts (high FNR, low FPR). The starkest disparity emerges at the intersection of language and urbanicity, where the model fails to identify most dropouts in a specific subgroup. SHAP analysis shows the model predicts through spatial-structural proxy features rather than identity features, and the pattern holds across all five model families.

Our contributions are:
\begin{itemize}
  \item A proxy equity audit framework demonstrating that independent algorithmic accountability is achievable using only publicly available survey data, without access to the system's training data or operational predictions---enabling accountability where direct system access is unavailable.
  \item A comprehensive fairness audit spanning seven demographic dimensions and three intersections with survey-weighted metrics, demonstrating that intersectional analysis reveals disparities hidden by single-axis evaluation---urban indigenous students emerge as the most systematically missed group only when language and urbanicity are crossed \cite{crenshaw1989demarginalizing,buolamwini2018gender}.
  \item Evidence that dropout prediction models encode structural inequities through spatial-structural proxy features rather than through explicit use of protected attributes, with implications for fairness interventions.
  \item An open-source, replicable audit framework---code, data pipeline, and analysis are publicly available to enable similar audits of educational EWS in other contexts.
\end{itemize}

We emphasize that this is a proxy audit: we have not accessed Alerta Escuela's predictions, training data, or operational feature set, and make no claims about the deployed system's specific fairness properties. Our findings characterize disparities that can emerge from survey-derived dropout prediction in Peru's demographic context.

% =============================================================================
\section{Related Work}
% =============================================================================

Dropout early warning systems have matured from Bowers's~\cite{bowers2010grades} indicator-based approach through Lakkaraju et al.'s~\cite{lakkaraju2015machine} ML framework to statewide deployments like Knowles's~\cite{knowles2015needles} Wisconsin system covering 225,000 students. Adelman et al.~\cite{adelman2018predicting} extended this work to the developing-country context, predicting dropout in Guatemala and Honduras with 80\% recall. Yet fairness audits did not follow this expansion: Perdomo et al.~\cite{perdomo2023difficult} evaluated Wisconsin's deployed EWS over a decade and found that structural features predict dropout as well as individual risk scores, while McMahon et al.~\cite{mcmahon2020reenvisioning} questioned whether flagging students without adequate support mechanisms constitutes a net benefit. Our paper extends this critical tradition by auditing an EWS-style model in a context where deployment occurs but fairness evaluation does not.

Kizilcec and Lee~\cite{kizilcec2020algorithmic} identified that fairness audits remained rare in education. Baker and Hawn~\cite{baker2022algorithmic} catalogued known biases and introduced ``slice analysis'' for disaggregated evaluation. Chouldechova~\cite{chouldechova2017fair} proved that no classifier can simultaneously satisfy calibration, equal FNR, and equal FPR across groups with different base rates---an impossibility result our findings directly illustrate. Pan and Zhang~\cite{pan2024examining} and Karimi-Haghighi et al.~\cite{karimihag2021predicting} examined fairness in dropout prediction but without survey weights or intersectional analysis. Gardner et al.~\cite{gardner2024debiasing} found most debiasing studies focus on gender and race in US/European contexts. Our paper fills this gap with a proxy audit in a developing-country, multilingual context using survey-weighted analysis across seven dimensions and three intersections.

Crenshaw~\cite{crenshaw1989demarginalizing} established that single-axis analysis systematically misses compound marginalization, and Kearns et al.~\cite{kearns2018preventing} proved this formally: auditing subgroups defined by single attributes is provably insufficient for ensuring fairness across intersections. Buolamwini and Gebru~\cite{buolamwini2018gender} demonstrated this computationally with facial recognition error rates invisible in single-axis analysis. In Peru, Cueto et al.~\cite{cueto2009explaining,cueto2016education} documented persistent educational disadvantage for indigenous-language speakers, reporting that only 37\% of indigenous students attend bilingual schools. Villegas-Ch et al.~\cite{villegas2023supporting} applied ML to dropout prediction in Latin America but without fairness audits. Our intersectional analysis---crossing language, geography, and poverty---examines whether these documented disparities are reproduced or amplified by algorithmic prediction.

% =============================================================================
\section{Data}
% =============================================================================

Peru has approximately 8 million school-age children, and dropout remains a persistent challenge---particularly in rural areas and among indigenous-language communities. The Ministry of Education (MINEDU) operates Alerta Escuela, which uses data from the Sistema de Informaci\'{o}n de Apoyo a la Gesti\'{o}n de la Instituci\'{o}n Educativa (SIAGIE) to flag students at risk of dropout \cite{minedu2023alerta,minedu2022estadistica}. Because SIAGIE administrative records are not publicly accessible, we use ENAHO survey data as a proxy to construct and audit an Alerta Escuela--style prediction model.

\begin{table}[htbp]
  \caption{ENAHO vs.\ SIAGIE Feature Availability. SIAGIE columns are inferred from public documentation \cite{minedu2023alerta,minedu2022estadistica}; we have not accessed SIAGIE records directly. This comparison documents the features \emph{not} available in our proxy model that may be present in the actual system.}
  \label{tab:enaho_siagie}
  \input{tables/table_09_enaho_siagie.tex}
\end{table}

The comparison in Table~\ref{tab:enaho_siagie} highlights a key limitation of the proxy approach: SIAGIE contains daily attendance records, multi-year student trajectory, and grade history that ENAHO does not capture. Our proxy model predicts from annual cross-sectional survey data, missing the longitudinal signal that presumably improves the actual system's predictive accuracy. However, the survey dimensions available in ENAHO---mother tongue, poverty, geography---are precisely those needed to study equity disparities, and these dimensions are either absent from or not publicly reported for SIAGIE-based models.

We use Peru's Encuesta Nacional de Hogares (ENAHO), a nationally representative household survey conducted annually by the Instituto Nacional de Estad\'{i}stica e Inform\'{a}tica (INEI) \cite{inei2023enaho}. ENAHO employs a multi-stage, stratified sampling design covering all 25 departments of Peru, with survey weights (FACTOR07) that account for the complex sampling structure and enable nationally representative inference. We extract data from Module 200 (demographic characteristics) and Module 300 (education), joining them by household and person identifiers.

Our analysis pools six annual waves (2018--2023) covering school-age children aged 6--17, yielding 150,135 individual-year observations after data cleaning. The 2020 wave is notably affected by the COVID-19 pandemic: INEI conducted phone interviews rather than in-person visits, resulting in a reduced sample of approximately 13,755 observations (compared to approximately 25,000 in a typical year) and 52\% null values in the education attendance variable (P303), which were dropped. We define dropout as a binary outcome: a child of school age who was enrolled in the previous academic year but is not currently attending, following MINEDU's operational definition.

\begin{table}[htbp]
  \caption{Sample Description by Demographic Dimensions}
  \label{tab:sample}
  \input{tables/table_01_sample.tex}
\end{table}

The sample is predominantly Spanish-speaking (approximately 84\%), with indigenous-language speakers comprising Quechua, Aymara, and other indigenous groups. Urban residents constitute approximately 65\% of observations. The sample spans all three major geographic regions: Costa (coast), Sierra (highlands), and Selva (Amazon lowlands).

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Language Group}
  \label{tab:language}
  \input{tables/table_02_language.tex}
\end{table}

Table~\ref{tab:language} reveals substantial disparities in weighted dropout rates across language groups. Indigenous-language speakers face rates 34\% higher than Spanish speakers on average. The Otros ind\'{i}genas group exhibits the highest dropout rate at 0.219, followed by Awajun at 0.205, compared to 0.153 for Castellano speakers---a gap that persists even after accounting for geographic and socioeconomic differences. For the fairness analysis (Section~\ref{sec:fairness}), Ashaninka and Awajun are grouped under ``Otros ind\'{i}genas'' due to small per-group sample sizes that would yield unreliable metric estimates; Extranjero speakers are excluded from language-dimension analysis given the proxy model's focus on indigenous--Spanish disparities. This consolidation accounts for the difference between the stated test set size ($n=25{,}635$) and the language fairness table sum ($n=25{,}592$): the 43 Extranjero students in the 2023 test set are excluded from Table~\ref{tab:fairness_language}.

\begin{table}[htbp]
  \caption{Weighted Dropout Rates by Region and Poverty Quintile}
  \label{tab:region_poverty}
  \input{tables/table_03_region_poverty.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig04_dropout_heatmap.pdf}
  \caption{Dropout rates by language group and rurality. Each cell shows the weighted dropout rate for the intersection of language and urban/rural geography.}
  \label{fig:dropout_heatmap}
\end{figure}

The Sierra and Selva regions exhibit higher dropout rates than the Costa, and a largely monotonic poverty gradient is visible (with a minor reversal between Q2 and Q3): the poorest quintile has substantially higher dropout rates than the wealthiest. Figure~\ref{fig:dropout_heatmap} further reveals that the interaction of language and rurality produces disparities that exceed what either dimension alone would suggest.

In addition to individual and household variables from ENAHO, we merge district-level spatial features to capture contextual effects. Census data provides district population and literacy rate z-scores. Nightlight intensity, measured by satellite remote sensing, serves as a proxy for local economic activity and infrastructure. Administrative records from MINEDU provide district-level primaria (primary) and secundaria (secondary) completion rates. Merge rates are high: 100\% for administrative and census data, and 95.9\% for nightlight data, with 44 districts (1.53\%) having primaria but no secundaria administrative records.

% =============================================================================
\section{Methods}
% =============================================================================

\subsection{Feature Engineering}

We engineer 25 features organized into three categories: \emph{individual demographics} (8 features: age, sex, nationality, mother tongue dummies), \emph{household characteristics} (8 features: parent education, poverty index, poverty quintile, working status, household size, birthplace match), and \emph{district-level spatial indicators} (9 features: nightlight intensity z-score, census literacy and population z-scores, administrative completion rates, historical dropout rate). Table~\ref{tab:lr_coefficients} lists all 25 features with their logistic regression coefficients. Nightlight z-score nulls (4.1\%) are imputed with 0.0 \cite{macnell2023implementing}; poverty quintiles are constructed using FACTOR07-weighted quantiles.

\subsection{Model Selection and Training}

We compare five model families chosen for complementary purposes. \emph{Logistic regression} provides interpretable coefficients and odds ratios; we fit both a scikit-learn implementation for prediction and a statsmodels GLM with Binomial family for statistical inference. \emph{LightGBM} \cite{ke2017lightgbm} serves as the primary predictive model, leveraging gradient boosting's ability to capture nonlinearities and feature interactions. \emph{XGBoost} \cite{chen2016xgboost} provides a second gradient boosting implementation for cross-architecture consistency checking. \emph{Random Forest} extends the ensemble comparison beyond boosting to bagging. \emph{MLP} (multilayer perceptron) provides a neural network baseline with fundamentally different inductive biases from tree-based models. If fairness findings hold across all five families, they reflect data structure rather than algorithmic artifacts.

Models are trained on 2018--2021 data ($n=98{,}023$), validated on 2022 ($n=26{,}477$), and tested on 2023 ($n=25{,}635$). This temporal split mirrors real-world deployment, where models trained on historical data must predict future cohorts. LightGBM hyperparameters are tuned via Optuna \cite{akiba2019optuna} with 100 trials, using early stopping on validation average precision (PR-AUC). All models incorporate ENAHO survey weights (FACTOR07) during training and evaluation to ensure nationally representative estimates.

\subsection{Calibration}

Gradient boosted trees produce probability estimates that are often poorly calibrated, particularly when class weighting is applied to handle imbalanced outcomes \cite{niculescumizil2005predicting}. We apply Platt scaling \cite{platt1999probabilistic} to the LightGBM model's raw probability outputs, fitting a sigmoid function on the validation set. This reduces the validation Brier score from 0.186 to 0.116---a 38\% improvement---confirming that calibration is critical for models with scale\_pos\_weight adjustments. The Platt scaling parameters ($A=-6.236$, $B=4.443$) compress the raw probability range, with calibrated probabilities reaching a maximum of approximately 0.43.

\subsection{Fairness Evaluation Framework}

Fairness evaluation uses the fairlearn framework \cite{bird2020fairlearn} to compute disaggregated metrics across seven demographic dimensions (language, natural region, rurality, poverty quintile, sex, nationality, age group) and three intersections (language $\times$ rurality, language $\times$ poverty quintile, language $\times$ region). For each subgroup, we compute four metrics: false negative rate (FNR, the proportion of actual dropouts missed by the model), false positive rate (FPR, the proportion of non-dropouts incorrectly flagged), precision, and PR-AUC. All metrics are computed with survey weights. This framework operationalizes the ``slice analysis'' approach advocated by Baker and Hawn \cite{baker2022algorithmic} and aligns with the audit methodology of Saleiro et al. \cite{saleiro2018aequitas}.

The choice of FNR as a primary fairness metric reflects its direct operational interpretation: a high FNR means the system fails to identify students who will drop out. From an equity perspective, FNR disparities indicate which populations are systematically rendered invisible to the early warning system. We complement FNR with FPR to capture the surveillance--invisibility trade-off that Chouldechova's \cite{chouldechova2017fair} impossibility theorem predicts will arise when base rates differ across groups.

SHAP TreeExplainer \cite{lundberg2020local} provides feature-level interpretability, decomposing each prediction into additive feature contributions. We compute SHAP values on the raw (uncalibrated) LightGBM model, as TreeExplainer requires direct access to the tree structure. SHAP interaction values are computed on a 1,000-row subsample of the test set.

% =============================================================================
\section{Results}
% =============================================================================

\begin{table}[htbp]
  \caption{Model Performance Comparison Across Five Families (Survey-Weighted Metrics)}
  \label{tab:models}
  \input{tables/table_04_models.tex}
\end{table}

Near-identical PR-AUC across three tree-based ensembles (Table~\ref{tab:models}) confirms fairness findings reflect data structure, not model artifacts. MLP's lower PR-AUC (0.238) is typical for structured tabular data \cite{ke2017lightgbm}. The calibrated LightGBM achieves test PR-AUC of 0.236, with a validation--test gap of 0.023---well within the 0.07 threshold indicating adequate generalization. Calibration reduces the test Brier score by 40\% (0.186 to 0.112), confirming that Platt scaling is essential for probability-based decisions.

\begin{table}[htbp]
  \caption{Logistic Regression Coefficients (All 25 Features)}
  \label{tab:lr_coefficients}
  \input{tables/table_05_lr_coefficients.tex}
\end{table}

Indigenous language variables dominate the linear model (``other indigenous'' odds ratio = 2.20), contrasting sharply with the SHAP analysis of tree-based models in Section~6, where spatial-structural features dominate. This paradigm difference (zero overlap in top-5 features between linear and tree-based models) demonstrates that feature ``importance'' depends on model family.

Precision-Recall curves and calibration plots appear in Appendix~\ref{sec:appendix_figures}.

\subsection{Predictive Validity}
\label{sec:predictive_validity}

A model without discriminatory power cannot produce interpretable fairness metrics---high FNR everywhere is not a fairness finding, it is a model failure.

The calibrated LightGBM model achieves a test PR-AUC of 0.236 against a no-skill baseline of 0.134 (population dropout prevalence), yielding a 1.76$\times$ lift in discrimination. The top-scoring 10\% of students contains 34.2\% actual dropouts---a lift of 2.54$\times$ over the 13.4\% baseline (Figure~\ref{fig:calibration_decile}). This decile-level concentration of risk confirms that the model's predictions are meaningful, not random.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig08_calibration_decile.pdf}
  \caption{Calibration by prediction decile for the LightGBM calibrated model. Bars show predicted probability (blue) and observed dropout rate (orange) per decile. Mean absolute calibration error = 0.018, indicating well-calibrated predictions. Baseline dropout prevalence = 0.134 (dashed line).}
  \label{fig:calibration_decile}
\end{figure}

The model is well-calibrated across the score range (mean absolute calibration error = 0.018), meaning a predicted probability of 0.30 corresponds to approximately 30\% actual dropout in that decile (Figure~\ref{fig:calibration_decile}). The Brier Skill Score of 0.040 for the calibrated model is positive, confirming it outperforms the prevalence baseline. Among the uncalibrated models, LR, XGBoost, and RF have negative Brier Skill Scores due to scale\_pos\_weight distorting raw probabilities. MLP achieves a marginal positive BSS of 0.012 because its sigmoid output is not affected by scale\_pos\_weight; nevertheless, Platt scaling on LightGBM remains the only well-calibrated model we recommend for probability-based decisions.

The modest PR-AUC is itself informative: a model achieving lift primarily through geographic stratification will produce predictable fairness failures where spatial and demographic profiles diverge---precisely the pattern documented in Section~\ref{sec:fairness}. Low absolute PR-AUC does not invalidate differential FNR findings: a model can be modestly predictive overall while exhibiting systematic and substantial differences in prediction errors across demographic subgroups. Section~\ref{sec:fairness} documents those differences across five model families---robustness across architectures provides stronger evidence than any single model's absolute performance.

\subsection{Cross-Architecture Consistency}
\label{sec:cross_architecture}

Table~\ref{tab:crossmodel_fnr} extends the cross-architecture consistency check to five model families (LR, LightGBM, XGBoost, RF, MLP) using the FNR disparity that is central to our fairness findings. Across all five architectures, Castellano speakers consistently show higher FNR than Quechua and other-indigenous speakers---the rank order that defines our surveillance--invisibility finding is not an artifact of the LightGBM implementation.

\begin{table}[htbp]
  \caption{False Negative Rate by Language Group Across Five Model Families. Aimara group ($n=76$) shows instability (MLP FNR=0.830); cross-architecture consistency claim is scoped to the castellano vs.\ indigenous pattern.}
  \label{tab:crossmodel_fnr}
  \input{tables/table_10_crossmodel_fnr.tex}
\end{table}

The absolute FNR values vary across architectures---LR shows a narrower range (0.065--0.584) than LightGBM (0.216--0.633)---but the ordinal pattern is consistent: castellano FNR exceeds quechua FNR, which exceeds other-indigenous FNR in all five models. This consistency across architectures with different inductive biases (linear vs.\ gradient boosting vs.\ neural network) indicates that the disparity reflects data structure, not modeling artifacts. The MLP Aimara FNR of 0.830 is a small-sample outlier ($n=76$).

\subsection{Feature Ablation}
\label{sec:ablation}

\begin{table}[htbp]
  \caption{FNR by Language Group Under Feature Ablation. ``Individual only'' removes all 7 district-level spatial features; ``Spatial only'' removes all 18 individual/household features. Each variant uses its own optimal threshold (max weighted F1 on validation).}
  \label{tab:ablation}
  \input{tables/table_11_ablation.tex}
\end{table}

Table~\ref{tab:ablation} tests whether the FNR disparity is driven by spatial features, individual features, or their combination. Castellano speakers have the highest FNR in all three variants, confirming that their invisibility is not an artifact of a particular feature set. The spatial-only model dramatically reduces castellano FNR from 0.633 to 0.317---when the model can only use district-level features, it flags students in disadvantaged districts regardless of language, compressing the language-based FNR gap. Conversely, the individual-only model slightly increases castellano FNR (0.649) while halving indigenous FNR. The full model's combination of spatial and individual features produces the worst outcome for castellano speakers: spatial features provide the primary prediction signal, but individual features allow the model to differentiate within districts, effectively ``un-flagging'' castellano students whose individual profiles do not match the dropout risk pattern that spatial features establish.

% =============================================================================
\section{Fairness Analysis}
\label{sec:fairness}
% =============================================================================

\begin{table*}[htbp]
  \caption{Fairness Metrics by Language Group (LightGBM Calibrated, Test 2023). FNR column includes 95\% bootstrap confidence intervals (1,000 replicates). $p$-values from permutation tests (5,000 replicates) against the Castellano reference group.}
  \label{tab:fairness_language}
  \input{tables/table_06_fairness_language.tex}
\end{table*}

\subsection{Language Dimension: The Surveillance--Invisibility Axis}

Table~\ref{tab:fairness_language} reveals a fundamental FNR--FPR trade-off across language groups. The model achieves low FNR for indigenous-language speakers (0.22 for other indigenous languages) but at the cost of high FPR (0.52)---a pattern we term ``surveillance bias,'' where the system correctly identifies most indigenous-language dropouts but also incorrectly flags many non-dropouts. Conversely, Spanish speakers face high FNR (0.63) with low FPR (0.18)---``invisibility bias'' where the majority of actual dropouts are missed by the system. Bootstrap 95\% confidence intervals confirm that the gap between Castellano FNR (0.633 [0.608, 0.656]) and other-indigenous FNR (0.216 [0.137, 0.310]) is statistically reliable (permutation $p<0.001$), as are the Quechua disparities ($p<0.001$). The Aimara gap ($p=0.053$) is suggestive but marginal given $n=76$.

This inverse FNR-FPR relationship is not a model bug but the mathematical consequence of Chouldechova's~\cite{chouldechova2017fair} impossibility result applied to groups with different base rates. Indigenous-language speakers have higher baseline dropout rates, so a model trained to minimize overall prediction error will flag them more aggressively. The result is a systematic redistribution of prediction errors: indigenous communities bear the burden of false alarms (surveillance) while Spanish-speaking dropouts bear the burden of being missed (invisibility).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig03_fnr_fpr_language.pdf}
  \caption{FNR and FPR by language group. The inverse relationship between FNR and FPR reveals the surveillance--invisibility trade-off.}
  \label{fig:fnr_fpr}
\end{figure}

The inverse relationship between FNR and FPR across language groups (Figure~\ref{fig:fnr_fpr}) forms a clear axis: as FNR decreases, FPR increases, with indigenous-language groups clustered at the high-detection/high-surveillance end and Spanish speakers at the low-detection/low-surveillance end.

\subsection{Other Demographic Dimensions}

\emph{Region:} Selva and Sierra show lower FNR than Costa---the model detects rural and remote dropouts more effectively because these students match the spatial profile associated with dropout risk. A calibration gap exists: students predicted as ``high risk'' in the Selva have a 28.1\% actual dropout rate versus 38.9\% in the Sierra, meaning the same risk score carries different meaning across regions.

\emph{Poverty:} A monotonic relationship emerges across poverty quintiles---students in poorer quintiles are flagged more frequently and have higher base dropout rates. This alignment between base rates and flagging rates means poverty-based disparities are partially expected, though the magnitude of the FNR gap between the poorest and wealthiest quintiles warrants attention.

\emph{Sex:} The gender gap is minimal, with an FNR difference of only 0.026 between male and female students. Sex is not a major axis of disparity in this model, consistent with the relatively small gender gap in Peruvian school enrollment at the primary and secondary levels.

\emph{Nationality:} With only 27 non-Peruvian students in the test set, this dimension is unusable for reliable fairness inference. Any metrics on this sample are unreliable.

\emph{Age:} Older students (ages 15--17) are flagged more accurately than younger students (ages 6--11), reflecting both higher base dropout rates among older students and the model's heavy reliance on age as a predictive feature.

\subsection{Intersectional Analysis}

\begin{table}[htbp]
  \caption{Intersection Analysis: Language $\times$ Rurality}
  \label{tab:intersection}
  \input{tables/table_07_intersection.tex}
\end{table}

Table~\ref{tab:intersection} presents the intersection-level analysis (see also Figure~\ref{fig:fnr_heatmap} in Appendix~\ref{sec:appendix_figures}). Urban indigenous students face an FNR of 0.753---the model misses three out of four of their dropouts. This intersection group is invisible in both language-only analysis (where other-indigenous FNR is 0.22, driven by rural indigenous students) and geography-only analysis (where urban FNR is moderate). Only by crossing language and urbanicity does this extreme disparity emerge, demonstrating the intersectionality imperative articulated by Crenshaw~\cite{crenshaw1989demarginalizing} and operationalized computationally by Buolamwini and Gebru~\cite{buolamwini2018gender}.

The mechanism behind this disparity is interpretable: the model predicts dropout primarily through spatial-structural features---nightlight intensity, district historical dropout rates, census literacy rates---that code indigenous communities as rural. Urban indigenous students ``break the spatial profile'': they live in urban areas with higher nightlight intensity and lower district-level dropout rates, but face the same educational barriers (language, cultural mismatch, discrimination) as their rural counterparts. The model has no pathway to identify them because the features that capture indigenous disadvantage in rural settings do not activate in urban ones. Sample caveat: $n=89$ for urban other-indigenous students in the test set.

\subsection{SHAP Interpretability}

\begin{table}[htbp]
  \caption{SHAP Feature Importance (Top 15)}
  \label{tab:shap}
  \input{tables/table_08_shap.tex}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig06_shap_bar.pdf}
  \caption{Mean absolute SHAP values for the top 15 features. Age and spatial-structural features dominate, while identity features (language, sex) have minimal direct importance.}
  \label{fig:shap_bar}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig07_shap_beeswarm.pdf}
  \caption{SHAP beeswarm plot showing feature value distributions and their impact on predictions. Red indicates high feature values; blue indicates low values.}
  \label{fig:shap_beeswarm}
\end{figure}

Table~\ref{tab:shap} and Figures~\ref{fig:shap_bar}--\ref{fig:shap_beeswarm} reveal how the model makes predictions, directly addressing RQ2. The top five SHAP features---age, nightlight z-score, working status, census literacy z-score, and poverty index z-score---are all spatial-structural variables. Identity features contribute minimally: the sex indicator (es\_mujer) ranks 16th out of 25 features with a mean absolute SHAP value of only 0.003, and the nationality indicator (es\_peruano) ranks 25th with effectively zero contribution, consistent with the $n=27$ non-Peruvian sample producing no learnable signal.

The top 5 SHAP features have zero overlap with the top 5 logistic regression features (which are dominated by indigenous language dummies). This paradigm difference reflects how tree-based models route predictions differently from linear models: where logistic regression must assign large coefficients to identity features to capture group-level differences, LightGBM can achieve similar discrimination through the continuous spatial-structural features that correlate with those identity categories. The fairness implications are significant: the model encodes structural inequities without using identity features directly. Removing protected attributes from the feature set would not mitigate the disparities documented above, because the model already operates through proxy features that carry the same information.

% =============================================================================
\section{Discussion}
% =============================================================================

\subsection{The Spatial Proxy Mechanism}

SHAP analysis reveals that the model uses geography as a proxy for demographic risk---nightlight intensity, district-level dropout rates, and census literacy rates collectively encode the spatial concentration of disadvantage. This creates systematic blind spots for populations that do not match spatial stereotypes. Urban indigenous students exemplify this failure: they reside in urban areas with favorable spatial indicators yet face educational barriers comparable to their rural counterparts. The model has no pathway to identify their risk because the features that capture indigenous disadvantage in rural settings do not activate in urban ones. This resonates with Perdomo et al.'s~\cite{perdomo2023difficult} argument that structural features predict dropout well---and extends it by showing that reliance on structural features creates predictable fairness failures at demographic intersections.

\subsection{Considerations for EWS Operators}

Our findings raise several considerations for operators of educational early warning systems:

\begin{itemize}
  \item Group-specific threshold adjustment could equalize FNR across language groups, reducing invisibility bias for Spanish speakers without necessarily increasing overall error. However, threshold adjustment redistributes errors rather than eliminating them---equalizing FNR would increase FPR for Spanish speakers---and the appropriate trade-off depends on the relative costs of missed dropouts versus false alarms in specific operational contexts.
  \item Supplementary identification mechanisms for urban indigenous students could address the intersection-level blind spot our analysis reveals. However, designing such mechanisms without creating additional surveillance of already-marginalized communities requires careful consideration of community perspectives and consent \cite{mcmahon2020reenvisioning}.
  \item Regular fairness auditing, conducted across multiple demographic dimensions and their intersections, could detect disparities before they become entrenched. The question of who should conduct such audits---system operators, independent researchers, affected communities, or regulatory bodies---remains open.
\end{itemize}

\subsection{Generalizability}

Our findings likely apply to similar EWS systems across Latin America and other developing regions that use spatial features for dropout prediction. The surveillance--invisibility dynamic may emerge wherever prediction models operate on populations with heterogeneous base rates and correlated spatial-demographic structure \cite{adelman2018predicting,villegas2023supporting}. Whether the specific intersection-level failures we document (urban indigenous invisibility) generalize depends on the degree to which indigenous populations in other countries exhibit similar rural-urban migration patterns.

\subsection{Normative Fairness Considerations}

Our analysis privileges FNR as the primary fairness metric because a missed dropout represents an irreversible harm: a student who leaves school without intervention faces compounding disadvantage that early warning systems exist to prevent. However, equalizing FNR across language groups would require lowering the classification threshold for Spanish speakers, substantially increasing their FPR---more non-dropout students flagged, more intervention resources consumed on false alarms. The appropriate trade-off depends on what happens after flagging. If the intervention is low-cost (an automated phone call or teacher notification), elevated FPR is tolerable and FNR equalization is the right objective. If the intervention is high-cost (home visits, social worker assignment), the resource burden of false positives may be prohibitive \cite{mcmahon2020reenvisioning}. We do not resolve this tension---the right criterion depends on operational context we cannot observe as external auditors. What we establish is that the current model implicitly prioritizes low FPR for the majority (Spanish speakers) at the cost of rendering their dropouts invisible \cite{chouldechova2017fair}.

% =============================================================================
\section{Limitations}
% =============================================================================

This paper audits a proxy model, not the actual Alerta Escuela system. The feature sets differ substantially (Table~\ref{tab:enaho_siagie}): ENAHO provides demographic and household variables while SIAGIE contains attendance and grade records. Our findings demonstrate what disparities \emph{can} emerge from survey-derived prediction, not what the deployed system produces.

Second, ENAHO's mother tongue variable (P300) captures language by self-report. Bilingual speakers may report Spanish as their mother tongue, potentially undercounting indigenous-language prevalence and understating the disparities we document. The true magnitude of language-based prediction disparities may be larger than our estimates.

Third, the 2020 wave is affected by the COVID-19 pandemic. INEI conducted phone interviews rather than in-person household visits, producing a reduced sample (approximately 13,755 observations versus approximately 25,000 in typical years) with 52\% null values in the education attendance variable. While we include 2020 in the training data after dropping null records, this year may not represent the same population as in-person survey years.

Fourth, some intersectional subgroups have small samples: $n=89$ for urban other-indigenous students in the 2023 test set and $n=27$ for non-Peruvian nationality (rendering this dimension unusable for reliable inference). Pooling validation and test data yields $n=167$ urban other-indigenous students with a FNR point estimate of 0.69, but the 95\% CI remains wide [0.39, 0.98]. A power analysis quantifies this ceiling: confirming FNR $>$ 0.50 at 80\% power requires 46 dropout observations, approximately 8 ENAHO survey years at the current rate of $\sim$6 urban other-indigenous dropouts per year. Detecting the FNR gap between this group and castellano speakers requires 192 dropout observations ($\sim$32 years). This is not merely a sample size limitation but a methodological ceiling: survey-based intersectional fairness auditing fundamentally cannot produce statistically significant results for subgroups contributing fewer than $\sim$6 positive observations per survey year. This finding argues directly for opening administrative data (SIAGIE), which covers the full student population and could provide the statistical power that survey data cannot.

Fifth, while we incorporate FACTOR07 survey weights throughout training and evaluation, the theoretical properties of survey-weighted gradient boosting are not fully established. MacNell et al.~\cite{macnell2023implementing} found that ignoring survey weights in gradient boosting can affect both prediction accuracy and feature importance rankings, supporting our decision to incorporate weights, but the formal statistical guarantees of weighted ML estimators under complex survey designs remain an active area of research.

% =============================================================================
\section{Ethical Considerations}
% =============================================================================

\paragraph{Positionality.} The author is Peruvian, a computer science self-learner at Universidad de Ingenier\'{i}a y Tecnolog\'{i}a (UTEC), and co-founder of Genera, an educational technology startup. This positionality shapes the work in important ways: as someone who believes AI can address education's one-size-fits-all problem, I undertook this audit precisely because systems deployed without scrutiny risk deepening the inequities I aim to address. I am not from the indigenous communities most affected by the disparities documented here, and this work should be understood as an outsider's technical audit rather than a community-centered assessment. A more complete evaluation would incorporate the perspectives of indigenous educators, families, and students directly affected by early warning systems.

\paragraph{Generative AI Disclosure.} Portions of the data pipeline code and manuscript preparation were assisted by generative AI tools (Claude, Anthropic). The author was responsible for all research design, methodological decisions, result interpretation, and substantive writing. AI assistance was used for code implementation and editorial refinement.

\paragraph{Data Ethics.} This study uses publicly available, de-identified survey data (ENAHO) released by INEI for research purposes. No individual students can be identified from the analysis, and no direct human subjects interaction was involved. The analysis operates exclusively on aggregate patterns in survey-weighted data.

% =============================================================================
\section{Conclusion}
% =============================================================================

This proxy equity audit of survey-derived dropout risk in Peru ($N=150{,}135$; 2018--2023) identifies Spanish-speaking students---the demographic majority---as the group most systematically missed by the model (FNR = 0.633), with suggestive evidence that urban indigenous students face even higher miss rates at the intersection of language and geography (pooled FNR = 0.69, $n=167$). These findings hold across five model families with different architectures, indicating that the disparity reflects data structure rather than algorithmic artifacts.

The proxy audit methodology demonstrates that independent algorithmic accountability is achievable using only publicly available survey data---an approach applicable wherever direct system access is unavailable. However, models that predict primarily through spatial-structural features create predictable blind spots at demographic intersections where geographic and social profiles diverge. Feature ablation confirms that spatial features drive the castellano invisibility pattern, while the full model's combination of spatial and individual features produces the worst FNR outcome for the majority group.

Our power analysis reveals a methodological ceiling: survey data fundamentally cannot produce statistically significant intersectional fairness results for subgroups contributing fewer than $\sim$6 positive observations per year. Opening SIAGIE administrative data would enable both direct system evaluation and the statistical power that intersectional auditing demands. As educational early warning systems proliferate globally, the question of who decides the appropriate fairness trade-off---and who audits the systems making that trade-off---remains open.

% =============================================================================
\appendix
\section{Supplementary Figures}
\label{sec:appendix_figures}
% =============================================================================

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig01_pr_curves.pdf}
  \caption{Precision-Recall curves for three of the five model families on the 2022 validation set. LightGBM and XGBoost curves largely overlap; RF and MLP curves are omitted for visual clarity (see Table~\ref{tab:models} for all five).}
  \label{fig:pr_curves}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig02_calibration.pdf}
  \caption{Calibration plot comparing uncalibrated and Platt-calibrated LightGBM probabilities. Platt scaling reduces test Brier score by 40\%.}
  \label{fig:calibration}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig05_fnr_heatmap.pdf}
  \caption{FNR heatmap by language and rurality intersection. The darkest cell (other indigenous, urban) represents the group most missed by the model.}
  \label{fig:fnr_heatmap}
\end{figure}

% =============================================================================
% Bibliography
% =============================================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
