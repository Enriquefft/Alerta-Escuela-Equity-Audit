---
phase: 10-cross-validation-admin-data
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/fairness/cross_validation.py
  - tests/gates/test_gate_3_3.py
  - data/exports/choropleth.json
autonomous: false

must_haves:
  truths:
    - "Pearson correlation between district-level model predictions and admin dropout rates is positive and p < 0.05"
    - "Mean absolute prediction error quantified separately for high-indigenous (>50%) and low-indigenous (<10%) districts"
    - "choropleth.json contains >1,500 districts each with ubigeo, predicted_dropout_rate, admin_dropout_rate, model_error, indigenous_language_pct, poverty_index"
    - "Gate test 3.3 passes with correlation, p-value, and MAE by indigenous group printed for human review"
  artifacts:
    - path: "src/fairness/cross_validation.py"
      provides: "District-level cross-validation pipeline"
      min_lines: 120
    - path: "tests/gates/test_gate_3_3.py"
      provides: "Gate test 3.3 validation"
      min_lines: 80
    - path: "data/exports/choropleth.json"
      provides: "Choropleth data for M4 site"
      contains: "districts"
  key_links:
    - from: "src/fairness/cross_validation.py"
      to: "data/processed/model_lgbm_calibrated.joblib"
      via: "joblib.load for calibrated model scoring"
      pattern: "joblib\\.load.*calibrated"
    - from: "src/fairness/cross_validation.py"
      to: "data/processed/enaho_with_features.parquet"
      via: "polars read_parquet for full feature matrix"
      pattern: "read_parquet.*enaho_with_features"
    - from: "src/fairness/cross_validation.py"
      to: "data/exports/choropleth.json"
      via: "json.dump export"
      pattern: "json\\.dump"
    - from: "tests/gates/test_gate_3_3.py"
      to: "data/exports/choropleth.json"
      via: "json.load for validation"
      pattern: "json\\.load"
---

<objective>
Aggregate calibrated LightGBM predictions to district level, correlate with administrative dropout rates, quantify prediction error by indigenous language prevalence, and export choropleth.json for the M4 scrollytelling site.

Purpose: Cross-validates the model's spatial predictions against external admin data, revealing whether the model systematically over/under-predicts in indigenous-majority districts -- a key equity finding.
Output: `src/fairness/cross_validation.py`, `data/exports/choropleth.json`, `tests/gates/test_gate_3_3.py`
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-cross-validation-admin-data/10-RESEARCH.md
@src/fairness/shap_analysis.py
@src/fairness/metrics.py
@src/data/features.py
@src/data/admin.py
@src/data/census.py
@tests/gates/test_gate_3_2.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: District-level cross-validation pipeline + choropleth export</name>
  <files>src/fairness/cross_validation.py, data/exports/choropleth.json</files>
  <action>
Create `src/fairness/cross_validation.py` with a `run_cross_validation_pipeline()` function following the same pattern as `shap_analysis.py` and `metrics.py` (standalone script with `if __name__ == "__main__"` entry point, `sys.path.insert` for imports, `find_project_root()` for paths).

The pipeline must perform these steps in order:

**Step 1 -- Load model + full dataset:**
- Load calibrated model from `data/processed/model_lgbm_calibrated.joblib` via joblib.
- Load full feature matrix from `data/processed/enaho_with_features.parquet` via polars.
- Extract the 25 features using `MODEL_FEATURES` from `data.features` and the meta columns UBIGEO, FACTOR07, year, dropout.

**Step 2 -- Score ALL 150,135 rows:**
- Extract X = feat.select(MODEL_FEATURES).to_pandas().to_numpy() (same pattern as baseline.py/calibration.py).
- Score: `cal_probs = cal_model.predict_proba(X)[:, 1]`.
- Add `prob_dropout` column to the polars DataFrame.

**Step 3 -- Aggregate to district level (FACTOR07-weighted):**
- Group by UBIGEO, compute: n_students (count), total_weight (sum FACTOR07), predicted_dropout_rate (weighted mean = sum(prob_dropout * FACTOR07) / sum(FACTOR07) * 100 to convert to percentage), actual_dropout_rate_enaho (weighted mean of dropout boolean * 100).

**Step 4 -- Load admin + census data and join:**
- Load admin data via `load_admin_dropout_rates()` from `data.admin`. Extract the DataFrame with UBIGEO, admin_primaria_rate, admin_secundaria_rate.
- Compute combined admin rate: average of primaria and secundaria where both exist; whichever is available where one is null. Store as `admin_dropout_rate`.
- Load census data via `load_census_2017()` from `data.census`. Extract DataFrame with UBIGEO, indigenous_lang_pct (rename census prefix to match export field name), poverty_rate.
- LEFT JOIN admin onto district predictions on UBIGEO.
- LEFT JOIN census onto the result on UBIGEO.
- Also do a FULL OUTER JOIN variant to capture 350 admin-only districts (those without ENAHO data) for the complete choropleth. These districts will have null predicted_dropout_rate but valid admin_dropout_rate.

**Step 5 -- Compute Pearson correlation (test_2023 only for purity):**
- For the correlation test, filter to districts present in test year 2023 only. Score only the 2023-year rows through the model, aggregate those separately to get "out-of-sample" district predictions. Then compute Pearson r using `scipy.stats.pearsonr` between predicted and admin rates.
- IMPORTANT: Both rates must be in same units (percentage, 0-100). Model predictions are 0-1, multiply by 100.
- Filter to districts where BOTH predicted and admin rates are non-null.
- Print: Pearson r, p-value, N districts used.
- Document caveat: model uses district_dropout_rate_admin_z as a feature (SHAP ~0.03), so partial mechanical correlation expected.

**Step 6 -- Stratified error analysis:**
- Compute model_error = predicted_dropout_rate - admin_dropout_rate for each district.
- Split into high_indigenous (indigenous_lang_pct > 50) and low_indigenous (indigenous_lang_pct < 10).
- Compute MAE for each group.
- Print: N districts per group, MAE per group, difference.

**Step 7 -- Export choropleth.json:**
- Build JSON structure with metadata:
  - `generated_at`: UTC ISO timestamp
  - `n_districts`: total count (should be ~1,890)
  - `n_with_predictions`: count with non-null predicted_dropout_rate (~1,540)
  - `correlation`: { pearson_r, p_value, n_districts_correlated, caveat: "Model uses district_dropout_rate_admin_z as feature (SHAP importance ~0.03)" }
  - `error_by_indigenous_group`: { high_indigenous_gt50: { n_districts, mean_absolute_error }, low_indigenous_lt10: { n_districts, mean_absolute_error } }
  - `districts`: array of records, each with: ubigeo, predicted_dropout_rate, admin_dropout_rate, model_error, indigenous_language_pct, poverty_index
- Round all floats to 4 decimal places.
- Replace NaN with None (serializes to JSON null). Use a helper like `def _safe(v): return None if v is None or (isinstance(v, float) and math.isnan(v)) else round(v, 4)`.
- Write to `data/exports/choropleth.json` with `json.dump(data, f, indent=2, ensure_ascii=False)`.

**Step 8 -- Console summary for human review:**
- Print formatted summary: Pearson r, p-value, N districts, high-indigenous MAE, low-indigenous MAE, total districts in choropleth, districts with predictions, districts without predictions.

**Key implementation details:**
- Use `import math` for `math.isnan` checks.
- Use `from scipy.stats import pearsonr`.
- Follow the established `sys.path.insert(0, str(Path(__file__).resolve().parents[1]))` pattern.
- Use `matplotlib.use("Agg")` if generating any scatter plot (optional: a scatter of predicted vs admin rates saves to `data/exports/figures/cross_validation_scatter.png`).
- For the poverty_index field in choropleth: use district-level mean poverty_index from enaho_with_features.parquet (the META_COLUMNS includes poverty_index).
  </action>
  <verify>
Run `uv run python src/fairness/cross_validation.py` from project root. Verify:
1. No errors during execution.
2. Console prints Pearson r (positive), p-value (< 0.05), district counts.
3. `data/exports/choropleth.json` exists, is valid JSON.
4. `python -c "import json; d=json.load(open('data/exports/choropleth.json')); print(d['n_districts'], d['correlation'])"` shows >1,500 districts and positive correlation.
  </verify>
  <done>
- `src/fairness/cross_validation.py` exists with `run_cross_validation_pipeline()` function.
- `data/exports/choropleth.json` has >1,500 districts with all 6 required fields per district.
- Pearson r is positive and p < 0.05.
- MAE computed for high-indigenous and low-indigenous district groups.
  </done>
</task>

<task type="auto">
  <name>Task 2: Gate test 3.3 + human review print</name>
  <files>tests/gates/test_gate_3_3.py</files>
  <action>
Create `tests/gates/test_gate_3_3.py` following the exact pattern of `test_gate_3_2.py` (module docstring, sys.path.insert, find_project_root, fixture loading choropleth.json, individual test functions, human-review print section at end).

**Fixture:**
- `choropleth_data()`: Load and parse `data/exports/choropleth.json`. Assert file exists with helpful error message pointing to the pipeline command.

**Tests (each is a separate function):**

1. `test_json_exists_and_valid` -- choropleth.json exists, is valid JSON, has required top-level keys: generated_at, n_districts, n_with_predictions, correlation, error_by_indigenous_group, districts.

2. `test_district_count_exceeds_1500` -- `n_districts > 1500`. Print actual count.

3. `test_district_required_fields` -- Sample first 20 districts, assert each has keys: ubigeo, predicted_dropout_rate, admin_dropout_rate, model_error, indigenous_language_pct, poverty_index.

4. `test_positive_correlation` -- `correlation.pearson_r > 0`. Print actual r value.

5. `test_correlation_significant` -- `correlation.p_value < 0.05`. Print actual p-value.

6. `test_error_by_indigenous_group_exists` -- Both `high_indigenous_gt50` and `low_indigenous_lt10` keys exist with `n_districts` > 0 and `mean_absolute_error` >= 0.

7. `test_predictions_coverage` -- `n_with_predictions >= 1500`. Print actual count.

8. `test_ubigeo_format` -- All ubigeo values are 6-character strings (sample 50 districts).

9. `test_no_nan_in_json` -- Read raw JSON text, assert "NaN" does not appear (NaN is invalid JSON; should be null).

**Human review print section (at bottom, runs last):**
- `test_human_review_summary` -- Prints formatted table:
  - Pearson r and p-value
  - N districts correlated
  - High-indigenous MAE and N districts
  - Low-indigenous MAE and N districts
  - Total districts in choropleth
  - Districts with predictions vs without
  - Sample of 5 districts with highest model_error (overprediction)
  - Sample of 5 districts with lowest model_error (underprediction)
  - The caveat about admin feature leakage
  - A clear "HUMAN REVIEW: Do indigenous-majority districts show higher prediction error?" prompt
  </action>
  <verify>
Run `uv run pytest tests/gates/test_gate_3_3.py -v -s` from project root. All tests pass. Human review summary prints clearly.
  </verify>
  <done>
- Gate test 3.3 has 9+ assertion tests plus human review summary.
- All tests pass when run against the generated choropleth.json.
- Human review section clearly presents correlation, MAE by group, and extreme-error districts.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>District-level cross-validation pipeline that scores all 150,135 ENAHO rows through the calibrated LightGBM model, aggregates to 1,540+ districts, correlates with admin dropout rates (Pearson r), quantifies prediction error by indigenous language prevalence, and exports choropleth.json with 1,890 districts.</what-built>
  <how-to-verify>
1. Review the gate test output (already printed with -s flag).
2. Check: Is the Pearson r positive? Is p < 0.05?
3. Check: Do high-indigenous districts (>50%) have higher MAE than low-indigenous (<10%)?
4. Review the 5 highest-error and 5 lowest-error districts -- do they reveal a spatial pattern?
5. Note: Correlation is partially mechanical because the model uses district_dropout_rate_admin_z as a feature (SHAP ~0.03). This caveat is documented.
6. Note: Admin data is synthetic (generated in Phase 3). Findings should be interpreted accordingly.
  </how-to-verify>
  <resume-signal>Type "approved" to continue to Phase 11, or describe issues to address.</resume-signal>
</task>

</tasks>

<verification>
1. `uv run python src/fairness/cross_validation.py` completes without error
2. `uv run pytest tests/gates/test_gate_3_3.py -v -s` -- all tests pass
3. `data/exports/choropleth.json` is valid JSON with >1,500 districts
4. Pearson r is positive with p < 0.05
5. MAE reported for both indigenous groups
</verification>

<success_criteria>
- XVAL-01: District-level predictions correlated with admin rates (Pearson r positive, p < 0.05)
- XVAL-02: Prediction error compared between high-indigenous and low-indigenous districts
- XVAL-03: choropleth.json exported with >1,500 districts matching M4 schema
- Gate test 3.3: All assertions pass; human reviews spatial equity patterns
</success_criteria>

<output>
After completion, create `.planning/phases/10-cross-validation-admin-data/10-01-SUMMARY.md`
</output>
