---
phase: 07-calibration-onnx-export-final-test
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/calibration.py
  - tests/gates/test_gate_2_3.py
  - data/exports/model_results.json
  - data/exports/onnx/lightgbm_dropout.onnx
  - data/exports/figures/calibration_plot.png
  - data/processed/model_lgbm_calibrated.joblib
  - data/processed/predictions_lgbm_calibrated.parquet
autonomous: false

must_haves:
  truths:
    - "Calibrated LightGBM has lower Brier score than uncalibrated on validation set"
    - "ONNX file predictions match Python model within 1e-4 on 100 random samples"
    - "Test set (2023) PR-AUC is within 0.07 of validation PR-AUC"
    - "model_results.json contains test_2023_final and test_2023_calibrated entries"
    - "Comparison table with Alerta Escuela published metrics is printed for human review"
  artifacts:
    - path: "src/models/calibration.py"
      provides: "Calibration + ONNX export + final test pipeline"
      min_lines: 200
    - path: "tests/gates/test_gate_2_3.py"
      provides: "Gate test 2.3 assertions"
      min_lines: 60
    - path: "data/exports/onnx/lightgbm_dropout.onnx"
      provides: "ONNX model for browser inference"
    - path: "data/exports/figures/calibration_plot.png"
      provides: "Calibration reliability diagram"
    - path: "data/processed/model_lgbm_calibrated.joblib"
      provides: "Persisted calibrated model"
  key_links:
    - from: "src/models/calibration.py"
      to: "data/processed/model_lgbm.joblib"
      via: "joblib.load"
      pattern: "joblib\\.load.*model_lgbm"
    - from: "src/models/calibration.py"
      to: "sklearn.frozen.FrozenEstimator"
      via: "FrozenEstimator wrapping LightGBM"
      pattern: "FrozenEstimator"
    - from: "src/models/calibration.py"
      to: "onnxmltools.convert.convert_lightgbm"
      via: "ONNX conversion"
      pattern: "convert_lightgbm"
    - from: "src/models/calibration.py"
      to: "data/exports/model_results.json"
      via: "JSON merge with existing entries"
      pattern: "model_results\\.json"
---

<objective>
Calibrate the LightGBM model with Platt scaling, export to ONNX for browser inference,
and evaluate on the 2023 test set exactly once -- the only time test data is touched
for any decision-making.

Purpose: scale_pos_weight=4.80 distorts raw probabilities. Calibration produces meaningful
risk scores for downstream fairness analysis (Phase 8) and SHAP (Phase 9). ONNX export
enables browser inference in the M4 scrollytelling site.

Output: calibration.py pipeline, calibrated model, ONNX file, calibration plot,
updated model_results.json with test_2023_final + test_2023_calibrated + Alerta Escuela
comparison, gate test 2.3, calibrated predictions parquet.
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-calibration-onnx-export-final-test/07-RESEARCH.md
@.planning/phases/06-lightgbm-xgboost/06-01-SUMMARY.md
@src/models/baseline.py
@src/models/lightgbm_xgboost.py
@data/exports/model_results.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create calibration + ONNX export + final test pipeline</name>
  <files>
    src/models/calibration.py
    data/exports/model_results.json
    data/exports/onnx/lightgbm_dropout.onnx
    data/exports/figures/calibration_plot.png
    data/processed/model_lgbm_calibrated.joblib
    data/processed/predictions_lgbm_calibrated.parquet
  </files>
  <action>
Create `src/models/calibration.py` following the patterns established in `baseline.py` and
`lightgbm_xgboost.py`. The module is a single pipeline script run via
`uv run python src/models/calibration.py`.

**Imports and setup:**
- Same sys.path pattern as baseline.py (insert parents[1], import from data.features, models.baseline, utils)
- Import `create_temporal_splits`, `_df_to_numpy`, `compute_metrics`, `_threshold_analysis` from baseline
- Import `FrozenEstimator` from `sklearn.frozen` (NOT cv='prefit' -- removed in sklearn 1.8.0)
- Import `CalibratedClassifierCV` from `sklearn.calibration`
- Import `CalibrationDisplay` from `sklearn.calibration`
- Import `brier_score_loss` from `sklearn.metrics`
- Import `convert_lightgbm` from `onnxmltools.convert`
- Import `FloatTensorType` from `onnxmltools.convert.common.data_types`
- Import `onnx` and `onnxruntime as rt`
- Use `matplotlib.use("Agg")` before importing pyplot (same as prior phases)

**Pipeline function `run_calibration_pipeline()` with these steps IN ORDER:**

**Step 1 -- Load model and data:**
- Load `model_lgbm.joblib` via joblib
- Load `enaho_with_features.parquet` via polars
- Create temporal splits using `create_temporal_splits(df)`
- Convert to numpy using `_df_to_numpy()` for val_df and test_df only (no training needed)

**Step 2 -- Uncalibrated validation baseline:**
- `uncal_proba_val = lgbm.predict_proba(X_val)[:, 1]`
- `brier_uncal = brier_score_loss(y_val, uncal_proba_val, sample_weight=w_val)`
- Print uncalibrated Brier score (~0.2115 expected)

**Step 3 -- Calibrate with Platt scaling on validation set:**
- Wrap model: `frozen = FrozenEstimator(lgbm)`
- Create calibrator: `cal_model = CalibratedClassifierCV(frozen, method="sigmoid")`
- Fit on validation: `cal_model.fit(X_val, y_val, sample_weight=w_val)`
- The FrozenEstimator sample_weight warning is BENIGN -- weights flow correctly to the calibration LR.
  Suppress the specific warning with `warnings.filterwarnings("ignore", message=".*FrozenEstimator.*sample_weight.*")`.
- Get calibrated validation probabilities: `cal_proba_val = cal_model.predict_proba(X_val)[:, 1]`
- `brier_cal = brier_score_loss(y_val, cal_proba_val, sample_weight=w_val)`
- Assert `brier_cal < brier_uncal` (calibration MUST improve Brier score)
- Print improvement: `f"Brier improvement: {brier_uncal:.4f} -> {brier_cal:.4f} ({(1 - brier_cal/brier_uncal)*100:.1f}% reduction)"`

**Step 4 -- Extract Platt parameters for browser-side calibration:**
- `calibrator = cal_model.calibrated_classifiers_[0].calibrators[0]`
- `platt_a = float(calibrator.a_)` (slope, expected ~-5.28)
- `platt_b = float(calibrator.b_)` (intercept, expected ~4.28)
- Print Platt A and B with JS formula: `calibrated = 1 / (1 + Math.exp(A * raw_prob + B))`

**Step 5 -- ONNX export of RAW LightGBM (NOT calibrated wrapper):**
- `initial_types = [("input", FloatTensorType([None, 25]))]` (25 features)
- `onnx_model = convert_lightgbm(lgbm, initial_types=initial_types, zipmap=False)`
- Create directory: `onnx_path.parent.mkdir(parents=True, exist_ok=True)`
- Save: `onnx.save(onnx_model, str(onnx_path))` where `onnx_path = root / "data/exports/onnx/lightgbm_dropout.onnx"`
- Assert size < 50 MB: `onnx_path.stat().st_size < 50 * 1024 * 1024`
- Print size in MB

**Step 6 -- ONNX prediction validation:**
- Create `onnxruntime.InferenceSession` with `CPUExecutionProvider`
- Sample 100 random rows from X_val using `np.random.default_rng(42)`
- Cast to float32: `X_sample.astype(np.float32)` (ONNX requires float32)
- Run inference: `sess.run(None, {input_name: X_float32})`
- Extract probabilities from `result[1][:, 1]` (zipmap=False gives tensor output)
- Compare with Python: `max_diff = np.max(np.abs(py_proba - onnx_proba))`
- Assert `max_diff < 1e-4` (relaxed tolerance for float32)
- Print max absolute difference

**Step 7 -- FINAL TEST EVALUATION (EXACTLY ONCE):**
This is the ONLY place test data is used for metrics. Do NOT evaluate test anywhere before this step.

- Get uncalibrated test predictions: `uncal_proba_test = lgbm.predict_proba(X_test)[:, 1]`
- Get calibrated test predictions: `cal_proba_test = cal_model.predict_proba(X_test)[:, 1]`
- Threshold analysis on VALIDATION (not test): reuse optimal_threshold from `_threshold_analysis(y_val, cal_proba_val, w_val)`
- Apply optimal threshold to both uncalibrated and calibrated test predictions
- Compute full metric suite (weighted + unweighted) for:
  - `test_2023_final`: uncalibrated LightGBM on test (using `compute_metrics()`)
  - `test_2023_calibrated`: calibrated LightGBM on test (using `compute_metrics()`)
- Check val-test gap: `abs(val_prauc - test_prauc) < 0.07` for calibrated model
- Print metrics side-by-side (uncalibrated vs calibrated on test)
- Print val-test PR-AUC gap

**Step 8 -- Calibration reliability diagram:**
- Use `CalibrationDisplay.from_predictions()` to plot both uncalibrated and calibrated on same axes
- Use n_bins=10, strategy="uniform"
- Title: "Calibration Curve: LightGBM Before vs After Platt Scaling (Validation 2022)"
- Save to `data/exports/figures/calibration_plot.png` at 150 dpi
- Follow matplotlib patterns from prior phases (Agg backend, tight_layout, plt.close)

**Step 9 -- Alerta Escuela comparison table:**
Print a clearly-formatted comparison table for human review:
```
=== ALERTA ESCUELA vs EQUITY AUDIT COMPARISON ===
Metric                 Alerta Escuela       Equity Audit (calibrated)
Algorithm              LightGBM             LightGBM (Optuna)
Data Source            SIAGIE admin          ENAHO survey
ROC-AUC                0.84-0.89            {test_roc_auc:.4f}
FNR                    57-64%               {test_fnr:.1f}%
PR-AUC                 Not published        {test_prauc:.4f}
Features               31                   25
Calibration            Not reported         Platt scaling ({brier_improvement:.1f}% Brier reduction)
Fairness Analysis      None published       6 dimensions + 3 intersections (Phase 8)
```
Note: ROC-AUC is not directly comparable due to different data sources and base rates (~14% survey vs ~2% admin). FNR comparison is more meaningful.

**Step 10 -- Save calibrated predictions parquet:**
- Build predictions DataFrame for test set with calibrated probabilities
- Include ID columns (CONGLOME, VIVIENDA, HOGAR, CODPERSO, year, UBIGEO, FACTOR07, dropout)
- Add columns: prob_dropout (calibrated), prob_dropout_uncalibrated (raw), pred_dropout, model="lightgbm_calibrated", threshold, split="test_2023"
- Also include validation set predictions (calibrated)
- Save to `data/processed/predictions_lgbm_calibrated.parquet`

**Step 11 -- Persist calibrated model:**
- `joblib.dump(cal_model, root / "data/processed/model_lgbm_calibrated.joblib")`

**Step 12 -- Update model_results.json:**
- Load existing model_results.json (has logistic_regression, lightgbm, xgboost entries)
- Add `test_2023_final` entry under lightgbm key (uncalibrated test metrics)
- Add `test_2023_calibrated` top-level key with:
  - metadata: calibration_method="platt_sigmoid", platt_a, platt_b, brier_uncalibrated, brier_calibrated, brier_improvement_pct, onnx_path, onnx_size_bytes, onnx_max_abs_diff, optimal_threshold (from calibrated val), year_note="Test year is 2023 (ENAHO 2024 unavailable)"
  - metrics: validate_2022 (calibrated weighted + unweighted), test_2023 (calibrated weighted + unweighted)
  - threshold_analysis: from calibrated validation predictions
  - alerta_escuela_comparison: { model, algorithm, data_source, roc_auc_range, fnr_range, features, fairness_analysis, notes, comparison_caveats }
- Write back with `json.dump(model_results, f, indent=2)`

**Step 13 -- Final summary:**
Print a summary block matching the pattern from prior phases.

**`if __name__ == "__main__":` block** to run `run_calibration_pipeline()`.

After creating the file, run: `uv run python src/models/calibration.py`
Verify it completes without errors and prints the expected output.
  </action>
  <verify>
`uv run python src/models/calibration.py` completes without errors.
Output includes:
- Brier improvement line showing reduction (~45%)
- Platt A and B parameters
- ONNX size < 50 MB
- ONNX max_diff < 1e-4
- Val-test PR-AUC gap < 0.07
- Alerta Escuela comparison table
- Final summary block
Verify: `python -c "import json; d=json.load(open('data/exports/model_results.json')); assert 'test_2023_calibrated' in d; print('OK')"` from project root.
Verify: `ls -la data/exports/onnx/lightgbm_dropout.onnx` shows file exists and is < 50 MB.
Verify: `ls -la data/exports/figures/calibration_plot.png` shows file exists.
  </verify>
  <done>
- calibration.py pipeline runs end-to-end producing all artifacts
- Brier score (calibrated) < Brier score (uncalibrated) on validation
- ONNX file at data/exports/onnx/lightgbm_dropout.onnx, < 50 MB, predictions match within 1e-4
- model_results.json has test_2023_calibrated entry with Platt params and Alerta Escuela comparison
- Calibration plot saved
- Test set touched exactly once
  </done>
</task>

<task type="auto">
  <name>Task 2: Create gate test 2.3</name>
  <files>tests/gates/test_gate_2_3.py</files>
  <action>
Create `tests/gates/test_gate_2_3.py` following the pattern of `test_gate_2_2.py`.

**Test structure:** pytest functions, each testing one success criterion.

**Fixtures:**
- `model_results` fixture: loads `data/exports/model_results.json`
- `calibrated_model` fixture: loads `data/processed/model_lgbm_calibrated.joblib`
- `onnx_path` fixture: returns Path to `data/exports/onnx/lightgbm_dropout.onnx`

**Tests (aim for 10-12 assertions):**

1. `test_calibrated_entry_exists` -- assert `"test_2023_calibrated"` in model_results
2. `test_brier_improvement` -- assert calibrated Brier < uncalibrated Brier on validation. Get both from model_results.json metadata (brier_uncalibrated and brier_calibrated fields).
3. `test_val_test_prauc_gap` -- calibrated val PR-AUC and calibrated test PR-AUC differ by < 0.07. Extract from model_results.json.
4. `test_onnx_exists_and_size` -- ONNX file exists at expected path and is < 50 MB (50 * 1024 * 1024 bytes).
5. `test_onnx_predictions_match` -- Load ONNX model and raw LightGBM model. Sample 100 random rows from validation data (rng seed=42). Cast to float32 for ONNX. Assert max absolute difference < 1e-4.
6. `test_platt_parameters` -- Platt A and B are present in metadata and are finite floats. A should be negative (sigmoid slope).
7. `test_calibration_plot_exists` -- `data/exports/figures/calibration_plot.png` exists and is > 1 KB.
8. `test_calibrated_model_persisted` -- `data/processed/model_lgbm_calibrated.joblib` exists and loads successfully.
9. `test_alerta_escuela_comparison` -- `alerta_escuela_comparison` key exists in test_2023_calibrated entry. It has fields: roc_auc_range, fnr_range, fairness_analysis.
10. `test_calibrated_predictions_parquet` -- `data/processed/predictions_lgbm_calibrated.parquet` exists, has expected columns (prob_dropout, prob_dropout_uncalibrated, pred_dropout, model, split), rows > 0.
11. `test_test_set_metrics_present` -- test_2023 key exists under test_2023_calibrated metrics with both weighted and unweighted sub-dicts, each having pr_auc, roc_auc, brier, f1, precision, recall, log_loss.

Run: `uv run pytest tests/gates/test_gate_2_3.py -v`
All tests must pass.
  </action>
  <verify>
`uv run pytest tests/gates/test_gate_2_3.py -v` shows all tests PASSED with 0 failures.
  </verify>
  <done>
Gate test 2.3 has 11 tests, all passing, covering calibration improvement, ONNX validity, val-test gap, Alerta Escuela comparison, and artifact existence.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Human review of calibration and Alerta Escuela comparison</name>
  <what-built>
Calibrated LightGBM model with Platt scaling, ONNX export for browser inference,
and final test set evaluation. Gate test 2.3 passes.
  </what-built>
  <how-to-verify>
1. Review calibration plot at `data/exports/figures/calibration_plot.png`:
   - Calibrated curve should be closer to the diagonal (perfect calibration) than uncalibrated
   - Uncalibrated curve should show systematic overconfidence from scale_pos_weight

2. Review the Alerta Escuela comparison table printed during pipeline run:
   - Our FNR should be in the 57-64% range (similar to Alerta Escuela)
   - ROC-AUC will be lower (~0.63 vs 0.84-0.89) due to different data sources -- this is EXPECTED
   - The key differentiator is fairness analysis (we have it, they don't)

3. Review Brier score improvement:
   - Should see ~45% reduction (0.2115 -> ~0.1157)

4. Review val-test PR-AUC gap:
   - Should be < 0.07 (research found 0.0233)
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues</resume-signal>
</task>

</tasks>

<verification>
1. `uv run python src/models/calibration.py` completes without errors
2. `uv run pytest tests/gates/test_gate_2_3.py -v` passes all tests
3. `uv run pytest tests/gates/ -v` passes all gate tests (regression check)
4. `python -c "import json; d=json.load(open('data/exports/model_results.json')); print(list(d.keys()))"` shows logistic_regression, lightgbm, xgboost, test_2023_calibrated
5. `ls -la data/exports/onnx/lightgbm_dropout.onnx` shows file exists
</verification>

<success_criteria>
- Calibrated Brier < uncalibrated Brier on validation (Platt scaling works)
- Test PR-AUC within 0.07 of validation PR-AUC (no extreme overfitting)
- ONNX file < 50 MB, predictions match Python within 1e-4
- model_results.json has test_2023_final metrics and test_2023_calibrated entry with Platt params + Alerta Escuela comparison
- Gate test 2.3 passes (11 assertions)
- Human approves calibration plot and comparison metrics
</success_criteria>

<output>
After completion, create `.planning/phases/07-calibration-onnx-export-final-test/07-01-SUMMARY.md`
</output>
