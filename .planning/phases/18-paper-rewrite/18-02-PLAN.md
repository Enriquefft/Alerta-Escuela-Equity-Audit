---
phase: 18-paper-rewrite
plan: "02"
type: execute
wave: 2
depends_on:
  - "18-01"
files_modified:
  - paper/main.tex
  - paper/tables/table_04_models.tex
  - paper/tables/table_06_fairness_language.tex
  - paper/tables/table_07_intersection.tex
  - paper/tables/table_09_enaho_siagie.tex
  - paper/tables/table_10_crossmodel_fnr.tex
  - paper/figures/fig08_calibration_decile.pdf
  - paper/figures/fig08_calibration_decile.png
autonomous: true

must_haves:
  truths:
    - "ENAHO-vs-SIAGIE comparison table present in Data section with limitation framing"
    - "Predictive Validity subsection appears BEFORE Fairness Analysis with lift-at-top-decile (2.54x), calibration-by-decile plot, BSS=0.040 discussion, and honest low-PR-AUC framing"
    - "Algorithm Independence subsection has cross-model FNR table (5 models x 4 language groups) with Aimara MLP caveat"
    - "Fairness language table includes inline CI format: 0.633 [0.608, 0.656]"
    - "Intersection table includes CI for other_indigenous_urban FNR: 0.753 [0.211, 1.000] with n=89 caveat"
    - "Model comparison table updated to include RF and MLP rows"
    - "Related Work trimmed to offset new content within 22-page budget"
    - "Paper compiles without errors"
  artifacts:
    - path: "paper/tables/table_09_enaho_siagie.tex"
      provides: "ENAHO vs SIAGIE feature comparison table"
      contains: "ENAHO (this study)"
    - path: "paper/tables/table_10_crossmodel_fnr.tex"
      provides: "Cross-model FNR table for algorithm independence"
      contains: "LightGBM"
    - path: "paper/figures/fig08_calibration_decile.pdf"
      provides: "Calibration-by-decile bar chart for Predictive Validity subsection"
    - path: "paper/main.tex"
      provides: "Updated paper with all new subsections and evidence"
      contains: "Predictive Validity"
  key_links:
    - from: "paper/main.tex Predictive Validity subsection"
      to: "paper/figures/fig08_calibration_decile.pdf"
      via: "\\includegraphics{figures/fig08_calibration_decile.pdf}"
      pattern: "fig08_calibration_decile"
    - from: "paper/main.tex Algorithm Independence subsection"
      to: "paper/tables/table_10_crossmodel_fnr.tex"
      via: "\\input{tables/table_10_crossmodel_fnr.tex}"
      pattern: "table_10_crossmodel_fnr"
    - from: "paper/main.tex Data section"
      to: "paper/tables/table_09_enaho_siagie.tex"
      via: "\\input{tables/table_09_enaho_siagie.tex}"
      pattern: "table_09_enaho_siagie"
---

<objective>
Add all new evidence into the paper: ENAHO-vs-SIAGIE comparison table in Data, Predictive Validity
and Algorithm Independence subsections in Results, CI integration into fairness tables, RF+MLP rows
in the model comparison table, and the calibration-by-decile figure. Also trim Related Work to
keep the total paper within 22 pages.

Purpose: Incorporate all Phase 16 (bootstrap CIs, permutation tests) and Phase 17 (RF+MLP, lift
analysis) evidence into the paper. Every numerical claim must be backed by the verified JSON
artifacts in data/exports/.

Output: Updated paper/main.tex with 2 new subsections and 1 new table in Data; updated and new
table files (table_06, table_07, table_09, table_10); new figure fig08; Related Work cuts.
All values sourced directly from data/exports/fairness_metrics.json,
data/exports/predictive_validity.json, and data/exports/model_results.json.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/18-paper-rewrite/18-CONTEXT.md
@.planning/phases/18-paper-rewrite/18-RESEARCH.md
@paper/main.tex
@paper/tables/table_04_models.tex
@paper/tables/table_06_fairness_language.tex
@paper/tables/table_07_intersection.tex
@.planning/phases/18-paper-rewrite/18-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate fig08, create new table files, update existing fairness tables</name>
  <files>
    paper/figures/fig08_calibration_decile.pdf
    paper/figures/fig08_calibration_decile.png
    paper/tables/table_09_enaho_siagie.tex
    paper/tables/table_10_crossmodel_fnr.tex
    paper/tables/table_04_models.tex
    paper/tables/table_06_fairness_language.tex
    paper/tables/table_07_intersection.tex
  </files>
  <action>
**A. Generate fig08_calibration_decile figure (Python script, run from repo root):**

Write a Python script (or inline command) that:
1. Reads data/exports/predictive_validity.json
2. Extracts calibration_by_decile.lightgbm_calibrated.deciles — a list of dicts with
   decile, pred_mean, obs_rate, lift fields
3. Creates a grouped bar chart: x-axis = decile 1-10, two bars per decile:
   predicted probability (pred_mean) and observed rate (obs_rate)
4. Adds a reference line at baseline prevalence (0.134)
5. Labels: "Predicted Probability" (blue bars), "Observed Dropout Rate" (orange bars)
6. Title: "Calibration by Prediction Decile (LightGBM Calibrated)"
7. Y-axis: "Dropout Rate / Predicted Probability", X-axis: "Score Decile (1=Lowest, 10=Highest)"
8. Saves to paper/figures/fig08_calibration_decile.pdf AND .png at 150 dpi
9. Note: Good calibration = bars nearly equal height. MACE=0.018 means they should be close.

Run the script with: uv run python scripts/generate_fig08.py
(Or write the script contents inline with: uv run python -c "...")

**B. Create paper/tables/table_09_enaho_siagie.tex:**

Create this file with the ENAHO-vs-SIAGIE comparison table.
Caption context: table is framed as a limitation, not a contribution.
SIAGIE column header must say "(Alerta Escuela, inferred from public documentation)".

Content (use lp{4.2cm}p{4.2cm} column spec for text wrapping):
```
\begin{tabular}{@{}lp{4.0cm}p{4.0cm}@{}}
\toprule
Feature Category & ENAHO (this study) & SIAGIE (Alerta Escuela, inferred) \\
\midrule
Demographics & Age, sex, mother tongue, nationality (self-report) & Name, DOB, sex, grade, school enrollment (administrative) \\
Economic & Poverty index, household expenditure, poverty quintile & Free lunch eligibility (inferred); no income/expenditure \\
Geographic & Department, district, natural region & School location, district code \\
Attendance/School & Current enrollment (annual self-report) & Daily attendance records, grade history \\
Longitudinal & 6 annual waves pooled (cross-section per year) & Continuous multi-year student trajectory \\
\midrule
\textit{Data characteristics} & & \\
\midrule
Coverage & 150,135 school-age obs (ages 6--17) & $\sim$2M enrolled students/year (estimated) \\
Unit of observation & Household survey respondent & Administrative student record \\
Frequency & Annual survey wave & Continuous / daily \\
Publicly accessible & Yes (INEI, open data) & No (MINEDU internal use only) \\
\bottomrule
\end{tabular}
```

**C. Create paper/tables/table_10_crossmodel_fnr.tex:**

Cross-model FNR table for Algorithm Independence subsection.
Values sourced from data/exports/predictive_validity.json#cross_model_fnr.
Exclude "unknown" (n=54) and "foreign" (n=43) groups — small sample artifacts.
Include a footnote row for Aimara MLP outlier.

Use column spec: @{}lSSSSS@{} (siunitx S columns for number alignment)
Add table note: "Aimara MLP FNR=0.830 (n=76); small sample — interpret with caution."

```
\begin{tabular}{@{}lSSSS[table-format=1.3]S[table-format=1.3]@{}}
\toprule
Language Group & {LR} & {LightGBM} & {XGBoost} & {RF} & {MLP} \\
\midrule
Castellano & 0.584 & 0.633 & 0.613 & 0.549 & 0.666 \\
Quechua & 0.192 & 0.416 & 0.284 & 0.259 & 0.525 \\
Otros ind\'{i}genas & 0.065 & 0.216 & 0.159 & 0.216 & 0.397 \\
Aimara* & 0.288 & 0.263 & 0.288 & 0.192 & 0.830 \\
\bottomrule
\multicolumn{6}{l}{\footnotesize * $n=76$; Aimara MLP FNR=0.830 is a small-sample outlier.} \\
\multicolumn{6}{l}{\footnotesize Algorithm independence claim applies to castellano vs. indigenous pattern only.} \\
\end{tabular}
```

**D. Update paper/tables/table_04_models.tex:**

Add RF and MLP rows. Source values from data/exports/model_results.json.
RF val PR-AUC=0.2613, MLP val PR-AUC=0.2380. Get test PR-AUC and ROC-AUC from model_results.json.
Change column header to "RF" and "MLP" (new columns), OR add rows to existing table.
Since the table currently has 3 model columns (LR, LightGBM, XGBoost), adding 2 more columns
would be too wide. Instead add two new rows at the bottom grouped by model:
Keep the same row structure (Metric as first column) but add RF and MLP as new rows labeled
"PR-AUC val (RF)", "PR-AUC val (MLP)" etc., OR restructure as rows=models, columns=metrics.

Preferred restructure (rows=models for easier reading):
```
\begin{tabular}{@{}lSSSSS@{}}
\toprule
Model & {PR-AUC (val)} & {PR-AUC (test)} & {ROC-AUC (val)} & {Brier (test)} & {BSS (test)} \\
\midrule
Logistic Regression & 0.210 & 0.193 & 0.604 & {---} & {$<$0} \\
LightGBM (raw) & 0.262 & 0.236 & 0.652 & {---} & {$<$0} \\
LightGBM (calibrated) & --- & 0.236 & --- & 0.112 & 0.040 \\
XGBoost & 0.263 & 0.239 & 0.648 & {---} & {$<$0} \\
RF & 0.261 & {---} & {---} & {---} & {$<$0} \\
MLP & 0.238 & {---} & {---} & {---} & {$<$0} \\
\bottomrule
\end{tabular}
```
Note: Read actual test PR-AUC for RF/MLP from model_results.json before writing — use real values.
If RF test PR-AUC not in model_results.json, use "---" and note in summary.

**E. Update paper/tables/table_06_fairness_language.tex:**

Add inline CI column for FNR and a p-value column. Use `table*` in main.tex for this table
(double-width) to accommodate the wider data. Update table to:
- Change FNR column from plain number to "FNR [95\% CI]" combined string column
- Add p-value column (reference group = Castellano shows "ref.")
- Source values from data/exports/fairness_metrics.json (verified in 18-RESEARCH.md):

Castellano:       0.633 [0.608, 0.656] | ref.
Otros indígenas:  0.216 [0.137, 0.310] | p<0.001
Aimara*:          0.263 [0.000, 0.559] | p=0.053
Quechua:          0.416 [0.355, 0.476] | p<0.001
Unknown†:         0.922 [0.712, 1.000] | p=0.262

Use column spec: @{}lrp{3.5cm}rrrr@{}
FNR+CI column uses p{3.5cm} (text column, not S-column).
Add footnote: "* $n<100$; small sample. † $n=54$; unreliable."
Add footnote: "Reference group for p-values: Castellano."

**F. Update paper/tables/table_07_intersection.tex:**

Add CI column for Urban FNR. Key value: other_indigenous_urban FNR=0.753 [0.211, 1.000] n=89.
Update the table to show CI inline for the Urban FNR column.
Other group Urban FNR CIs: read from data/exports/fairness_metrics.json
  intersections.language_x_rural.groups for quechua_urban and castellano_urban.
Castellano urban: FNR and CI from fairness_metrics.json.
Add footnote about wide CI for other_indigenous_urban: "Wide CI reflects $n=89$; point estimate
is robust but uncertainty is high."
  </action>
  <verify>
ls /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/tables/table_09_enaho_siagie.tex
ls /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/tables/table_10_crossmodel_fnr.tex
ls /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/figures/fig08_calibration_decile.pdf
grep "0.633 \[0.608" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/tables/table_06_fairness_language.tex
grep "0.753" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/tables/table_07_intersection.tex
</verify>
  <done>
- paper/tables/table_09_enaho_siagie.tex exists and contains SIAGIE inferred column
- paper/tables/table_10_crossmodel_fnr.tex exists with 5 model columns and Aimara footnote
- paper/figures/fig08_calibration_decile.pdf exists
- table_06 FNR column contains "0.633 [0.608, 0.656]" format
- table_07 shows "0.753" for other_indigenous_urban with CI and n=89 caveat
- table_04 has RF and MLP rows
</done>
</task>

<task type="auto">
  <name>Task 2: Add new subsections to main.tex and trim Related Work</name>
  <files>paper/main.tex</files>
  <action>
Make the following structural edits to paper/main.tex. Read the current file first to find exact
insertion points after Plan 01 edits.

**A. Add ENAHO-vs-SIAGIE table to Data section:**

In the Data section (Section 3), after the paragraph that ends "...because SIAGIE administrative
records are not publicly accessible, we use ENAHO survey data as a proxy to construct and audit an
Alerta Escuela--style prediction model." (currently line ~113), insert:

```latex
\begin{table}[htbp]
  \caption{ENAHO vs.\ SIAGIE Feature Availability. SIAGIE columns are inferred from public
  documentation \cite{minedu2023alerta,minedu2022estadistica}; we have not accessed SIAGIE
  records directly. This comparison documents the features \emph{not} available in our proxy
  model that may be present in the actual system.}
  \label{tab:enaho_siagie}
  \input{tables/table_09_enaho_siagie.tex}
\end{table}

The comparison in Table~\ref{tab:enaho_siagie} highlights a key limitation of the proxy approach:
SIAGIE contains daily attendance records, multi-year student trajectory, and grade history that
ENAHO does not capture. Our proxy model predicts from annual cross-sectional survey data, missing
the longitudinal signal that likely improves the actual system's predictive accuracy. However,
the survey dimensions available in ENAHO---mother tongue, poverty, geography---are precisely those
needed to study equity disparities, and these dimensions are either absent from or not publicly
reported for SIAGIE-based models.
```

**B. Add Predictive Validity subsection to Results:**

In the Results section (Section 5), add a new subsection BEFORE the existing "Fairness Analysis"
section. Place it after the existing "calibration" discussion (currently ending around line ~213,
after Figure 2 caption). Insert:

```latex
\subsection{Predictive Validity}
\label{sec:predictive_validity}

Before examining fairness properties, we establish that the model has meaningful predictive signal.
A model without discriminatory power cannot produce interpretable fairness metrics---high FNR
everywhere is not a fairness finding, it is a model failure.

The calibrated LightGBM model achieves a test PR-AUC of 0.236 against a no-skill baseline of
0.134 (population dropout prevalence), yielding a 1.76x lift in discrimination. The top-scoring
10\% of students contains 34.2\% actual dropouts---a lift of 2.54x over the 13.4\% baseline
(Table~\ref{tab:models}). This decile-level concentration of risk confirms that the model's
predictions are meaningful, not random.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\columnwidth]{figures/fig08_calibration_decile.pdf}
  \caption{Calibration by prediction decile for the LightGBM calibrated model. Bars show predicted
  probability (blue) and observed dropout rate (orange) per decile. Mean absolute calibration error
  = 0.018, indicating well-calibrated predictions. Baseline dropout prevalence = 0.134 (dashed
  line).}
  \label{fig:calibration_decile}
\end{figure}

Figure~\ref{fig:calibration_decile} shows calibration by prediction decile. The model is
well-calibrated across the score range (mean absolute calibration error = 0.018), meaning a
predicted probability of 0.30 corresponds to approximately 30\% actual dropout in that decile.
The Brier Skill Score of 0.040 for the calibrated model is positive but modest, reflecting that
correct calibration---critical for fair decision-making---comes at some cost to overall
discrimination. All uncalibrated models (LR, XGBoost, RF, MLP) have negative Brier Skill Scores,
confirming that Platt scaling is not cosmetic but essential for probability validity.

We acknowledge that PR-AUC of 0.236 is a modest absolute value. However, low absolute PR-AUC
does not invalidate differential FNR findings: a model can be modestly predictive overall while
exhibiting systematic and substantial differences in prediction errors across demographic subgroups.
The fairness analysis that follows documents those differences across five model families with
different architectures---robustness across architectures provides stronger evidence than any
single model's absolute performance.
```

**C. Expand Algorithm Independence subsection with cross-model table:**

Find the existing Algorithm Independence content in Results (currently the passage about
"LightGBM and XGBoost achieve near-identical validation PR-AUC (0.262 vs. 0.263)...algorithm-
independence ratio of 1.0006"). Replace or expand this to:

1. Keep the PR-AUC ratio sentence as-is (algorithm-independence ratio 1.0006)
2. Add the new cross-model FNR table reference after the existing model performance paragraph:

```latex
\subsection{Algorithm Independence}
\label{sec:algorithm_independence}

Table~\ref{tab:crossmodel_fnr} extends the algorithm-independence check to five model families
(LR, LightGBM, XGBoost, RF, MLP) using the FNR disparity that is central to our fairness
findings. Across all five architectures, Castellano speakers consistently show higher FNR than
Quechua and other-indigenous speakers---the rank order that defines our surveillance--invisibility
finding is not an artifact of the LightGBM implementation.

\begin{table}[htbp]
  \caption{False Negative Rate by Language Group Across Five Model Families.
  Aimara group ($n=76$) shows instability (MLP FNR=0.830); algorithm independence claim
  is scoped to the castellano vs.\ indigenous pattern.}
  \label{tab:crossmodel_fnr}
  \input{tables/table_10_crossmodel_fnr.tex}
\end{table}

The absolute FNR values vary across architectures---LR shows a narrower range (0.065--0.584)
than LightGBM (0.216--0.633)---but the ordinal pattern is consistent: castellano FNR exceeds
quechua FNR, which exceeds other-indigenous FNR in all five models. This consistency across
architectures with different inductive biases (linear vs.\ gradient boosting vs.\ neural network)
indicates that the disparity reflects data structure, not modeling artifacts. We note that the
MLP Aimara FNR of 0.830 is an outlier driven by the small sample ($n=76$) and should not be
interpreted as a substantive finding.
```

**D. Update model comparison table reference to include RF/MLP:**

In the Results section, where Table~\ref{tab:models} is described (line ~188), update the prose
to mention RF and MLP:

Replace "LightGBM and XGBoost achieve near-identical validation PR-AUC (0.262 vs. 0.263)"
With: "LightGBM, XGBoost, and RF achieve near-identical validation PR-AUC (0.262, 0.263, and
0.261 respectively). MLP achieves PR-AUC of 0.238, lower than the tree-based ensembles as is
typical on structured tabular data \cite{ke2017lightgbm}."

Update table reference: change `\input{tables/table_04_models.tex}` caption to mention "five
model families" and update the prose references accordingly.

**E. Update table_06 reference to use table* (double-width) for CI columns:**

In main.tex, find the `\begin{table}[htbp]` that contains `\input{tables/table_06_fairness_language.tex}`
and change it to `\begin{table*}[htbp]` (double-width) to accommodate the wider CI columns.
Update `\end{table}` to `\end{table*}` for this table only.

Add a sentence in the Language Dimension subsection text that references the CIs:
After "Table~\ref{tab:fairness_language} reveals a fundamental FNR--FPR trade-off across language
groups", add: "Bootstrap 95\% confidence intervals confirm that the gap between Castellano FNR
(0.633 [0.608, 0.656]) and other-indigenous FNR (0.216 [0.137, 0.310]) is statistically
reliable (permutation $p<0.001$), as are the Quechua disparities ($p<0.001$). The Aimara gap
($p=0.053$) is suggestive but marginal given $n=76$."

**F. Trim Related Work to offset page budget:**

Make these specific cuts to keep total paper within 22 pages:

1. **"Intersectionality in ML Fairness" subsection** (lines ~101-107, ~200 words): Compress to
   2-3 sentences. The citations (Crenshaw, Buolamwini, Kearns, Hebert-Johnson) appear in
   Discussion anyway. Keep only: (a) Crenshaw citation with 1 sentence on intersectionality
   framework, (b) Buolamwini citation with 1 sentence on computational demonstration, (c) 1
   sentence connecting to this paper. Remove the Kearns and Hebert-Johnson paragraph entirely.

2. **"Fairness in Latin American Educational AI" subsection** (lines ~95-99, ~250 words): Remove
   the Zawacki-Richter paragraph (3 sentences about AI applications in Latin American higher
   education). It is the least directly relevant citation. Keep the rest.

3. **Methods "Feature Engineering" subsection** (lines ~156-159, ~200 words): This describes 25
   features in detail. Since Table~\ref{tab:lr_coefficients} (table_05) already lists all 25
   features, condense the prose to 2-3 sentences summarizing the categories (individual, household,
   spatial) and pointing to the table. Cut the long parenthetical detail about individual features.

4. **Appendix placeholder** (lines ~369-372): Remove the entire Appendix section if it only
   contains "Additional disaggregated fairness metrics, regional SHAP decompositions, and model
   hyperparameter details are available in the supplementary materials." with no actual content.
   This placeholder adds ~0.25 pages with no value.

After cuts, compile and verify page count is within 22 pages.
  </action>
  <verify>
grep -n "Predictive Validity" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
grep -n "Algorithm Independence" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
grep -n "table_09_enaho_siagie" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
grep -n "table_10_crossmodel_fnr" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
grep -n "fig08_calibration_decile" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
grep -n "table\*" /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper/main.tex
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit/paper && pdflatex -interaction=nonstopmode main.tex 2>&1 | grep -E "^!" | head -10
</verify>
  <done>
- "Predictive Validity" subsection present in main.tex before Fairness Analysis
- "Algorithm Independence" subsection present with table_10 reference
- table_09_enaho_siagie referenced in Data section with limitation framing caption
- fig08_calibration_decile.pdf referenced in Predictive Validity subsection
- table_06 wrapped in table* (double-width) environment
- pdflatex produces no fatal errors (! lines)
- Paper compiles to PDF
</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. Compile: cd paper && pdflatex -interaction=nonstopmode main.tex (no fatal errors)
2. Compile again for references: pdflatex -interaction=nonstopmode main.tex
3. Check page count: pdfinfo paper/main.pdf | grep Pages (must be ≤22)
4. grep "Predictive Validity\|Algorithm Independence\|table_09\|table_10\|fig08" paper/main.tex
5. grep "0.633 \[0.608" paper/tables/table_06_fairness_language.tex
6. grep "2.54" paper/main.tex (lift value in Predictive Validity section)
7. grep "We have not accessed" paper/main.tex (still present from Plan 01)
</verification>

<success_criteria>
- All 8 files modified/created
- ENAHO-vs-SIAGIE table in Data section with limitation framing
- Predictive Validity subsection before Fairness Analysis: lift 2.54x, BSS 0.040, calibration-by-decile figure
- Algorithm Independence subsection: cross-model FNR table (5 models), rank-order consistency claim scoped correctly
- Fairness language table: inline CI format "0.633 [0.608, 0.656]", p-values, wide table*
- Intersection table: other_indigenous_urban FNR "0.753 [0.211, 1.000]" with n=89 caveat
- Model table: RF val PR-AUC=0.261, MLP val PR-AUC=0.238 rows present
- Paper compiles without errors
- Page count ≤22 (verified via pdfinfo)
</success_criteria>

<output>
After completion, create .planning/phases/18-paper-rewrite/18-02-SUMMARY.md
</output>
