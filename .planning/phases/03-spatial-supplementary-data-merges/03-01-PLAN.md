---
phase: 03-spatial-supplementary-data-merges
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: 
  - src/data/admin.py
  - src/data/census.py
  - src/data/nightlights.py
  - src/data/merge.py
  - src/data/__init__.py
  - tests/gates/test_gate_1_3.py
  - tests/gates/test_gate_1_4.py
  - tests/unit/test_admin_loader.py
  - tests/unit/test_census_loader.py
  - tests/unit/test_nightlights_loader.py
  - tests/unit/test_merge_pipeline.py
  - data/processed/full_dataset.parquet
autonomous: false
user_setup:
  - service: datosabiertos.gob.pe
    why: "Administrative dropout rate CSVs need manual verification and download"
    env_vars: []
    dashboard_config:
      - task: "Verify current admin dropout URLs are accessible"
        location: "https://www.datosabiertos.gob.pe"
      - task: "Update ADMIN_URLS in download.py if 404 errors occur"
        location: "src/data/download.py line 54-57"

must_haves:
  truths:
    - "Admin dropout rates load with ~1,890 districts for primaria (~0.93% mean) and ~1,846 for secundaria (~2.05% mean), all UBIGEO zero-padded"
    - "full_dataset.parquet has the same row count as the input ENAHO DataFrame (no rows gained or lost from merges)"
    - "ENAHO-to-admin merge rate exceeds 85%; Census merge rate exceeds 90%; nightlights coverage exceeds 85%"
    - "Gate tests 1.3 and 1.4 pass all assertions (UBIGEO integrity, merge rates, no duplicates, null column report)"
    - "Lima districts show low dropout rates, Amazonas districts show high -- directionally correct"
  artifacts:
    - path: "src/data/admin.py"
      provides: "Administrative dropout rate loader with UBIGEO padding"
      min_lines: 80
      exports: ["load_admin_dropout_rates", "AdminResult"]
    - path: "src/data/census.py"
      provides: "Census 2017 district-level indicators loader"
      min_lines: 80
      exports: ["load_census_2017", "CensusResult"]
    - path: "src/data/nightlights.py"
      provides: "VIIRS nightlights district-level economic proxy loader"
      min_lines: 80
      exports: ["load_viirs_nightlights", "NightlightsResult"]
    - path: "src/data/merge.py"
      provides: "Sequential LEFT JOIN merge pipeline with validation"
      min_lines: 100
      exports: ["merge_spatial_data", "validate_merge_pipeline"]
    - path: "data/processed/full_dataset.parquet"
      provides: "Final merged dataset ready for feature engineering"
      contains: "ENAHO + admin + census + nightlights data"
    - path: "tests/gates/test_gate_1_3.py"
      provides: "Gate test 1.3 for admin merge validation"
      exports: ["test_gate_1_3"]
    - path: "tests/gates/test_gate_1_4.py"
      provides: "Gate test 1.4 for census/nightlights merge validation"
      exports: ["test_gate_1_4"]
  key_links:
    - from: "src/data/merge.py"
      to: "src/data/enaho.py"
      via: "load_all_years() output as merge input"
      pattern: "from.*enaho.*import.*load_all_years"
    - from: "src/data/admin.py"
      to: "data/raw/admin/*.csv"
      via: "polars.read_csv() with UBIGEO validation"
      pattern: "pl\\.read_csv.*admin"
    - from: "tests/gates/test_gate_1_3.py"
      to: "src/data/merge.py"
      via: "merge_spatial_data() function validation"
      pattern: "merge_spatial_data|validate_merge_pipeline"
---

<objective>
Implement comprehensive spatial and supplementary data merges that enrich ENAHO microdata with district-level administrative dropout rates, Census 2017 indicators, and VIIRS nightlights via robust LEFT JOIN operations on UBIGEO.

Purpose: Enable district-level contextual features for fairness analysis by combining individual-level survey data with area-level administrative and spatial indicators.
Output: Validated merged dataset (full_dataset.parquet) with comprehensive error handling, gate tests, and merge rate reporting.
</objective>

<execution_context>
@/home/hybridz/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-spatial-supplementary-data-merges/03-RESEARCH.md
@.planning/reference/specs.md

# Existing code patterns to follow
@src/data/enaho.py
@src/utils.py
@src/data/download.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create admin.py loader for district dropout rates</name>
  <files>src/data/admin.py</files>
  <action>
Create admin.py following established patterns from enaho.py:

1. Import dependencies: polars, logging, dataclasses, utils (pad_ubigeo, find_project_root)
2. Define AdminResult dataclass with:
   - df: polars DataFrame with merged primaria/secundaria rates
   - primaria_rate: float (mean across districts)
   - secundaria_rate: float (mean across districts)
   - districts_count: int (unique districts)
   - warnings: list[str]

3. Implement load_admin_dropout_rates():
   - Load both admin CSVs from data/raw/admin/ (primaria_2023.csv, secundaria_2023.csv)
   - Use polars.read_csv() with try/except for missing files
   - Zero-pad UBIGEO using pad_ubigeo() utility
   - Calculate district means primaria_rate, secundaria_rate
   - Validate: all UBIGEO exactly 6 chars, rates within 0-100%, no duplicate UBIGEO
   - Merge primaria and secundaria on UBIGEO with validate="1:1"
   - Return AdminResult with comprehensive stats

4. Follow error handling pattern from enaho.py with descriptive error messages

5. Include type hints and comprehensive docstring following project style
</action>
<verify>python -c "from src.data.admin import load_admin_dropout_rates; result = load_admin_dropout_rates(); print(f'Districts: {result.districts_count}, Primaria: {result.primaria_rate:.2%}, Secundaria: {result.secundaria_rate:.2%}')"</verify>
<done>Admin dropout rates loaded successfully with ~1,890 districts, ~0.93% primaria rate, ~2.05% secundaria rate, all UBIGEO zero-padded</done>
</task>

<task type="auto">
  <name>Task 2: Create census.py loader for Census 2017 district indicators</name>
  <files>src/data/census.py</files>
  <action>
Create census.py following the admin.py pattern:

1. Import dependencies: polars, logging, dataclasses, utils
2. Define CensusResult dataclass with:
   - df: polars DataFrame with district-level indicators
   - districts_count: int
   - coverage_stats: dict[str, float] (coverage rates for key variables)
   - warnings: list[str]

3. Implement load_census_2017():
   - Look for census CSV in data/raw/census/census_2017_districts.csv
   - If file doesn't exist: create placeholder function that returns CensusResult with empty DataFrame and warning about missing data
   - When file exists: load with polars.read_csv(), zero-pad UBIGEO
   - Select relevant indicators per spec: poverty indices, indigenous language prevalence, access to services
   - Validate: all UBIGEO exactly 6 chars, no duplicate UBIGEO, reasonable value ranges
   - Calculate coverage statistics for key variables (non-null rates)
   - Return CensusResult

4. Include robust error handling for missing file or corrupted data

5. Follow project docstring and type hint patterns
</action>
<verify>python -c "from src.data.census import load_census_2017; result = load_census_2017(); print(f'Districts: {result.districts_count}, Coverage: {result.coverage_stats}')"</verify>
<done>Census 2017 data loaded with district indicators, UBIGEO validation, and coverage statistics (>90% coverage expected for key variables)</done>
</task>

<task type="auto">
  <name>Task 3: Create nightlights.py loader for VIIRS economic proxy</name>
  <files>src/data/nightlights.py</files>
<action>
Create nightlights.py following established patterns:

1. Import dependencies: polars, logging, dataclasses, utils
2. Define NightlightsResult dataclass with:
   - df: polars DataFrame with district-level nightlight intensity
   - districts_count: int
   - coverage_rate: float
   - stats: dict[str, float] (mean, median, min, max)
   - warnings: list[str]

3. Implement load_viirs_nightlights():
   - Look for nightlights CSV in data/raw/nightlights/viirs_districts.csv
   - If file doesn't exist: create placeholder returning NightlightsResult with empty DataFrame and warning
   - When file exists: load with polars.read_csv(), zero-pad UBIGEO
   - Validate: all UBIGEO exactly 6 chars, no negative values, no duplicate UBIGEO
   - Calculate coverage rate (non-null districts / total expected ~1,839)
   - Compute basic statistics (mean, median, min, max)
   - Return NightlightsResult

4. Include comprehensive validation and error handling

5. Follow project patterns for docstrings and type hints
</action>
<verify>python -c "from src.data.nightlights import load_viirs_nightlights; result = load_viirs_nightlights(); print(f'Districts: {result.districts_count}, Coverage: {result.coverage_rate:.2%}, Mean: {result.stats.get(\"mean\", \"N/A\")}')"</verify>
<done>VIIRS nightlights loaded with district-level economic proxy, no negative values, >85% coverage expected</done>
</task>

<task type="auto">
  <name>Task 4: Create merge.py for spatial data integration pipeline</name>
  <files>src/data/merge.py</files>
<action>
Create merge.py implementing robust sequential LEFT JOIN pipeline:

1. Import dependencies: polars, logging, dataclasses, utils, and all loader functions
2. Define MergeResult dataclass with:
   - df: polars DataFrame (final merged dataset)
   - initial_rows: int (ENAHO input count)
   - final_rows: int (should equal initial_rows)
   - merge_rates: dict[str, float] (coverage rates for each source)
   - null_report: dict[str, float] (null rates for new columns >10%)
   - warnings: list[str]

3. Implement validate_merge_pipeline():
   - Takes original ENAHO df and final merged df
   - Verifies row count unchanged (no loss/gain)
   - Calculates merge rates for each source (admin, census, nightlights)
   - Identifies columns with >10% nulls for human review
   - Checks for duplicate rows using .is_duplicated().any()
   - Returns comprehensive validation dict

4. Implement merge_spatial_data():
   - Input: ENAHO DataFrame (from load_all_years())
   - Step 1: Load admin data with load_admin_dropout_rates()
     - LEFT JOIN on UBIGEO with validate="m:1" and coalesce=True
     - Log merge rate and any issues
   - Step 2: Load census data with load_census_2017()
     - LEFT JOIN on UBIGEO with validate="m:1" and coalesce=True
     - Log merge rate and coverage
   - Step 3: Load nightlights with load_viirs_nightlights()
     - LEFT JOIN on UBIGEO with validate="m:1" and coalesce=True
     - Log merge rate and coverage
   - After each join: validate no row count increase, log warnings
   - Return MergeResult with comprehensive statistics

5. Include robust error handling for each merge step

6. Follow project patterns for logging and validation
</action>
<verify>python -c "from src.data.merge import merge_spatial_data; from src.data.enaho import load_all_years; enaho_df = load_all_years().df; result = merge_spatial_data(enaho_df); print(f'Rows preserved: {result.initial_rows == result.final_rows}, Admin merge: {result.merge_rates.get(\"admin\", 0):.2%}')"</verify>
<done>Spatial merge pipeline preserves ENAHO row count, achieves >85% admin merge rate, provides comprehensive validation reporting</done>
</task>

<task type="auto">
  <name>Task 5: Create unit tests for all loaders and merge pipeline</name>
  <files>tests/unit/test_admin_loader.py tests/unit/test_census_loader.py tests/unit/test_nightlights_loader.py tests/unit/test_merge_pipeline.py</files>
<action>
Create comprehensive unit tests following existing test patterns:

1. test_admin_loader.py:
   - test_load_admin_success(): Mock admin CSV files, verify correct loading, UBIGEO padding, rate calculations
   - test_load_admin_missing_file(): Test graceful handling when admin files don't exist
   - test_ubigeo_validation(): Test UBIGEO zero-padding and duplicate detection
   - test_rate_validation(): Test admin dropout rate reasonableness checks

2. test_census_loader.py:
   - test_load_census_success(): Mock census CSV, verify loading and coverage calculations
   - test_load_census_missing(): Test placeholder behavior when file missing
   - test_census_validation(): Test UBIGEO validation and value range checks
   - test_coverage_stats(): Test coverage rate calculations for key variables

3. test_nightlights_loader.py:
   - test_load_nightlights_success(): Mock VIIRS CSV, verify loading and stats
   - test_load_nightlights_missing(): Test placeholder behavior
   - test_negative_values(): Test validation rejects negative nightlight values
   - test_coverage_calculation(): Test district coverage rate calculation

4. test_merge_pipeline.py:
   - test_merge_spatial_data_success(): Test complete pipeline with mocked data
   - test_row_count_preservation(): Verify LEFT JOIN doesn't change ENAHO row count
   - test_merge_rate_calculation(): Test merge rate calculations for each source
   - test_null_reporting(): Test identification of >10% null columns
   - test_duplicate_detection(): Test .is_duplicated() validation

All tests should:
- Use pytest fixtures for consistent test data
- Follow existing test naming conventions
- Include comprehensive assertions
- Mock file operations to avoid I/O dependencies
- Test both success and error scenarios
</action>
<verify>pytest tests/unit/test_admin_loader.py tests/unit/test_census_loader.py tests/unit/test_nightlights_loader.py tests/unit/test_merge_pipeline.py -v</verify>
<done>All unit tests pass, validating loader functionality, error handling, UBIGEO validation, and merge pipeline correctness</done>
</task>

<task type="auto">
  <name>Task 6: Create Gate tests 1.3 and 1.4 for spatial merge validation</name>
  <files>tests/gates/test_gate_1_3.py tests/gates/test_gate_1_4.py</files>
<action>
Create comprehensive gate tests following existing gate test patterns:

1. test_gate_1_3.py (Admin merge validation):
   - test_ubigeo_format(): All UBIGEO exactly 6 characters
   - test_admin_rate_validation(): Admin mean rates within 20% of expected (0.93% primaria, 2.05% secundaria)
   - test_department_count(): 25 unique departments represented
   - test_admin_merge_rate(): ENAHO-to-admin merge rate > 0.85
   - test_row_count_preservation(): Row count unchanged after admin merge
   - test_no_duplicate_ubigeo(): No duplicate UBIGEO in admin data
   - test_spot_check_districts(): Lima shows low dropout, Amazonas shows high

2. test_gate_1_4.py (Census/nightlights merge validation):
   - test_census_merge_rate(): Census merge rate > 0.90
   - test_nightlights_validation(): No negative values, coverage > 0.85
   - test_no_duplicate_rows(): Check with .is_duplicated().any() == False
   - test_column_count(): Final column count is sum of unique columns (minus duplicate keys)
   - test_null_reporting(): Print columns with >10% nulls for human review
   - test_merge_completeness(): All three sources successfully merged

Both gate tests should:
- Import actual data (no mocking)
- Include comprehensive assertions with descriptive error messages
- Print key statistics for human review
- Follow existing gate test structure with setUp/tearDown if needed
- Include explicit success criteria validation
- Report merge rates and coverage statistics
</action>
<verify>pytest tests/gates/test_gate_1_3.py tests/gates/test_gate_1_4.py -v -s</verify>
<done>Gate tests 1.3 and 1.4 pass all assertions, validating UBIGEO integrity, merge rates (>85% admin, >90% census, >85% nightlights), no duplicates, and comprehensive null reporting</done>
</task>

<task type="auto">
  <name>Task 7: Save final merged dataset and update package exports</name>
  <files>src/data/__init__.py data/processed/full_dataset.parquet</files>
<action>
1. Update src/data/__init__.py:
   - Add exports for new modules: admin, census, nightlights, merge
   - Import key functions: load_admin_dropout_rates, load_census_2017, load_viirs_nightlights, merge_spatial_data
   - Maintain backward compatibility with existing enaho exports
   - Follow existing export patterns

2. Create integration script to generate full_dataset.parquet:
   - Load pooled ENAHO data using load_all_years()
   - Run merge_spatial_data() to get enriched dataset
   - Save to data/processed/full_dataset.parquet using .write_parquet()
   - Log merge statistics and validation results
   - Include timestamp and data provenance metadata

3. Ensure data/processed/ directory exists and is properly structured

4. Add data validation before saving:
   - Verify row count matches ENAHO input
   - Check for critical nulls in key columns
   - Validate UBIGEO integrity in final dataset
   - Log any data quality issues

5. Follow existing file saving patterns and error handling
</action>
<verify>python -c "import polars as pl; df = pl.read_parquet('data/processed/full_dataset.parquet'); print(f'Final dataset: {df.height} rows, {df.width} columns, UBIGEO unique: {df.select(\"ubigeo\").n_unique()}')"</verify>
<done>full_dataset.parquet saved successfully with same row count as ENAHO input, enriched with admin/census/nightlights data, ready for feature engineering</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete spatial merge pipeline (admin + census + nightlights) with validation</what-built>
  <how-to-verify>
    1. Run gate tests: pytest tests/gates/test_gate_1_3.py tests/gates/test_gate_1_4.py -v -s
    2. Review merge statistics printed by tests:
       - ENAHO-to-admin merge rate (>85%)
       - Census merge rate (>90%) 
       - Nightlights coverage (>85%)
       - Columns with >10% nulls
    3. Spot-check districts: Lima should show low admin dropout rates, Amazonas should show high rates
    4. Verify final dataset: python -c "import polars as pl; df = pl.read_parquet('data/processed/full_dataset.parquet'); print(f'Dataset shape: {df.shape}'); print(df.select(['ubigeo', 'dropout', 'admin_primaria_rate', 'admin_secundaria_rate']).head())"
  </how-to-verify>
  <resume-signal>Type "approved" if merge rates look correct and spot-checked districts show expected patterns, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Execute the complete spatial merge validation suite:

```bash
# Run all gate tests with verbose output
pytest tests/gates/test_gate_1_3.py tests/gates/test_gate_1_4.py -v -s

# Verify final merged dataset
python -c "
import polars as pl
from src.data.enaho import load_all_years
from src.data.merge import merge_spatial_data

# Load and validate
enaho_df = load_all_years().df
result = merge_spatial_data(enaho_df)

print(f'=== PHASE 3 VALIDATION ===')
print(f'ENAHO rows: {result.initial_rows:,}')
print(f'Final rows: {result.final_rows:,}')
print(f'Rows preserved: {result.initial_rows == result.final_rows}')
print(f'Merge rates: {result.merge_rates}')
print(f'Columns >10% null: {len([c for c, pct in result.null_report.items() if pct > 0.1])}')

# Spot check districts
spot_check = result.df.filter(
    pl.col('ubigeo').is_in(['150101', '010101'])  # Lima, Amazonas
).select(['ubigeo', 'dropout', 'admin_primaria_rate'])
print(f'Spot check districts:')
print(spot_check)
"
```

Critical validation criteria:
- [ ] All UBIGEO values exactly 6 characters
- [ ] Admin mean rates: primaria ~0.93%, secundaria ~2.05% (within 20%)
- [ ] 25 unique departments represented
- [ ] ENAHO-to-admin merge rate > 85%
- [ ] Census merge rate > 90%
- [ ] Nightlights coverage > 85%
- [ ] No duplicate rows in final dataset
- [ ] Row count unchanged from ENAHO input
- [ ] Lima districts show lower dropout rates than Amazonas
</verification>

<success_criteria>
Phase 3 success is achieved when:

1. **Data Loading Completeness**: All three loaders (admin.py, census.py, nightlights.py) successfully load and validate their respective datasets with proper UBIGEO zero-padding and error handling

2. **Merge Pipeline Integrity**: merge_spatial_data() executes sequential LEFT JOINs preserving ENAHO row count, achieving merge rates of >85% (admin), >90% (census), >85% (nightlights)

3. **Validation Coverage**: Gate tests 1.3 and 1.4 pass all assertions, including UBIGEO format validation, rate reasonableness checks, duplicate detection, and null reporting

4. **Data Quality**: full_dataset.parquet contains the complete merged dataset ready for Phase 4 feature engineering, with comprehensive metadata and provenance

5. **Human Verification**: Spot-checked districts show directionally correct patterns (Lima low dropout, Amazonas high dropout) and columns with >10% nulls are reviewed and deemed acceptable

The spatial enrichment pipeline enables district-level contextual features for fairness analysis while maintaining data integrity and comprehensive validation.
</success_criteria>

<output>
After completion, create `.planning/phases/03-spatial-supplementary-data-merges/03-01-SUMMARY.md`
</output>