---
phase: 04-feature-engineering-descriptive-statistics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/features.py
  - src/data/__init__.py
  - data/processed/enaho_with_features.parquet
  - tests/unit/test_features.py
autonomous: true

must_haves:
  truths:
    - "build_features(full_dataset) returns a FeatureResult with all 19+ spec features as lowercase columns"
    - "All binary features (es_mujer, lang_*, rural, es_peruano, is_working, juntos_participant, has_disability, is_secundaria_age) contain only values {0, 1}"
    - "poverty_quintile has exactly 5 unique values with each representing ~20% of weighted population"
    - "District-level spatial features are z-score standardized with _z suffix"
    - "enaho_with_features.parquet saved with 150,135 rows and all model + meta columns"
  artifacts:
    - path: "src/data/features.py"
      provides: "Feature engineering pipeline"
      exports: ["FeatureResult", "build_features"]
    - path: "data/processed/enaho_with_features.parquet"
      provides: "Complete feature matrix"
    - path: "tests/unit/test_features.py"
      provides: "Unit tests for feature construction"
    - path: "src/data/__init__.py"
      provides: "Updated package exports"
  key_links:
    - from: "src/data/features.py"
      to: "data/processed/full_dataset.parquet"
      via: "polars read_parquet"
      pattern: "pl\\.read_parquet"
    - from: "src/data/features.py"
      to: "src/data/enaho.py"
      via: "reuse _find_module_file and _read_data_file helpers"
      pattern: "_find_module_file|_read_data_file"
---

<objective>
Create the feature engineering pipeline that transforms full_dataset.parquet into a complete feature matrix with all 19+ model features per spec Section 5.

Purpose: The feature matrix is the input for all modeling phases (5-7) and the descriptive statistics (Plan 04-02). Every model feature must have the exact column name from the spec, correct type (binary {0,1}, categorical, continuous), and no feature with >30% nulls.

Output: `src/data/features.py` with `build_features()`, `data/processed/enaho_with_features.parquet`, unit tests, updated package exports.
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-feature-engineering-descriptive-statistics/04-CONTEXT.md
@.planning/phases/04-feature-engineering-descriptive-statistics/04-RESEARCH.md
@.planning/phases/03-spatial-supplementary-data-merges/03-01-SUMMARY.md

Key source files:
@src/data/enaho.py -- _find_module_file(), _read_data_file(), JOIN_KEYS, _KEY_OVERRIDES
@src/data/__init__.py -- current package exports
@src/data/build_dataset.py -- shows full_dataset.parquet build pattern
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create src/data/features.py with build_features() pipeline</name>
  <files>src/data/features.py</files>
  <action>
Create `src/data/features.py` implementing the full feature engineering pipeline. Follow the established Result dataclass pattern.

**FeatureResult dataclass:**
```python
@dataclass
class FeatureResult:
    df: pl.DataFrame        # Full DataFrame with original + engineered columns
    stats: dict             # Feature counts, null rates, quintile balance
    warnings: list[str]     # Non-fatal issues (e.g., missing ESCALE data)
    model_features: list[str]  # List of lowercase model feature column names
```

**Main function `build_features(df: pl.DataFrame) -> FeatureResult`:**

Takes the full_dataset DataFrame (from full_dataset.parquet) and returns a FeatureResult with all engineered features added as new lowercase columns.

**Feature construction (in order):**

1. **Direct mappings (no additional data needed):**
   - `age` = P208A cast to Int64 (already 6-17)
   - `is_secundaria_age` = 1 if P208A >= 12 else 0 (user-requested binary)
   - `es_mujer` = 1 if P207 == 2 else 0
   - `lang_castellano` = 1 if P300A == 4 else 0
   - `lang_quechua` = 1 if P300A == 1 else 0
   - `lang_aimara` = 1 if P300A == 2 else 0
   - `lang_other_indigenous` = 1 if p300a_harmonized == 3 else 0
   - `lang_foreign` = 1 if P300A in (6, 7) else 0
   - `rural` = 1 if ESTRATO >= 6 else 0
   - `region_natural` = categorical string from DOMINIO: {1,2,3,8}="costa", {4,5,6}="sierra", {7}="selva"
   - `is_sierra` = 1 if DOMINIO in (4,5,6) else 0 (model dummy, costa as reference)
   - `is_selva` = 1 if DOMINIO == 7 else 0 (model dummy)
   - `department` = UBIGEO[:2] (analysis key, not a model feature)
   - `district_dropout_rate_admin` = admin_primaria_rate for age 6-11, admin_secundaria_rate for age 12-17; fallback to primaria_rate when secundaria is null
   - `nightlight_intensity` = nightlights_mean_radiance (direct rename)
   - `poverty_index` = census_poverty_rate (direct rename)

2. **Weighted poverty quintile:**
   - `poverty_quintile` = weighted quintile (1-5) based on census_poverty_rate with FACTOR07 weights
   - Sort all rows by census_poverty_rate
   - Compute cumulative FACTOR07 weight
   - Assign quintile: cumulative 0-20% = 1, 20-40% = 2, ..., 80-100% = 5
   - Q1 = least poor, Q5 = most poor

3. **Supplementary features (load from raw ENAHO modules):**

   Create a private helper `_load_supplementary_features(years: list[int]) -> pl.DataFrame` that loads additional columns from raw ENAHO modules for each year, concatenates, and returns a DataFrame keyed on (CONGLOME, VIVIENDA, HOGAR, CODPERSO, year).

   Reuse `_find_module_file()` and `_read_data_file()` from `enaho.py` by importing them:
   ```python
   from data.enaho import _find_module_file, _read_data_file, _KEY_OVERRIDES
   ```

   For each year in the dataset:
   - **Module 200 (P209, P210, P203):** Load birthplace (P209), disability (P210), relationship to head (P203) from the same Module 200 file already used by enaho.py. Only read these 3 columns plus join keys.
   - **Module 500 (P501):** Load employment status. Only read P501 + join keys. Use prefix "Enaho01a", module "500".
   - **Module 700 (P710_04):** Load JUNTOS participation. Household-level (join on CONGLOME, VIVIENDA, HOGAR). Use prefix "Enaho01", module "700". Column might be named P710_4 in some years (handle both).
   - **Sumaria (INGHOG1D):** Load household income. Household-level. The sumaria file is typically named "sumaria-YYYY.dta" or "Sumaria-YYYY.dta". Search for it directly with a glob pattern on year_dir.

   **Construct supplementary features:**
   - `es_peruano` = 1 for all rows by default; set to 0 where P209 == 5 (foreign-born). P209 is only available for ages 12+; for ages 6-11, assume Peruvian (only ~0.1% foreign-born even among 12+).
   - `has_disability` = 1 if P210 == 1 else 0
   - `is_working` = 1 if P501 == 1 else 0; for unmatched rows (mostly ages 6-13), default to 0
   - `juntos_participant` = 1 if P710_04 == 1 else 0; for unmatched households, default to 0
   - `log_income` = log(INGHOG1D + 1) using numpy; for unmatched, use median imputation
   - `parent_education_years` = education years of household head (P203==1) or spouse (P203==2). Convert P301A to years: {1:0, 2:0, 3:6, 4:6, 5:11, 6:11, 7:14, 9:16, 12:18}. For children where no parent found, use median imputation. This is complex: extract parent education per household from Module 200+300, then join back to children.

   If any module file is not found for a year, log a warning and set features from that module to their defaults. Do NOT raise exceptions -- graceful degradation per established loader pattern.

4. **Z-score standardization (per CONTEXT.md user decision):**
   Standardize all district-level spatial features. Compute mean/std on non-null values, apply `(x - mean) / std`. Store as `_z` suffix columns:
   - `district_dropout_rate_admin_z`
   - `nightlight_intensity_z`
   - `poverty_index_z`
   - `census_indigenous_lang_pct_z`
   - `census_literacy_rate_z`
   - `census_electricity_pct_z`
   - `census_water_access_pct_z`

   For null nightlight values (4.1%), standardize non-null then impute nulls with 0 (median of z-scored distribution) and document this choice.

5. **school_student_teacher_ratio:**
   Set to null (ESCALE data not available). Log warning: "school_student_teacher_ratio: ESCALE data not available. Feature set to null. Model runs with 18+ features." Do NOT include in MODEL_FEATURES list.

**Constants to define at module level:**

```python
MODEL_FEATURES = [
    "age", "is_secundaria_age", "es_mujer",
    "lang_castellano", "lang_quechua", "lang_aimara",
    "lang_other_indigenous", "lang_foreign",
    "rural", "is_sierra", "is_selva",
    "district_dropout_rate_admin_z", "nightlight_intensity_z",
    "poverty_index_z", "poverty_quintile",
    "es_peruano", "has_disability", "is_working",
    "juntos_participant", "log_income", "parent_education_years",
    "census_indigenous_lang_pct_z", "census_literacy_rate_z",
    "census_electricity_pct_z", "census_water_access_pct_z",
]

META_COLUMNS = [
    "UBIGEO", "FACTOR07", "year", "dropout",
    "p300a_original", "p300a_harmonized",
    "region_natural", "department",
    "poverty_index", "nightlight_intensity",
    "district_dropout_rate_admin",
]
```

**Stats dict should contain:**
- total_rows, total_features, model_feature_count
- null_rates: dict mapping feature name to null percentage
- quintile_balance: dict mapping quintile (1-5) to weighted population share
- binary_validation: dict mapping binary feature name to unique values set
- supplementary_load_summary: dict mapping module to rows_matched count

**Use logging.getLogger(__name__) throughout.** Log info messages for each feature group construction step. Log warnings for missing modules, imputation, known gaps.
  </action>
  <verify>
Run: `uv run python -c "import sys; sys.path.insert(0, 'src'); from data.features import build_features, FeatureResult, MODEL_FEATURES; print(f'MODEL_FEATURES: {len(MODEL_FEATURES)} features'); print(MODEL_FEATURES)"`

Verify:
- Import succeeds without error
- MODEL_FEATURES has >= 19 entries
- All feature names are lowercase
  </verify>
  <done>
`src/data/features.py` exists with `FeatureResult` dataclass, `build_features()` function, `MODEL_FEATURES` and `META_COLUMNS` constants. All 19+ features constructed with correct types and names.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build enaho_with_features.parquet + unit tests + update exports</name>
  <files>
    src/data/__init__.py
    tests/unit/test_features.py
    data/processed/enaho_with_features.parquet
  </files>
  <action>
**Part A: Run build_features and save parquet**

After Task 1 creates features.py, test the full pipeline:

```python
import sys
sys.path.insert(0, "src")
import polars as pl
from data.features import build_features

df = pl.read_parquet("data/processed/full_dataset.parquet")
result = build_features(df)
print(f"Rows: {result.df.height}")
print(f"Model features: {len(result.model_features)}")
print(f"Stats: {result.stats}")
for w in result.warnings:
    print(f"  WARNING: {w}")
```

Save the result:
```python
# Select model features + meta columns for the parquet
result.df.write_parquet("data/processed/enaho_with_features.parquet")
```

Verify row count == 150,135 (same as full_dataset.parquet).

**Part B: Update src/data/__init__.py**

Add to imports:
```python
from data.features import FeatureResult, build_features, MODEL_FEATURES, META_COLUMNS
```

Add to __all__:
```python
# Features
"FeatureResult",
"build_features",
"MODEL_FEATURES",
"META_COLUMNS",
```

**Part C: Create tests/unit/test_features.py**

Write focused unit tests for feature construction logic. Use small synthetic DataFrames (do NOT load real data). Test:

1. **test_binary_features_encoding** -- Create a 10-row DataFrame with P207, ESTRATO, P300A, DOMINIO values. Run feature construction helpers. Assert all binary features are {0, 1}.

2. **test_poverty_quintile_balance** -- Create a 100-row DataFrame with census_poverty_rate uniformly distributed and FACTOR07 all equal. Assert 5 quintiles with ~20 rows each.

3. **test_region_natural_mapping** -- Test DOMINIO-to-region mapping: DOMINIO 1,2,3,8 -> costa; 4,5,6 -> sierra; 7 -> selva. Assert all 8 DOMINIO values map correctly.

4. **test_admin_rate_age_matching** -- Test that primaria-age (age 8) gets admin_primaria_rate and secundaria-age (age 14) gets admin_secundaria_rate; fallback when secundaria is null.

5. **test_z_score_standardization** -- Create a column with known values [10, 20, 30, None]. After z-scoring, mean of non-null should be ~0, std ~1. Null stays null (or imputed to 0 for nightlights).

6. **test_model_features_list** -- Assert MODEL_FEATURES has >= 19 entries. Assert all entries are lowercase strings. Assert no duplicates.

Run tests with: `uv run pytest tests/unit/test_features.py -v`
  </action>
  <verify>
Run:
```bash
uv run pytest tests/unit/test_features.py -v
```
and:
```bash
uv run python -c "
import sys; sys.path.insert(0, 'src')
import polars as pl
df = pl.read_parquet('data/processed/enaho_with_features.parquet')
print(f'Rows: {df.height}, Cols: {df.width}')
print(f'Columns: {sorted(df.columns)}')
# Check binary features
for col in ['es_mujer', 'rural', 'lang_castellano']:
    vals = df[col].unique().sort().to_list()
    print(f'{col} unique values: {vals}')
# Check quintile balance
print(f'Quintile values: {df[\"poverty_quintile\"].unique().sort().to_list()}')
"
```

Verify:
- All unit tests pass
- enaho_with_features.parquet has 150,135 rows
- Binary features have only {0, 1}
- poverty_quintile has values {1, 2, 3, 4, 5}
- All MODEL_FEATURES columns present
  </verify>
  <done>
- `data/processed/enaho_with_features.parquet` exists with 150,135 rows and all model + meta columns
- `tests/unit/test_features.py` passes all 6+ unit tests
- `src/data/__init__.py` exports FeatureResult, build_features, MODEL_FEATURES, META_COLUMNS
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `src/data/features.py` exists with FeatureResult, build_features(), MODEL_FEATURES (>= 19 entries)
2. `data/processed/enaho_with_features.parquet` has 150,135 rows, all model features, all meta columns
3. All binary features contain only {0, 1}
4. poverty_quintile has 5 groups, each ~20% weighted population (+/- 30%)
5. _z columns exist for all district-level spatial features
6. `uv run pytest tests/unit/test_features.py -v` passes
7. `src/data/__init__.py` exports new symbols
</verification>

<success_criteria>
- Feature matrix contains all 19+ model features with exact lowercase column names
- All binary features are {0, 1}; poverty quintiles have 5 groups with ~20% weighted population each
- enaho_with_features.parquet saved with 150,135 rows
- Unit tests pass
- No feature has >30% null rate (except school_student_teacher_ratio which is excluded)
</success_criteria>

<output>
After completion, create `.planning/phases/04-feature-engineering-descriptive-statistics/04-01-SUMMARY.md`
</output>
