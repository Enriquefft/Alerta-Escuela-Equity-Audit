---
phase: 05-baseline-model-temporal-splits
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/baseline.py
  - tests/gates/test_gate_2_1.py
  - data/processed/predictions_lr.parquet
  - data/processed/model_lr.joblib
  - data/exports/model_results.json
  - data/exports/figures/pr_curve_lr.png
autonomous: false

must_haves:
  truths:
    - "Temporal splits have zero year overlap: train=2018-2021, validate=2022, test=2023"
    - "Logistic regression converges and achieves PR-AUC > 0.14 on validation"
    - "Weighted metrics differ from unweighted metrics (FACTOR07 actually applied)"
    - "model_results.json exists with logistic_regression entry including validate_2022 metrics and threshold analysis at 0.3/0.4/0.5/0.6/0.7"
    - "LR coefficients show sensible signs: poverty increases risk, rural increases risk, indigenous languages increase risk"
    - "Gate test 2.1 passes all assertions"
  artifacts:
    - path: "src/models/baseline.py"
      provides: "Temporal splits, LR training, evaluation, threshold analysis, JSON export"
      exports: ["create_temporal_splits", "train_logistic_regression", "compute_metrics", "run_baseline_pipeline"]
    - path: "data/exports/model_results.json"
      provides: "Model metrics, coefficients, threshold analysis in M4 schema"
    - path: "data/processed/predictions_lr.parquet"
      provides: "Per-row validation and test set predictions for Phase 8 fairness and Phase 9 SHAP"
    - path: "data/processed/model_lr.joblib"
      provides: "Persisted sklearn LogisticRegression model"
    - path: "data/exports/figures/pr_curve_lr.png"
      provides: "Precision-recall curve visualization with threshold markers"
    - path: "tests/gates/test_gate_2_1.py"
      provides: "Gate test for baseline model validation"
  key_links:
    - from: "src/models/baseline.py"
      to: "data/processed/enaho_with_features.parquet"
      via: "polars read_parquet"
      pattern: "pl\\.read_parquet.*enaho_with_features"
    - from: "src/models/baseline.py"
      to: "src/data/features.py"
      via: "MODEL_FEATURES import"
      pattern: "from data\\.features import MODEL_FEATURES"
    - from: "src/models/baseline.py"
      to: "sklearn.linear_model"
      via: "LogisticRegression training"
      pattern: "LogisticRegression"
    - from: "src/models/baseline.py"
      to: "statsmodels"
      via: "GLM(Binomial) for coefficient inference"
      pattern: "sm\\.GLM"
    - from: "tests/gates/test_gate_2_1.py"
      to: "data/exports/model_results.json"
      via: "json.load validation"
      pattern: "json\\.load"
---

<objective>
Train a logistic regression baseline with temporal splits (train=2018-2021, validate=2022, test=2023), compute the full survey-weighted metric suite, perform threshold analysis, export model_results.json, save predictions as parquet, persist the model, generate a PR curve, and write gate test 2.1.

Purpose: This phase establishes temporal split discipline and the full evaluation pipeline (metric computation, threshold tuning, JSON export, prediction storage) that Phases 6-7 will reuse for gradient boosting models. The LR baseline also provides interpretable coefficients for the human review gate -- confirming that the feature set captures expected equity relationships (poverty/rurality/indigenous language increase dropout risk).

Output: `src/models/baseline.py`, `data/exports/model_results.json`, `data/processed/predictions_lr.parquet`, `data/processed/model_lr.joblib`, `data/exports/figures/pr_curve_lr.png`, `tests/gates/test_gate_2_1.py`
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-baseline-model-temporal-splits/05-CONTEXT.md
@.planning/phases/05-baseline-model-temporal-splits/05-RESEARCH.md

Key source files:
@src/data/features.py -- MODEL_FEATURES list (25 features), META_COLUMNS
@src/data/descriptive.py -- matplotlib patterns (Agg backend, figure setup, plt.close())
@data/processed/enaho_with_features.parquet -- feature matrix input (150,135 rows)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create src/models/baseline.py with temporal splits, LR training, evaluation, and all exports</name>
  <files>
    src/models/baseline.py
    data/exports/model_results.json
    data/processed/predictions_lr.parquet
    data/processed/model_lr.joblib
    data/exports/figures/pr_curve_lr.png
  </files>
  <action>
Create `src/models/baseline.py` implementing the full baseline modeling pipeline.

**Module structure and imports:**

```python
"""Baseline logistic regression model with temporal splits and survey-weighted evaluation.

Establishes temporal split discipline (train=2018-2021, validate=2022, test=2023)
and trains a logistic regression baseline with survey-weighted evaluation, threshold
analysis, and coefficient inference. This module sets patterns reused by Phases 6-7.

Usage::

    uv run python src/models/baseline.py
"""

from __future__ import annotations

import json
import logging
import sys
from datetime import datetime, timezone
from pathlib import Path

import joblib
import numpy as np
import polars as pl
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    average_precision_score,
    roc_auc_score,
    f1_score,
    precision_score,
    recall_score,
    brier_score_loss,
    log_loss,
    precision_recall_curve,
    PrecisionRecallDisplay,
)
import statsmodels.api as sm

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from data.features import MODEL_FEATURES
from utils import find_project_root

logger = logging.getLogger(__name__)
```

**Constants:**

```python
TRAIN_YEARS = [2018, 2019, 2020, 2021]
VALIDATE_YEAR = 2022
TEST_YEAR = 2023
FIXED_THRESHOLDS = [0.3, 0.4, 0.5, 0.6, 0.7]

# Identifying columns to carry into predictions parquet
ID_COLUMNS = ["CONGLOME", "VIVIENDA", "HOGAR", "CODPERSO", "year", "UBIGEO", "FACTOR07", "dropout"]
```

**Function 1: `create_temporal_splits(df: pl.DataFrame) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]`**

Split by year with assertions:
- train_df: filter year in TRAIN_YEARS
- val_df: filter year == VALIDATE_YEAR
- test_df: filter year == TEST_YEAR
- Assert train years == {2018, 2019, 2020, 2021}
- Assert val years == {2022}
- Assert test years == {2023}
- Assert no overlap between any pair
- Assert train + val + test == total rows
- Print split sizes: "Train: {n} rows ({years}), Validate: {n} rows ({year}), Test: {n} rows ({year})"
- Return (train_df, val_df, test_df)

**Function 2: `_df_to_numpy(df: pl.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray]`**

Convert Polars DataFrame to numpy arrays at sklearn boundary:
- X = df.select(MODEL_FEATURES).to_numpy() -- shape (n, 25)
- y = df["dropout"].cast(pl.Int8).to_numpy() -- CRITICAL: Boolean must cast to Int8
- w = df["FACTOR07"].to_numpy()
- Return (X, y, w)

**Function 3: `compute_metrics(y_true, y_prob, y_pred, weights=None) -> dict`**

Compute the full metric suite:
```python
return {
    "pr_auc": float(average_precision_score(y_true, y_prob, sample_weight=weights)),
    "roc_auc": float(roc_auc_score(y_true, y_prob, sample_weight=weights)),
    "f1": float(f1_score(y_true, y_pred, sample_weight=weights)),
    "precision": float(precision_score(y_true, y_pred, sample_weight=weights)),
    "recall": float(recall_score(y_true, y_pred, sample_weight=weights)),
    "brier": float(brier_score_loss(y_true, y_prob, sample_weight=weights)),
    "log_loss": float(log_loss(y_true, y_prob, sample_weight=weights)),
}
```

**Function 4: `train_logistic_regression(X_train, y_train, w_train) -> LogisticRegression`**

Train sklearn LogisticRegression:
- solver="lbfgs", max_iter=1000, C=1.0, class_weight="balanced", random_state=42
- lr.fit(X_train, y_train, sample_weight=w_train)
- Print: "LR converged in {lr.n_iter_[0]} iterations"
- Return lr

**Function 5: `_get_coefficient_inference(X_train, y_train, w_train) -> list[dict]`**

Train statsmodels GLM(Binomial) for coefficient inference:
- X_const = sm.add_constant(X_train)
- glm = sm.GLM(y_train.astype(float), X_const, family=sm.families.Binomial(), freq_weights=w_train)
- glm_result = glm.fit()
- feature_names = ["intercept"] + list(MODEL_FEATURES)
- Build coefficient table: list of dicts with feature, coefficient, std_error, odds_ratio, p_value, ci_lower, ci_upper (all rounded to 6 decimal places)
- Verify consistency: assert that sklearn coef_[0] and statsmodels params[1] match within 0.1 (same first non-intercept feature). Log a warning if they differ significantly (class_weight interaction may cause slight differences).
- Return coef_table

**Function 6: `_threshold_analysis(y_true, y_prob, w) -> dict`**

Threshold tuning at 5 fixed thresholds + optimal:
- Get precision_recall_curve(y_true, y_prob, sample_weight=w)
- Compute F1 at each curve threshold: 2*(p*r)/(p+r+1e-10)
- Find optimal threshold = argmax weighted F1
- For each threshold in FIXED_THRESHOLDS:
  - y_pred_t = (y_prob >= t).astype(int)
  - Compute weighted metrics (f1, precision, recall) and unweighted metrics
  - Store in threshold list
- Also add the optimal threshold entry
- Return dict with optimal_threshold, optimization_target="max_weighted_f1", thresholds list

**Function 7: `_save_predictions(df, y_prob, y_pred, optimal_threshold, split_name, output_path)`**

Save per-row predictions as parquet:
- Build Polars DataFrame from df's ID_COLUMNS
- Add prob_dropout, pred_dropout columns from numpy arrays
- Add model="logistic_regression", threshold=optimal_threshold, split=split_name as literal columns
- Write to output_path
- Print: "Predictions saved: {n} rows to {output_path}"

**Function 8: `_plot_pr_curve(y_true, y_prob, w, thresholds_data, optimal_threshold, output_path)`**

Generate PR curve PNG following Phase 4 matplotlib patterns:
- Use PrecisionRecallDisplay.from_predictions(y_true, y_prob, sample_weight=w, name="LR (weighted)")
- Figure size: (10, 7)
- Add horizontal dashed line at the base dropout rate (weighted positive rate) labeled "Chance level"
- Add markers at the 5 fixed thresholds on the curve (find nearest precision/recall point for each threshold)
- Highlight the optimal threshold with a star marker
- Title: "Precision-Recall Curve: Logistic Regression (Validation 2022)"
- Add text annotation near optimal threshold showing its value
- plt.tight_layout(), savefig at 150 DPI, plt.close()

**Function 9: `_print_coefficient_table(coef_table)`**

Console-print coefficient table for human review:
```
=== LOGISTIC REGRESSION COEFFICIENTS ===
Feature                           Coef      SE        OR      p-value
--------------------------------------------------------------------
intercept                        X.XXXX   X.XXXX   X.XXXX   X.XXXX
poverty_quintile                 X.XXXX   X.XXXX   X.XXXX   X.XXXX  ***
rural                            X.XXXX   X.XXXX   X.XXXX   X.XXXX  ***
lang_other_indigenous            X.XXXX   X.XXXX   X.XXXX   X.XXXX  ***
age                              X.XXXX   X.XXXX   X.XXXX   X.XXXX  ***
es_mujer                         X.XXXX   X.XXXX   X.XXXX   X.XXXX  ***
...
```
Mark equity-relevant features (poverty_quintile, rural, lang_other_indigenous, lang_quechua, lang_aimara, age, es_mujer, es_peruano) with `***`.

**Function 10: `_print_metrics_comparison(weighted, unweighted, split_name)`**

Console-print weighted vs unweighted metrics side-by-side:
```
=== METRICS: {split_name} ===
Metric         Weighted    Unweighted    Diff
----------------------------------------------
PR-AUC         X.XXXX      X.XXXX       X.XXXX
ROC-AUC        X.XXXX      X.XXXX       X.XXXX
F1             X.XXXX      X.XXXX       X.XXXX
...
```

**Function 11: `_print_threshold_table(threshold_data)`**

Console-print threshold analysis table:
```
=== THRESHOLD ANALYSIS ===
Threshold   W-F1     W-Prec   W-Recall   UW-F1    UW-Prec  UW-Recall
----------------------------------------------------------------------
0.30       X.XXXX   X.XXXX   X.XXXX     X.XXXX   X.XXXX   X.XXXX
0.40       X.XXXX   X.XXXX   X.XXXX     X.XXXX   X.XXXX   X.XXXX
...
optimal*   X.XXXX   X.XXXX   X.XXXX     X.XXXX   X.XXXX   X.XXXX
```

**Function 12: `_build_model_results_json(metadata, metrics_val, metrics_test, threshold_data, coef_table) -> dict`**

Build the model_results.json structure:
```python
{
    "logistic_regression": {
        "metadata": {
            "model_type": "LogisticRegression",
            "train_years": TRAIN_YEARS,
            "validate_year": VALIDATE_YEAR,
            "test_year": TEST_YEAR,
            "n_train": ...,
            "n_validate": ...,
            "n_test": ...,
            "n_features": len(MODEL_FEATURES),
            "feature_names": list(MODEL_FEATURES),
            "class_weight": "balanced",
            "solver": "lbfgs",
            "max_iter": 1000,
            "C": 1.0,
            "n_iter_actual": ...,
            "convergence": True,
            "year_shift_note": "ENAHO 2024 unavailable; train/val/test shifted back by 1 year from spec",
            "covid_note": "2020 has reduced sample (~13,755 rows) due to COVID phone interviews",
            "weight_note": "freq_weights inflates effective n to ~25M; all p-values are effectively 0. Use odds ratios and coefficient signs for interpretation.",
        },
        "metrics": {
            "validate_2022": {"weighted": {...}, "unweighted": {...}},
            "test_2023": {"weighted": {...}, "unweighted": {...}},
        },
        "threshold_analysis": {...},
        "coefficients": [...],
    }
}
```

Round all float values to 6 decimal places.

**Function 13: `run_baseline_pipeline() -> dict`**

Main orchestration function:

1. Load enaho_with_features.parquet
2. Create temporal splits
3. Convert to numpy (train, val, test)
4. Train sklearn LR
5. Get predictions for val and test: y_prob = lr.predict_proba(X)[:, 1]
6. Get coefficient inference from statsmodels
7. Print coefficient table (highlight equity features)
8. Determine optimal threshold from validation PR curve (max weighted F1)
9. Apply optimal threshold: y_pred = (y_prob >= optimal_threshold).astype(int)
10. Compute weighted and unweighted metrics for val and test
11. Assert weighted != unweighted for PR-AUC: `assert abs(weighted["pr_auc"] - unweighted["pr_auc"]) > 0.001`
12. Print metrics comparison tables (val and test)
13. Run threshold analysis
14. Print threshold analysis table
15. Save predictions parquet (val + test combined into one file)
16. Persist model with joblib
17. Generate PR curve
18. Build and save model_results.json
19. Print final summary

**if __name__ == "__main__" block:**

```python
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(name)s - %(levelname)s - %(message)s")
    run_baseline_pipeline()
```

**After creating baseline.py, run the pipeline:**
```bash
cd PROJECT_ROOT && uv run python src/models/baseline.py
```

Verify all outputs are generated.
  </action>
  <verify>
Run the baseline pipeline:
```bash
uv run python src/models/baseline.py
```

Then verify outputs:
```bash
ls -la data/exports/model_results.json
ls -la data/processed/predictions_lr.parquet
ls -la data/processed/model_lr.joblib
ls -la data/exports/figures/pr_curve_lr.png

uv run python -c "
import json
with open('data/exports/model_results.json') as f:
    d = json.load(f)
lr = d['logistic_regression']
print('PR-AUC (val, weighted):', lr['metrics']['validate_2022']['weighted']['pr_auc'])
print('PR-AUC (val, unweighted):', lr['metrics']['validate_2022']['unweighted']['pr_auc'])
print('Optimal threshold:', lr['threshold_analysis']['optimal_threshold'])
print('N coefficients:', len(lr['coefficients']))
print('N thresholds:', len(lr['threshold_analysis']['thresholds']))

import polars as pl
pred = pl.read_parquet('data/processed/predictions_lr.parquet')
print('Predictions shape:', pred.shape)
print('Predictions columns:', pred.columns)
"
```

Verify:
- model_results.json exists, is valid JSON, has logistic_regression key
- PR-AUC > 0.14 on validation
- Weighted != unweighted metrics
- Optimal threshold is between 0.1 and 0.9
- 26 coefficients (intercept + 25 features)
- 6 threshold entries (5 fixed + 1 optimal)
- predictions_lr.parquet has val + test rows with prob_dropout column
- model_lr.joblib exists
- pr_curve_lr.png exists
  </verify>
  <done>
- `src/models/baseline.py` exists with create_temporal_splits, train_logistic_regression, compute_metrics, run_baseline_pipeline
- Logistic regression converges and PR-AUC > 0.14 on validation
- Weighted and unweighted metrics differ
- `data/exports/model_results.json` has logistic_regression entry with validate_2022 + test_2023 metrics, threshold_analysis, and coefficients
- `data/processed/predictions_lr.parquet` has per-row predictions for val + test
- `data/processed/model_lr.joblib` persists the sklearn model
- `data/exports/figures/pr_curve_lr.png` shows PR curve with threshold markers
- Console output shows coefficient table, metrics comparison, and threshold analysis
  </done>
</task>

<task type="auto">
  <name>Task 2: Create gate test 2.1 for baseline model validation</name>
  <files>tests/gates/test_gate_2_1.py</files>
  <action>
Create `tests/gates/test_gate_2_1.py` following the established gate test pattern from test_gate_1_1.py through test_gate_1_5.py.

**Gate test structure:**

```python
"""Gate Test 2.1: Baseline Model + Temporal Splits Validation.

Validates:
- Temporal split correctness (no year overlap)
- Logistic regression convergence and PR-AUC > 0.14
- Weighted vs unweighted metric divergence
- model_results.json schema and content
- Threshold analysis at 5 fixed points + optimal
- Coefficient sensibility (signs for equity-relevant features)
- predictions_lr.parquet existence and schema
- model_lr.joblib persistence
"""

import json
import sys
from pathlib import Path

import numpy as np
import polars as pl
import pytest

sys.path.insert(0, str(Path(__file__).resolve().parents[2] / "src"))

from data.features import MODEL_FEATURES
from utils import find_project_root
```

**Test functions:**

**test_temporal_splits_no_overlap()**
- Load enaho_with_features.parquet
- Verify year column has values in {2018, 2019, 2020, 2021, 2022, 2023}
- Verify train years (2018-2021) = 98,023 rows (+/- 100)
- Verify validate year (2022) = 26,477 rows (+/- 100)
- Verify test year (2023) = 25,635 rows (+/- 100)
- Assert total = 150,135
- Print: "Train: {n}, Validate: {n}, Test: {n}, Total: {n}"

**test_model_results_json_exists()**
- Assert data/exports/model_results.json exists
- Load JSON, assert valid
- Assert "logistic_regression" is a top-level key
- Print: "model_results.json loaded successfully"

**test_model_results_metadata()**
- Load model_results.json
- Check metadata: model_type, train_years, validate_year, test_year, n_features, feature_names, class_weight, convergence
- Assert n_features == 25
- Assert train_years == [2018, 2019, 2020, 2021]
- Assert convergence == True
- Print metadata summary

**test_validation_pr_auc_above_baseline()**
- Load model_results.json
- Get weighted PR-AUC from validate_2022
- Assert PR-AUC > 0.14 (beating random baseline for ~14% dropout rate)
- Print: "Validation PR-AUC (weighted): {value:.4f} > 0.14 PASS"

**test_weighted_differs_from_unweighted()**
- Load model_results.json
- Get weighted and unweighted PR-AUC from validate_2022
- Assert abs(weighted - unweighted) > 0.001
- Print both values and difference
- Also check at least one other metric (ROC-AUC or F1) differs

**test_threshold_analysis_complete()**
- Load model_results.json
- Assert threshold_analysis has optimal_threshold (between 0.05 and 0.95)
- Assert optimization_target == "max_weighted_f1"
- Assert thresholds list has entries for 0.3, 0.4, 0.5, 0.6, 0.7
- Assert each entry has weighted_f1, weighted_precision, weighted_recall
- Print threshold analysis table

**test_coefficients_sensible_signs()**
- Load model_results.json
- Get coefficients list
- Assert 26 coefficients (intercept + 25 features)
- Check equity-relevant feature signs:
  - poverty_quintile: coefficient > 0 (higher quintile = more poverty = more dropout)
  - rural: coefficient > 0 (rural increases dropout risk)
  - age: print the coefficient (direction depends on model dynamics)
- Print coefficient table with odds ratios for all equity-relevant features
- NOTE: Do NOT hard-assert the sign of every feature -- only print for human review. The logistic regression with class_weight='balanced' and survey weights may produce unexpected signs for some features due to the weight interaction. The human gate is the final arbiter.

**test_predictions_parquet_exists()**
- Assert data/processed/predictions_lr.parquet exists
- Load and verify:
  - Has columns: CONGLOME, VIVIENDA, HOGAR, CODPERSO, year, UBIGEO, FACTOR07, dropout, prob_dropout, pred_dropout, model, threshold
  - prob_dropout values between 0 and 1
  - pred_dropout values in {0, 1}
  - model == "logistic_regression" for all rows
  - Row count = val + test rows (~52,112)
- Print: "Predictions: {n} rows, prob range [{min:.4f}, {max:.4f}]"

**test_model_joblib_exists()**
- Assert data/processed/model_lr.joblib exists
- Load with joblib.load and verify it has predict_proba method
- Print: "Model loaded, has predict_proba: True"

**test_pr_curve_figure_exists()**
- Assert data/exports/figures/pr_curve_lr.png exists
- Assert file size > 10KB (not empty/corrupt)
- Print: "PR curve figure: {size_kb:.1f} KB"

**test_test_set_metrics()**
- Load model_results.json
- Get test_2023 metrics
- Assert test_2023 weighted PR-AUC exists and is > 0.0
- Assert test_2023 weighted and unweighted both present
- Print: "Test PR-AUC (weighted): {value:.4f}"

**Summary section at end of file:**

Add `if __name__ == "__main__"` block printing:
```
=== GATE TEST 2.1 SUMMARY ===
Temporal splits: Train=98,023 (2018-2021), Val=26,477 (2022), Test=25,635 (2023)
LR convergence: PASS (n_iter={n})
Val PR-AUC (weighted): {val:.4f} > 0.14 PASS
Weighted != Unweighted: PASS (diff={diff:.4f})
Threshold analysis: 5 fixed + optimal={opt:.4f}
Coefficients: 26 entries (intercept + 25 features)
  poverty_quintile: coef={c:.4f}, OR={or:.4f}
  rural:            coef={c:.4f}, OR={or:.4f}
  lang_quechua:     coef={c:.4f}, OR={or:.4f}
  age:              coef={c:.4f}, OR={or:.4f}
  es_mujer:         coef={c:.4f}, OR={or:.4f}
Predictions: {n} rows saved
Model persisted: model_lr.joblib
PR curve: pr_curve_lr.png
```
  </action>
  <verify>
Run gate test:
```bash
uv run pytest tests/gates/test_gate_2_1.py -v -s
```

Verify: all tests pass, console output shows PR-AUC > 0.14, weighted != unweighted, coefficient signs visible.
  </verify>
  <done>
- Gate test 2.1 passes all assertions
- Console output shows all key metrics for human review
- Temporal splits verified (zero overlap)
- PR-AUC > 0.14 on validation
- Weighted and unweighted metrics differ
- Coefficient table printed with odds ratios for equity-relevant features
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete baseline modeling pipeline with:
- Temporal splits: train=2018-2021 (98,023 rows), validate=2022 (26,477), test=2023 (25,635)
- Logistic regression trained with class_weight='balanced' and FACTOR07 survey weights
- Full metric suite: PR-AUC, ROC-AUC, F1, precision, recall, Brier, log-loss (both weighted and unweighted)
- Threshold analysis at 0.3, 0.4, 0.5, 0.6, 0.7 plus optimal (max weighted F1)
- Coefficient inference via statsmodels GLM: coefficients, standard errors, odds ratios, p-values
- model_results.json with all metrics, thresholds, and coefficients
- predictions_lr.parquet with per-row predictions for Phase 8 fairness
- PR curve visualization with threshold markers
- Gate test 2.1 passing all assertions

Key items for human review:
- LR coefficient signs for equity-relevant features (poverty, rural, indigenous language, age, sex)
- PR-AUC exceeds 0.14 (beats random baseline)
- Weighted vs unweighted metric divergence confirms FACTOR07 is applied
  </what-built>
  <how-to-verify>
1. Review gate test output with coefficient table:
   ```bash
   uv run pytest tests/gates/test_gate_2_1.py -v -s
   ```
   Check:
   - poverty_quintile coefficient is positive (higher poverty quintile = more dropout)
   - rural coefficient is positive (rural increases risk)
   - Indigenous language features (lang_quechua, lang_aimara, lang_other_indigenous) have reasonable signs
   - Age coefficient: check if positive (older = more dropout) or negative (model may capture non-linear effects)
   - es_mujer: check direction (gender effects may vary)
   - PR-AUC > 0.14 confirmed

2. Review the PR curve visualization:
   Open `data/exports/figures/pr_curve_lr.png`:
   - Curve should be above the chance level line
   - 5 threshold markers should be visible along the curve
   - Optimal threshold should be highlighted

3. Review model_results.json:
   ```bash
   uv run python -c "
   import json
   with open('data/exports/model_results.json') as f:
       d = json.load(f)
   lr = d['logistic_regression']
   print('=== VALIDATION METRICS ===')
   for k, v in lr['metrics']['validate_2022']['weighted'].items():
       uw = lr['metrics']['validate_2022']['unweighted'][k]
       print(f'  {k:10s}  W={v:.4f}  UW={uw:.4f}  diff={abs(v-uw):.4f}')
   print()
   print('=== EQUITY-RELEVANT COEFFICIENTS ===')
   equity_feats = ['poverty_quintile', 'rural', 'lang_quechua', 'lang_aimara', 'lang_other_indigenous', 'age', 'es_mujer', 'es_peruano']
   for c in lr['coefficients']:
       if c['feature'] in equity_feats:
           print(f\"  {c['feature']:30s}  coef={c['coefficient']:+.4f}  OR={c['odds_ratio']:.4f}\")
   "
   ```

4. Rejection criteria:
   - REJECT if PR-AUC < 0.14 (model is worse than random)
   - REJECT if poverty_quintile coefficient is negative AND rural coefficient is negative (both wrong would indicate a fundamental pipeline error)
   - ACCEPT if coefficient signs are generally sensible even if a few are surprising
  </how-to-verify>
  <resume-signal>Type "approved" if LR coefficients show sensible odds ratios (poverty increases risk, urban decreases risk, age effects visible). Describe any issues if adjustments needed.</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. `src/models/baseline.py` exists with create_temporal_splits, train_logistic_regression, compute_metrics, run_baseline_pipeline
2. Temporal splits: train=2018-2021 (98,023), validate=2022 (26,477), test=2023 (25,635), zero overlap
3. LR converges and validation PR-AUC > 0.14
4. Weighted metrics differ from unweighted (FACTOR07 applied)
5. `data/exports/model_results.json` has logistic_regression entry with validate_2022 + test_2023 metrics, threshold_analysis at 0.3/0.4/0.5/0.6/0.7, and 26 coefficients
6. `data/processed/predictions_lr.parquet` has ~52,112 rows with prob_dropout and pred_dropout
7. `data/processed/model_lr.joblib` loads and has predict_proba
8. `data/exports/figures/pr_curve_lr.png` exists
9. Gate test 2.1 passes: `uv run pytest tests/gates/test_gate_2_1.py -v`
10. Human approves LR coefficient signs and odds ratios
</verification>

<success_criteria>
- Temporal splits correct with zero year overlap
- Logistic regression converges, PR-AUC > 0.14 on validation
- Weighted metrics differ from unweighted
- model_results.json, predictions_lr.parquet, model_lr.joblib, pr_curve_lr.png all exist with correct content
- Gate test 2.1 passes all assertions
- Human reviews and approves LR coefficients for equity-relevant features
</success_criteria>

<output>
After completion, create `.planning/phases/05-baseline-model-temporal-splits/05-01-SUMMARY.md`
</output>
