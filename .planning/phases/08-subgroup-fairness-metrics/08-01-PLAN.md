---
phase: 08-subgroup-fairness-metrics
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/fairness/metrics.py
  - tests/gates/test_gate_3_1.py
  - data/exports/fairness_metrics.json
autonomous: false

must_haves:
  truths:
    - "TPR, FPR, FNR, precision, and PR-AUC computed per subgroup for all 6 dimensions with FACTOR07 survey weights"
    - "Calibration-by-group shows actual dropout rate among predicted high-risk (>0.7 uncalibrated) per group"
    - "Three intersectional analyses (language x rurality, sex x poverty, language x region) with <50 groups flagged"
    - "fairness_metrics.json matches M4 schema with dimensions, groups, gaps, and intersections"
    - "Gate test 3.1 passes; FNR by language group and calibration table printed for human review"
  artifacts:
    - path: "src/fairness/metrics.py"
      provides: "compute_fairness_metrics pipeline with dimension analysis, calibration-by-group, gap computation, intersection analysis, JSON export"
      min_lines: 200
    - path: "tests/gates/test_gate_3_1.py"
      provides: "Gate test 3.1 validating fairness metrics completeness, sample sizes, weighted vs unweighted difference, JSON schema"
      min_lines: 80
    - path: "data/exports/fairness_metrics.json"
      provides: "M4-schema-compliant fairness metrics JSON with all 6 dimensions + 3 intersections"
  key_links:
    - from: "src/fairness/metrics.py"
      to: "data/processed/predictions_lgbm_calibrated.parquet"
      via: "polars read_parquet + filter test_2023"
      pattern: "read_parquet.*predictions_lgbm"
    - from: "src/fairness/metrics.py"
      to: "data/processed/enaho_with_features.parquet"
      via: "polars LEFT JOIN on ID keys to get sensitive features"
      pattern: "read_parquet.*enaho_with_features"
    - from: "src/fairness/metrics.py"
      to: "fairlearn.metrics.MetricFrame"
      via: "Two-MetricFrame pattern (binary + proba) with sample_params"
      pattern: "MetricFrame"
    - from: "src/fairness/metrics.py"
      to: "data/exports/fairness_metrics.json"
      via: "json.dump with M4 schema structure"
      pattern: "json\\.dump.*fairness_metrics"
    - from: "tests/gates/test_gate_3_1.py"
      to: "data/exports/fairness_metrics.json"
      via: "json.load and structural assertions"
      pattern: "json\\.load.*fairness_metrics"
---

<objective>
Compute comprehensive fairness metrics across all 6 protected dimensions (language, sex, geography, region, poverty, nationality) and 3 intersections (language x rurality, sex x poverty, language x region) using fairlearn MetricFrame with FACTOR07 survey weights. Export results to M4-schema-compliant fairness_metrics.json. Validate with gate test 3.1.

Purpose: This is the core deliverable of the equity audit. The fairness metrics quantify where the dropout prediction model systematically fails different student populations -- which groups are missed (high FNR), which are falsely flagged (high FPR), and whether "high risk" means the same thing across groups (calibration).

Output: `src/fairness/metrics.py`, `tests/gates/test_gate_3_1.py`, `data/exports/fairness_metrics.json`
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-subgroup-fairness-metrics/08-RESEARCH.md
@.planning/phases/07-calibration-onnx-export-final-test/07-01-SUMMARY.md
@src/data/features.py
@src/models/baseline.py
@src/models/calibration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement fairness metrics pipeline</name>
  <files>src/fairness/metrics.py</files>
  <action>
Create `src/fairness/metrics.py` implementing the full fairness metrics pipeline. Follow the established pipeline pattern (`if __name__ == "__main__":` with a `run_fairness_pipeline()` function).

**Data Loading:**
- Load `data/processed/predictions_lgbm_calibrated.parquet` (has `prob_dropout` calibrated, `prob_dropout_uncalibrated`, `pred_dropout`, `model`, `split`, + ID columns: CONGLOME, VIVIENDA, HOGAR, CODPERSO, year, UBIGEO, FACTOR07, dropout).
- Filter to `split == "test_2023"` only. Assert 25,635 rows.
- Load `data/processed/enaho_with_features.parquet` and LEFT JOIN on `["CONGLOME", "VIVIENDA", "HOGAR", "CODPERSO", "year"]` to get sensitive features: `p300a_harmonized`, `p300a_original`, `region_natural`, `es_mujer`, `rural`, `es_peruano`, `poverty_quintile`, `department`, + model features for language dummies.
- Assert join produces same row count (no gain/loss).
- Cast `dropout` to `Int8` before `.to_numpy()` (Boolean dtype issue from Phase 5).

**Binary predictions:**
- Load threshold from `data/exports/model_results.json` key `test_2023_calibrated.threshold` (value: 0.165716).
- Compute `y_pred = (prob_dropout >= threshold).astype(int)` using CALIBRATED probabilities.

**6 Dimensions to analyze (each gets per-group TPR, FPR, FNR, precision, PR-AUC + gaps):**

1. **language** (harmonized, min_sample=100): Map from feature dummies -- `lang_castellano`, `lang_quechua`, `lang_aimara`, `lang_other_indigenous`, `lang_foreign`. Create a categorical array with values: "castellano", "quechua", "aimara", "other_indigenous", "foreign". Groups: whichever dummy == 1. If none == 1, classify as "unknown" (should not happen but handle gracefully).

2. **language_disaggregated** (p300a_original, min_sample=50): Use `p300a_original` column directly (numeric codes). Map codes to readable labels: {1: "quechua", 2: "aimara", 3: "other_indigenous", 4: "castellano", 5: "portuguese", 6: "english", 7: "other_foreign", 10: "ashaninka", 11: "awajun", 12: "shipibo", 13: "shawi", 14: "matsigenka", 15: "achuar"}. Note: not all codes may appear in test set.

3. **sex** (es_mujer, min_sample=100): Map 1 -> "female", 0 -> "male".

4. **geography** (rural, min_sample=100): Map 1 -> "rural", 0 -> "urban".

5. **region** (region_natural, min_sample=100): Use `region_natural` column directly. Values: "Costa", "Sierra", "Selva".

6. **poverty** (poverty_quintile, min_sample=100): Use `poverty_quintile` column (values 1-5). Map to "Q1" through "Q5" labels.

7. **nationality** (es_peruano, min_sample=100): Map 1 -> "peruvian", 0 -> "non_peruvian". Note: non_peruvian has only ~27 observations -- will be flagged.

**Two-MetricFrame pattern (CRITICAL -- see research):**

For each dimension, create TWO MetricFrame instances:
- `mf_binary`: metrics = {tpr, fpr, fnr, precision} with `y_pred=y_pred_binary` and `sample_params` repeating `{'sample_weight': weights}` per metric name.
- `mf_proba`: metrics = {pr_auc: average_precision_score} with `y_pred=y_prob_calibrated` and `sample_params={'pr_auc': {'sample_weight': weights}}`.

Merge results via `pd.concat([mf_binary.by_group, mf_proba.by_group], axis=1)`.

Metric implementations:
- `tpr`: `lambda y, p, sample_weight=None: recall_score(y, p, sample_weight=sample_weight, zero_division=np.nan)`
- `precision`: Same pattern with `precision_score`, `zero_division=np.nan`.
- `fnr`: `1.0 - recall_score(...)`, handle NaN.
- `fpr`: Custom `weighted_fpr(y_true, y_pred, sample_weight=None)` -- see research Pattern 2. sklearn has no fpr_score.
- `pr_auc`: `average_precision_score` directly.

**Calibration-by-group (CRITICAL -- use UNCALIBRATED probabilities):**

For each dimension, compute: among students with `prob_dropout_uncalibrated > 0.7`, what is the actual weighted dropout rate per group? Use `min_high_risk=30` -- if fewer than 30 students in a group have uncalibrated prob > 0.7, report `actual_dropout_rate: null` for that group.

Document in the JSON metadata that uncalibrated probabilities are used for high-risk analysis because calibrated probs max at 0.431 (Platt scaling compression).

**Gap computation for each dimension:**
- `equalized_odds_tpr`: max(TPR) - min(TPR) across groups (dropping NaN).
- `equalized_odds_fpr`: max(FPR) - min(FPR) across groups.
- `predictive_parity`: max(precision) - min(precision) across groups.
- `max_fnr_gap`: max(FNR) - min(FNR) across groups.
- `max_fnr_groups`: [group_with_highest_FNR, group_with_lowest_FNR].

**3 Intersectional analyses (min_sample=50):**

Use pandas DataFrame as `sensitive_features` argument to MetricFrame for automatic intersection:

1. **language_x_rural**: Create DataFrame with columns "language" (harmonized 4 groups, excluding foreign) and "rural" (urban/rural). Produces intersection groups like "castellano_urban", "quechua_rural", etc.

2. **sex_x_poverty**: Create DataFrame with "sex" (male/female) and "poverty" (Q1-Q5). Produces groups like "female_Q1", "male_Q3", etc.

3. **language_x_region**: Create DataFrame with "language" (harmonized 4 groups) and "region" (Costa/Sierra/Selva). Produces groups like "quechua_sierra", "aimara_costa", etc.

For each intersection, compute the same metrics (TPR, FPR, FNR, precision, PR-AUC) + calibration. Flag groups with <50 unweighted observations (`flagged_small_sample: true`). Compute gaps.

**Small sample flagging:**
- Primary dimensions: flag if `n_unweighted < 100` (per spec).
- Disaggregated language: flag if `n_unweighted < 50`.
- Intersections: flag if `n_unweighted < 50`.
- Add `"flagged_small_sample": true` to affected group entries.

**JSON export -- `data/exports/fairness_metrics.json`:**

Match the M4 schema structure:
```json
{
  "generated_at": "ISO timestamp",
  "model": "lightgbm",
  "threshold": 0.165716,
  "threshold_type": "calibrated",
  "calibration_note": "Calibrated probs max at 0.431; high-risk (>0.7) uses uncalibrated probs",
  "test_set": "2023",
  "n_test": 25635,
  "n_dropouts": 3500,
  "dimensions": {
    "language": {
      "sensitive_feature": "p300a_harmonized",
      "min_sample": 100,
      "groups": {
        "castellano": {
          "n_unweighted": N,
          "n_weighted": N.NN,
          "tpr": 0.NNNNNN,
          "fpr": 0.NNNNNN,
          "fnr": 0.NNNNNN,
          "precision": 0.NNNNNN,
          "pr_auc": 0.NNNNNN,
          "calibration_high_risk": {
            "n_predicted_high": N,
            "actual_dropout_rate": 0.NNNNNN
          }
        }
      },
      "gaps": {
        "equalized_odds_tpr": 0.NNNNNN,
        "equalized_odds_fpr": 0.NNNNNN,
        "predictive_parity": 0.NNNNNN,
        "max_fnr_gap": 0.NNNNNN,
        "max_fnr_groups": ["group_high", "group_low"]
      }
    },
    "language_disaggregated": { ... },
    "sex": { ... },
    "geography": { ... },
    "region": { ... },
    "poverty": { ... },
    "nationality": { ... }
  },
  "intersections": {
    "language_x_rural": { "groups": { ... }, "gaps": { ... } },
    "sex_x_poverty": { "groups": { ... }, "gaps": { ... } },
    "language_x_region": { "groups": { ... }, "gaps": { ... } }
  }
}
```

Round all float metrics to 6 decimal places. Use `default=str` in `json.dump` for any numpy type serialization issues. Indent=2 for readability.

**Console output (printed at end of pipeline):**

Print a summary table showing:
1. FNR by language group (harmonized) -- the core equity finding.
2. FNR by rural/urban.
3. Calibration table: for each dimension's groups, n_predicted_high and actual_dropout_rate among high-risk.
4. Max FNR gap and groups for each dimension.
5. Flagged small-sample groups.
6. Total dimensions analyzed, total intersection groups, JSON file size.

Run with: `uv run python src/fairness/metrics.py`
  </action>
  <verify>
Run `uv run python src/fairness/metrics.py` and confirm:
1. Pipeline completes without errors
2. `data/exports/fairness_metrics.json` is created, valid JSON, has 7+ dimension keys + 3 intersection keys
3. FNR by language group is printed (indigenous languages should have higher FNR than castellano)
4. Calibration table shows actual dropout rates among high-risk per group
5. All regression tests still pass: `uv run pytest tests/ -x -q`
  </verify>
  <done>
fairness_metrics.json exists with all 7 dimensions (language, language_disaggregated, sex, geography, region, poverty, nationality) and 3 intersections (language_x_rural, sex_x_poverty, language_x_region). Each dimension has per-group TPR/FPR/FNR/precision/PR-AUC + calibration_high_risk + gaps. Small sample groups are flagged. Pipeline prints FNR and calibration summary.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write gate test 3.1</name>
  <files>tests/gates/test_gate_3_1.py</files>
  <action>
Create `tests/gates/test_gate_3_1.py` following the established gate test pattern (sys.path.insert, find_project_root, pytest fixtures, descriptive test names).

**Test fixtures:**
- `fairness_data`: Load and parse `data/exports/fairness_metrics.json`. Fail if file doesn't exist.

**Tests:**

1. `test_json_exists_and_valid`: Assert `fairness_metrics.json` exists, is valid JSON, has required top-level keys (`generated_at`, `model`, `threshold`, `dimensions`, `intersections`).

2. `test_all_dimensions_present`: Assert `dimensions` dict has all 7 keys: `language`, `language_disaggregated`, `sex`, `geography`, `region`, `poverty`, `nationality`.

3. `test_dimension_groups_have_required_metrics`: For each dimension, for each group, assert keys `n_unweighted`, `n_weighted`, `tpr`, `fpr`, `fnr`, `precision`, `pr_auc` exist. Assert `calibration_high_risk` key exists with `n_predicted_high`.

4. `test_primary_dimension_sample_sizes`: For primary dimensions (language, sex, geography, region, poverty), assert non-flagged groups have `n_unweighted >= 100`. For nationality, assert `es_peruano=0` group is flagged.

5. `test_disaggregated_language_threshold`: For `language_disaggregated`, assert threshold is 50 (not 100). Assert any group with n_unweighted < 50 is flagged.

6. `test_weighted_differs_from_unweighted`: Load predictions parquet, compute unweighted TPR for sex dimension (simple recall_score without weights). Load weighted TPR from JSON. Assert they differ (confirms FACTOR07 actually applied). Difference should be > 0.001.

7. `test_gaps_present`: For each dimension, assert `gaps` dict exists with keys: `equalized_odds_tpr`, `equalized_odds_fpr`, `predictive_parity`, `max_fnr_gap`, `max_fnr_groups`.

8. `test_intersections_present`: Assert `intersections` has 3 keys: `language_x_rural`, `sex_x_poverty`, `language_x_region`. Each intersection has `groups` with at least 3 non-empty groups.

9. `test_intersection_small_sample_flagging`: At least one intersection group has `flagged_small_sample: true` (known from research: aimara_urban=25, aimara_costa=8, etc.).

10. `test_fnr_consistency`: For each dimension, for each group, assert `fnr + tpr` is approximately 1.0 (within 0.001). This validates the metric computation is correct.

11. `test_model_and_threshold`: Assert `model == "lightgbm"` and `threshold` is approximately 0.165716 (within 0.001).

**Human-review print block (at end of test_gate_3_1.py, in a final test):**

`test_print_human_review`: Always passes, but prints:
- "=== FNR BY LANGUAGE GROUP (HARMONIZED) ===" with group name, FNR, n_unweighted for each language group.
- "=== FNR BY GEOGRAPHY ===" with urban vs rural FNR.
- "=== CALIBRATION BY GROUP (HIGH RISK >0.7 UNCALIBRATED) ===" for language dimension: group, n_predicted_high, actual_dropout_rate.
- "=== MAX FNR GAPS BY DIMENSION ===" for each dimension: dimension name, max_fnr_gap, groups.
- "=== FLAGGED SMALL SAMPLE GROUPS ===" listing all groups with flagged_small_sample=true.
- "=== INTERSECTIONAL HIGHLIGHTS ===" for language_x_rural: group, FNR, n_unweighted (sorted by FNR descending).

Use `print()` with `-s` flag hint in docstring (user runs `uv run pytest tests/gates/test_gate_3_1.py -v -s`).

Run with: `uv run pytest tests/gates/test_gate_3_1.py -v -s`
  </action>
  <verify>
Run `uv run pytest tests/gates/test_gate_3_1.py -v -s` and confirm:
1. All 11+ tests pass
2. Human review tables print with actual FNR values by language group
3. Calibration table shows meaningful differences across groups
4. Full regression: `uv run pytest tests/ -x -q` all pass
  </verify>
  <done>
Gate test 3.1 passes with 11+ assertions. FNR by language group, calibration table, and intersectional highlights print for human review. All prior gate tests continue to pass.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete fairness metrics across 6 protected dimensions and 3 intersections, computed with FACTOR07 survey weights using fairlearn MetricFrame. Exported to data/exports/fairness_metrics.json matching M4 schema. Gate test 3.1 validates structure and prints findings for review.
  </what-built>
  <how-to-verify>
Review the gate test output (already printed by Task 2). Focus on:

1. **FNR gap direction**: Do indigenous language groups (quechua, aimara, other_indigenous) have HIGHER FNR than castellano? Higher FNR = model misses more dropouts in that group = equity concern. This is the core finding.

2. **Calibration by group**: Among students predicted "high risk" (uncalibrated prob > 0.7), is the actual dropout rate similar across groups? If the model says "high risk" but actual dropout differs by 20+ percentage points between groups, the model's risk label means different things for different populations.

3. **Intersectional findings**: Do language x rurality intersections reveal compounded disadvantage (e.g., quechua_rural has highest FNR)? Are findings for <50 observation groups appropriately flagged and not over-interpreted?

4. **Nationality dimension**: es_peruano=0 has only 27 students -- confirm it is flagged and findings noted as unreliable.

5. **Overall pattern**: Do the fairness findings align with the descriptive statistics from Phase 4 (Awajun 0.2047 dropout rate vs Castellano 0.1526)?

Run: `uv run pytest tests/gates/test_gate_3_1.py -v -s` to see the tables.
Optionally: `cat data/exports/fairness_metrics.json | python -m json.tool | head -100` to inspect JSON structure.
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues with the fairness findings</resume-signal>
</task>

</tasks>

<verification>
1. `uv run python src/fairness/metrics.py` completes without errors
2. `uv run pytest tests/gates/test_gate_3_1.py -v -s` -- all tests pass, review tables print
3. `uv run pytest tests/ -x -q` -- full regression passes (all 50+ prior tests + 11+ new)
4. `python -c "import json; d=json.load(open('data/exports/fairness_metrics.json')); print(len(d['dimensions']), 'dimensions,', len(d['intersections']), 'intersections')"` -- prints "7 dimensions, 3 intersections"
</verification>

<success_criteria>
1. fairness_metrics.json exists with 7 dimensions (language, language_disaggregated, sex, geography, region, poverty, nationality) and 3 intersections
2. Each dimension has per-group TPR/FPR/FNR/precision/PR-AUC + calibration_high_risk + gaps
3. Small sample groups flagged (aimara < 100, es_peruano=0 < 100, several intersections < 50)
4. Gate test 3.1: 11+ tests pass
5. FNR by language group printed for human review showing equity gap direction
6. Calibration-by-group printed showing whether "high risk" means different things per group
7. Human approves findings
</success_criteria>

<output>
After completion, create `.planning/phases/08-subgroup-fairness-metrics/08-01-SUMMARY.md`
</output>
