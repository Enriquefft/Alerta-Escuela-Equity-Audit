---
phase: 02-multi-year-loader-harmonization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/enaho.py
  - tests/unit/test_enaho_loader.py
  - tests/gates/test_gate_1_2.py
autonomous: true

must_haves:
  truths:
    - "load_all_years() returns a pooled polars DataFrame with ~150K-160K rows across 6 years (2018-2023)"
    - "Each row has a year column identifying its source year"
    - "p300a_harmonized column collapses codes 10-15 to code 3 for cross-year comparison"
    - "p300a_original column preserves raw INEI codes including disaggregated 10-15 for 2020+ analysis"
    - "2020 COVID year loads without crashing despite 52% P303 nulls"
    - "Gate test 1.2 passes all assertions"
  artifacts:
    - path: "src/data/enaho.py"
      provides: "PooledENAHOResult, POOLED_COLUMNS, harmonize_p300a(), load_all_years(), P303-null handling in load_enaho_year()"
      contains: "class PooledENAHOResult"
    - path: "tests/unit/test_enaho_loader.py"
      provides: "Unit tests for harmonization and multi-year logic"
      contains: "TestHarmonizeP300A"
    - path: "tests/gates/test_gate_1_2.py"
      provides: "Gate test validating pooled data, year coverage, harmonization stability"
      contains: "test_gate_1_2"
  key_links:
    - from: "src/data/enaho.py:load_all_years"
      to: "src/data/enaho.py:load_enaho_year"
      via: "iterates over years calling load_enaho_year()"
      pattern: "load_enaho_year\\(year\\)"
    - from: "src/data/enaho.py:load_all_years"
      to: "src/data/enaho.py:harmonize_p300a"
      via: "calls harmonize_p300a on pooled DataFrame after concat"
      pattern: "harmonize_p300a\\(pooled\\)"
    - from: "src/data/enaho.py:load_enaho_year"
      to: "P303-null drop"
      via: "drops P303-null rows before existing null-threshold check"
      pattern: "P303.*is_not_null"
---

<objective>
Extend the Phase 1 ENAHO single-year loader to stack 6 years (2018-2023) into one consistent pooled DataFrame with P300A mother tongue harmonization. This is the foundation for all downstream multi-year analysis (features, modeling, fairness).

Purpose: Requirements DATA-03 (multi-year loading) and DATA-04 (P300A harmonization) are both satisfied here. The pooled dataset is the input to Phase 3 (spatial merges) and all subsequent phases.

Output: Modified `src/data/enaho.py` with `load_all_years()`, `harmonize_p300a()`, P303-null handling; new unit tests; new gate test 1.2.
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-multi-year-loader-harmonization/02-RESEARCH.md
@.planning/phases/01-enaho-single-year-loader/01-01-SUMMARY.md
@src/data/enaho.py
@tests/unit/test_enaho_loader.py
@tests/gates/test_gate_1_1.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend enaho.py with P303-null handling, multi-year loader, and P300A harmonization</name>
  <files>src/data/enaho.py</files>
  <action>
Modify `src/data/enaho.py` with these changes (in order):

**1. Add P303-null row dropping to `load_enaho_year()` (BEFORE step 4 -- the existing P303/P306 null-fill logic)**

Insert between step 3b (drop unmatched Module 300 rows) and step 4 (handle nulls in P303/P306):

```python
# 3c. Drop rows where P303 is null (COVID reduced questionnaire)
# These rows cannot contribute to dropout analysis (no prior enrollment info).
# Affects 2020 (~52.3% of school-age) and 2021 (~4.6%).
p303_null_count = df["P303"].null_count()
if p303_null_count > 0:
    pre_p303 = len(df)
    df = df.filter(pl.col("P303").is_not_null())
    n_p303_dropped = pre_p303 - len(df)
    frac_p303 = n_p303_dropped / pre_p303 if pre_p303 > 0 else 0
    warnings.append(
        f"Dropped {n_p303_dropped} rows ({frac_p303:.1%}) with P303 null "
        f"(COVID reduced questionnaire / phone interview)"
    )
    logger.info(
        "Dropped %d P303-null rows (%.1f%%), %d remaining",
        n_p303_dropped, frac_p303 * 100, len(df),
    )
```

This ensures `load_enaho_year(2020)` no longer raises ValueError. The existing step 4 null-fill logic then handles any remaining tiny null fractions (<0.5%).

**2. Add POOLED_COLUMNS constant** (after the existing _KEY_OVERRIDES constant):

```python
# Columns selected for the pooled multi-year DataFrame.
# Only these ~20 columns are kept before vertical concatenation to ensure
# identical schemas across years (raw merged DataFrames have 512-548 columns).
POOLED_COLUMNS = [
    # Identifiers
    "CONGLOME", "VIVIENDA", "HOGAR", "CODPERSO",
    # Geographic
    "UBIGEO", "DOMINIO", "ESTRATO",
    # Demographics (Module 200)
    "P207",     # Sex (1=Male, 2=Female)
    "P208A",    # Age
    # Education (Module 300)
    "P300A",    # Mother tongue (original code)
    "P301A",    # Education level
    "P303",     # Enrolled last year
    "P306",     # Enrolled this year
    "P307",     # Currently attending
    # Survey weight
    "FACTOR07",
    # Constructed
    "dropout",  # Binary dropout target
    # Year identifier
    "year",     # Added by load_all_years
]
```

**3. Add PooledENAHOResult dataclass** (after ENAHOResult):

```python
@dataclass
class PooledENAHOResult:
    """Container for pooled multi-year ENAHO data.

    Attributes
    ----------
    df : pl.DataFrame
        Pooled DataFrame with year column and harmonized P300A.
    per_year_stats : list[dict]
        Stats dict from each load_enaho_year() call.
    warnings : list[str]
        All warnings, prefixed with [year].
    """
    df: pl.DataFrame
    per_year_stats: list[dict] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
```

**4. Add harmonize_p300a() function** (in the Public API section, before load_all_years):

```python
# Disaggregated indigenous language codes introduced in ENAHO 2020+.
# These were previously aggregated under code 3 ("Otra lengua nativa").
_DISAGG_CODES = [10, 11, 12, 13, 14, 15]


def harmonize_p300a(df: pl.DataFrame) -> pl.DataFrame:
    """Add harmonized and original P300A mother tongue columns.

    - p300a_original: raw code from INEI (preserves 10-15 for 2020+ analysis)
    - p300a_harmonized: codes 10-15 collapsed to 3 for cross-year comparison

    P300A code reference (verified from Stata value labels):
        1 = Quechua
        2 = Aymara
        3 = Otra lengua nativa (aggregate pre-2020; residual 2020+)
        4 = Castellano (~80% of respondents)
        6 = Portugues
        7 = Otra lengua extranjera
        8 = No escucha/no habla
        9 = Lengua de senas peruanas
       10 = Ashaninka (2020+)
       11 = Awajun/Aguarun (2020+)
       12 = Shipibo-Konibo (2020+)
       13 = Shawi/Chayahuita (2020+)
       14 = Matsigenka/Machiguenga (2020+)
       15 = Achuar (2020+)
    """
    return df.with_columns([
        pl.col("P300A").alias("p300a_original"),
        pl.when(pl.col("P300A").is_in(_DISAGG_CODES))
        .then(pl.lit(3))
        .otherwise(pl.col("P300A"))
        .cast(pl.Int64)
        .alias("p300a_harmonized"),
    ])
```

**5. Add load_all_years() function** (in the Public API section, after harmonize_p300a):

```python
def load_all_years(
    years: list[int] | None = None,
) -> PooledENAHOResult:
    """Load and pool ENAHO data across multiple years.

    Calls :func:`load_enaho_year` for each year, selects a fixed column set
    (POOLED_COLUMNS) for schema consistency, concatenates vertically, and
    applies P300A harmonization.

    Parameters
    ----------
    years : list[int] | None
        Years to load. Defaults to 2018-2023 (2024 not yet available).

    Returns
    -------
    PooledENAHOResult
        Pooled DataFrame with year column and p300a_harmonized/p300a_original.
    """
    if years is None:
        years = list(range(2018, 2024))  # 2018-2023

    frames: list[pl.DataFrame] = []
    all_stats: list[dict] = []
    all_warnings: list[str] = []

    for year in years:
        logger.info("Loading ENAHO year %d...", year)
        result = load_enaho_year(year)

        # Add year column
        df = result.df.with_columns(pl.lit(year).alias("year"))

        # Select only columns needed by downstream phases
        available = [c for c in POOLED_COLUMNS if c in df.columns]
        df = df.select(available)

        frames.append(df)
        all_stats.append(result.stats)
        all_warnings.extend(
            [f"[{year}] {w}" for w in result.warnings]
        )
        logger.info(
            "Year %d: %d rows selected (%d columns)",
            year, len(df), len(available),
        )

    # Vertical concat (enforces identical schemas)
    pooled = pl.concat(frames, how="vertical")
    logger.info("Pooled DataFrame: %d rows, %d columns", len(pooled), len(pooled.columns))

    # Apply P300A harmonization on the pooled data
    pooled = harmonize_p300a(pooled)

    logger.info(
        "Harmonization complete. Columns: %s",
        sorted(pooled.columns),
    )

    return PooledENAHOResult(
        df=pooled,
        per_year_stats=all_stats,
        warnings=all_warnings,
    )
```

**6. Update the module docstring** to mention multi-year loading:

Change the first line from:
```
"""ENAHO single-year data loader.
```
to:
```
"""ENAHO data loader (single-year and multi-year).
```

And update the Usage section to also show:
```python
    from data.enaho import load_all_years

    pooled = load_all_years()
    print(pooled.df.height, pooled.per_year_stats)
```

**IMPORTANT:** Do NOT change any existing function signatures or behavior except for the P303-null drop insertion in `load_enaho_year()`. The existing 29 tests must still pass.
  </action>
  <verify>
Run existing tests to confirm no regressions:

```bash
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit && uv run pytest tests/unit/ -v
```

All 28 unit tests must still pass. Then smoke-test the 2020 loader to confirm P303-null handling works:

```bash
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit && uv run python -c "
import sys; sys.path.insert(0, 'src')
from data.enaho import load_enaho_year
result = load_enaho_year(2020)
print(f'2020: {result.stats}')
print(f'Warnings: {result.warnings}')
"
```

Should print ~13K-14K rows for 2020 with a warning about P303-null rows dropped.
  </verify>
  <done>
- `load_enaho_year(2020)` returns without error, with ~13,755 usable rows and a P303-null drop warning
- `load_enaho_year(2023)` still returns the same results as before (29 existing tests pass)
- `PooledENAHOResult`, `harmonize_p300a()`, `load_all_years()`, `POOLED_COLUMNS` are importable from `data.enaho`
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for harmonization + multi-year loading, and gate test 1.2</name>
  <files>tests/unit/test_enaho_loader.py, tests/gates/test_gate_1_2.py</files>
  <action>
**1. Add harmonization and multi-year unit tests to `tests/unit/test_enaho_loader.py`:**

Add these test classes at the end of the file:

```python
from data.enaho import PooledENAHOResult, harmonize_p300a, _DISAGG_CODES


class TestHarmonizeP300A:
    """Tests for the P300A mother tongue harmonization function."""

    def test_codes_10_to_15_collapse_to_3(self):
        """Disaggregated indigenous codes 10-15 should collapse to 3."""
        df = pl.DataFrame({"P300A": [10, 11, 12, 13, 14, 15]})
        result = harmonize_p300a(df)
        assert result["p300a_harmonized"].to_list() == [3, 3, 3, 3, 3, 3]

    def test_original_codes_preserved(self):
        """p300a_original should keep the raw INEI codes."""
        df = pl.DataFrame({"P300A": [10, 11, 12, 13, 14, 15]})
        result = harmonize_p300a(df)
        assert result["p300a_original"].to_list() == [10, 11, 12, 13, 14, 15]

    def test_non_disagg_codes_unchanged(self):
        """Codes 1, 2, 3, 4 should pass through harmonization unchanged."""
        df = pl.DataFrame({"P300A": [1, 2, 3, 4]})
        result = harmonize_p300a(df)
        assert result["p300a_harmonized"].to_list() == [1, 2, 3, 4]

    def test_mixed_codes(self):
        """Mixed bag of old and new codes produces correct harmonization."""
        df = pl.DataFrame({"P300A": [4, 1, 10, 3, 15, 2]})
        result = harmonize_p300a(df)
        assert result["p300a_harmonized"].to_list() == [4, 1, 3, 3, 3, 2]
        assert result["p300a_original"].to_list() == [4, 1, 10, 3, 15, 2]

    def test_harmonized_column_is_int64(self):
        """p300a_harmonized should be Int64 dtype."""
        df = pl.DataFrame({"P300A": [1, 10, 4]})
        result = harmonize_p300a(df)
        assert result["p300a_harmonized"].dtype == pl.Int64

    def test_null_p300a_preserved(self):
        """Null P300A values should remain null in both columns."""
        df = pl.DataFrame({"P300A": [1, None, 10]}, schema={"P300A": pl.Int64})
        result = harmonize_p300a(df)
        assert result["p300a_harmonized"][1] is None
        assert result["p300a_original"][1] is None

    def test_disagg_codes_constant_complete(self):
        """_DISAGG_CODES should contain exactly codes 10-15."""
        assert sorted(_DISAGG_CODES) == [10, 11, 12, 13, 14, 15]


class TestPooledENAHOResult:
    """Tests for the PooledENAHOResult container."""

    def test_defaults(self):
        """PooledENAHOResult has sensible defaults."""
        df = pl.DataFrame({"x": [1, 2, 3]})
        result = PooledENAHOResult(df=df)
        assert isinstance(result.df, pl.DataFrame)
        assert result.per_year_stats == []
        assert result.warnings == []

    def test_with_stats_and_warnings(self):
        """PooledENAHOResult stores provided stats and warnings."""
        df = pl.DataFrame({"x": [1]})
        stats = [{"year": 2023, "total_rows": 100}]
        warnings = ["[2023] test warning"]
        result = PooledENAHOResult(df=df, per_year_stats=stats, warnings=warnings)
        assert len(result.per_year_stats) == 1
        assert result.warnings[0].startswith("[2023]")
```

**2. Create `tests/gates/test_gate_1_2.py`:**

This gate test validates the complete pooled multi-year dataset against real ENAHO data. Model it after `test_gate_1_1.py`.

```python
"""Gate Test 1.2: Multi-Year Loader + Harmonization Validation

Validates that load_all_years() produces a correctly pooled dataset:
- Pooled row count in expected range (~150K-160K)
- All 6 years present (2018-2023)
- Harmonization columns exist and are correct
- No disaggregated codes (10-15) in harmonized column
- Harmonization stability: code 3 proportion ratio < 2.0x
- Dropout count in expected range (~18K+)
- Column schema validation
- Per-year stats summary printed
"""

import sys
sys.path.insert(0, "src")

import polars as pl
from data.enaho import load_all_years


def test_gate_1_2():
    result = load_all_years()
    df = result.df

    # Print warnings if any
    if result.warnings:
        print("\n--- WARNINGS ---")
        for w in result.warnings:
            print(f"  WARN: {w}")

    # --- Pooled row count ---
    total = df.height
    assert 130_000 <= total <= 190_000, f"FAIL: pooled rows {total} outside [130K, 190K]"
    print(f"  PASS: pooled row count = {total:,}")

    # --- Year coverage ---
    years = sorted(df["year"].unique().to_list())
    assert years == [2018, 2019, 2020, 2021, 2022, 2023], (
        f"FAIL: expected years 2018-2023, got {years}"
    )
    print(f"  PASS: year coverage = {years}")

    # --- Per-year row counts ---
    print("\n--- PER-YEAR BREAKDOWN ---")
    for year in years:
        year_df = df.filter(pl.col("year") == year)
        n = year_df.height
        n_dropout = year_df.filter(pl.col("dropout")).height
        wt_total = year_df["FACTOR07"].sum()
        wt_dropout = year_df.filter(pl.col("dropout"))["FACTOR07"].sum()
        wt_rate = wt_dropout / wt_total if wt_total > 0 else 0.0
        print(f"  {year}: {n:>7,} rows, {n_dropout:>5,} dropouts, {wt_rate:.2%} weighted rate")

    # --- Harmonization columns exist ---
    assert "p300a_original" in df.columns, "FAIL: missing p300a_original"
    assert "p300a_harmonized" in df.columns, "FAIL: missing p300a_harmonized"
    print("  PASS: harmonization columns present")

    # --- No disaggregated codes in harmonized column ---
    harmonized_vals = df["p300a_harmonized"].drop_nulls().unique().to_list()
    for code in [10, 11, 12, 13, 14, 15]:
        assert code not in harmonized_vals, (
            f"FAIL: disaggregated code {code} found in p300a_harmonized"
        )
    print("  PASS: no disaggregated codes (10-15) in harmonized column")

    # --- Disaggregated codes present in original for 2020+ ---
    post2020 = df.filter(pl.col("year") >= 2020)
    original_vals = post2020["p300a_original"].drop_nulls().unique().to_list()
    disagg_present = [c for c in [10, 11, 12, 13, 14, 15] if c in original_vals]
    assert len(disagg_present) > 0, (
        "FAIL: no disaggregated codes found in p300a_original for 2020+"
    )
    print(f"  PASS: disaggregated codes in p300a_original for 2020+: {sorted(disagg_present)}")

    # --- Harmonization stability: code 3 proportion ---
    proportions = {}
    for year in years:
        year_df = df.filter(pl.col("year") == year)
        n_code3 = year_df.filter(pl.col("p300a_harmonized") == 3).height
        prop = n_code3 / year_df.height if year_df.height > 0 else 0
        proportions[year] = prop

    ratio = max(proportions.values()) / min(proportions.values()) if min(proportions.values()) > 0 else float("inf")
    assert ratio < 2.0, (
        f"FAIL: harmonized code 3 proportion ratio {ratio:.2f} exceeds 2.0x. "
        f"Proportions: {proportions}"
    )
    print(f"  PASS: harmonization stability ratio = {ratio:.2f} (< 2.0x)")
    for year, prop in sorted(proportions.items()):
        print(f"    {year}: {prop:.3%} code 3 (harmonized)")

    # --- Dropout count ---
    total_dropouts = df.filter(pl.col("dropout")).height
    assert total_dropouts >= 18_000, (
        f"FAIL: expected 18K+ dropouts, got {total_dropouts:,}"
    )
    print(f"  PASS: total dropouts = {total_dropouts:,}")

    # --- Column schema validation ---
    expected_cols = [
        "UBIGEO", "P208A", "P207", "P303", "P306",
        "FACTOR07", "dropout", "year",
        "p300a_original", "p300a_harmonized",
    ]
    for col in expected_cols:
        assert col in df.columns, f"FAIL: missing column {col}"
    print(f"  PASS: all expected columns present")

    # --- Per-year stats from loader ---
    assert len(result.per_year_stats) == 6, (
        f"FAIL: expected 6 per-year stats, got {len(result.per_year_stats)}"
    )
    print(f"  PASS: {len(result.per_year_stats)} per-year stats collected")

    # --- Summary ---
    print(f"\n--- GATE TEST 1.2 SUMMARY ---")
    print(f"  Pooled rows: {total:,}")
    print(f"  Years: {years}")
    print(f"  Total dropouts: {total_dropouts:,}")
    print(f"  Harmonization stability: {ratio:.2f}x")
    print(f"  Warnings: {len(result.warnings)}")
    print(f"  Columns: {sorted(df.columns)}")
```

**IMPORTANT:** The gate test 1.2 loads ALL 6 years of real data. It will take ~60-90 seconds. Run with `-s` flag to see print output.
  </action>
  <verify>
Run unit tests first (fast):

```bash
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit && uv run pytest tests/unit/test_enaho_loader.py -v
```

All existing + new unit tests must pass (28 existing + ~9 new = ~37).

Then run gate test 1.2 (slow, ~60-90s, needs real data):

```bash
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit && uv run pytest tests/gates/test_gate_1_2.py -v -s
```

Gate test must pass with printed summary showing 6 years, ~150K+ rows, ~22K+ dropouts.

Finally run full test suite to confirm no regressions:

```bash
cd /home/hybridz/Projects/Alerta-Escuela-Equity-Audit && uv run pytest tests/ -v
```

All tests green (unit + gates).
  </verify>
  <done>
- 9 new unit tests pass: 7 for harmonize_p300a (codes collapse, original preserved, nulls, dtype, mixed), 2 for PooledENAHOResult
- Gate test 1.2 passes: pooled rows ~150K-160K, years [2018-2023], no codes 10-15 in harmonized column, stability ratio < 2.0x, dropouts >= 18K
- All 28 existing tests still pass (no regressions)
- Per-year breakdown printed showing 2020 with reduced rows (~13K-14K) as expected
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- all tests pass (unit + gates)
2. `load_all_years()` returns PooledENAHOResult with ~150K-160K rows
3. `p300a_harmonized` has NO codes 10-15; `p300a_original` has codes 10-15 for 2020+
4. `load_enaho_year(2020)` does not raise ValueError (P303-null rows dropped with warning)
5. Year column contains exactly [2018, 2019, 2020, 2021, 2022, 2023]
6. Harmonization stability ratio < 2.0x
</verification>

<success_criteria>
1. `load_all_years()` returns a pooled polars DataFrame with ~150K-160K rows, ~22K+ unweighted dropouts, and a `year` column spanning 2018-2023
2. `p300a_harmonized` column collapses codes 10-15 back to code 3; `p300a_original` preserves disaggregated codes for 2020+ analysis
3. Harmonization stability: code 3 proportion ratio < 2.0x across years
4. Gate test 1.2 passes all assertions (year coverage, column consistency, harmonization stability, pooled counts)
5. All pre-existing tests still pass (no regressions from P303-null handling)
</success_criteria>

<output>
After completion, create `.planning/phases/02-multi-year-loader-harmonization/02-01-SUMMARY.md`
</output>
