---
phase: 06-lightgbm-xgboost
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/lightgbm_xgboost.py
  - tests/gates/test_gate_2_2.py
  - data/exports/model_results.json
  - data/processed/predictions_lgbm.parquet
  - data/processed/predictions_xgb.parquet
  - data/processed/model_lgbm.joblib
  - data/processed/model_xgb.joblib
  - data/exports/figures/pr_curve_lgbm.png
  - data/exports/figures/pr_curve_xgb.png
autonomous: false

must_haves:
  truths:
    - "LightGBM validation PR-AUC (weighted) exceeds LR baseline of 0.2103"
    - "XGBoost validation PR-AUC is within 5% of LightGBM (algorithm-independence)"
    - "No single feature accounts for more than 50% of LightGBM normalized gain importance"
    - "model_results.json contains logistic_regression, lightgbm, and xgboost entries"
    - "Top-10 LightGBM feature importances printed; age, poverty, rurality appear in top-5"
    - "Gate test 2.2 passes all assertions"
  artifacts:
    - path: "src/models/lightgbm_xgboost.py"
      provides: "LightGBM and XGBoost training pipelines with Optuna tuning"
      exports: ["run_lgbm_xgb_pipeline"]
    - path: "data/exports/model_results.json"
      provides: "Updated JSON with lightgbm and xgboost entries (preserving LR)"
    - path: "data/processed/predictions_lgbm.parquet"
      provides: "LightGBM per-row predictions (val + test) for Phase 8 fairness"
    - path: "data/processed/predictions_xgb.parquet"
      provides: "XGBoost per-row predictions (val + test) for Phase 8"
    - path: "data/processed/model_lgbm.joblib"
      provides: "Persisted LightGBM model for Phase 7 calibration + ONNX"
    - path: "data/processed/model_xgb.joblib"
      provides: "Persisted XGBoost model"
    - path: "data/exports/figures/pr_curve_lgbm.png"
      provides: "LightGBM precision-recall curve"
    - path: "data/exports/figures/pr_curve_xgb.png"
      provides: "XGBoost precision-recall curve"
    - path: "tests/gates/test_gate_2_2.py"
      provides: "Gate test validating LightGBM/XGBoost outputs"
  key_links:
    - from: "src/models/lightgbm_xgboost.py"
      to: "src/models/baseline.py"
      via: "imports create_temporal_splits, _df_to_numpy, compute_metrics, _threshold_analysis, constants"
      pattern: "from models\\.baseline import"
    - from: "src/models/lightgbm_xgboost.py"
      to: "src/data/features.py"
      via: "MODEL_FEATURES import"
      pattern: "from data\\.features import MODEL_FEATURES"
    - from: "src/models/lightgbm_xgboost.py"
      to: "data/exports/model_results.json"
      via: "JSON read-merge-write (preserving LR entry)"
      pattern: "json\\.load.*json\\.dump"
    - from: "tests/gates/test_gate_2_2.py"
      to: "data/exports/model_results.json"
      via: "json.load validation of all three model entries"
      pattern: "json\\.load"
---

<objective>
Train Optuna-tuned LightGBM (primary model, matching Alerta Escuela's algorithm) and XGBoost (comparison model) on the ENAHO dropout dataset, evaluate both with survey-weighted metrics, extract feature importances, update model_results.json, and verify algorithm-independence of fairness findings via gate test 2.2.

Purpose: LightGBM is the primary model whose predictions drive all downstream fairness analysis (Phases 8-10) and SHAP interpretability (Phase 9). XGBoost serves as an algorithm-independence check -- if both gradient boosting algorithms produce similar performance and feature importances, we can be confident that fairness findings are not artifacts of one particular algorithm. This phase reuses all evaluation patterns from Phase 5's baseline.py.

Output: `src/models/lightgbm_xgboost.py`, `tests/gates/test_gate_2_2.py`, updated `data/exports/model_results.json`, prediction parquets, persisted models, PR curve PNGs
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-lightgbm-xgboost/06-CONTEXT.md
@.planning/phases/06-lightgbm-xgboost/06-RESEARCH.md
@.planning/phases/05-baseline-model-temporal-splits/05-01-SUMMARY.md

Key source files:
@src/models/baseline.py -- reuse create_temporal_splits, _df_to_numpy, compute_metrics, _threshold_analysis, constants
@src/data/features.py -- MODEL_FEATURES list (25 features)
@data/processed/enaho_with_features.parquet -- feature matrix input (150,135 rows)
@data/exports/model_results.json -- existing LR entry (must preserve when adding LGBM/XGB entries)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create src/models/lightgbm_xgboost.py with LightGBM and XGBoost training pipelines</name>
  <files>
    src/models/lightgbm_xgboost.py
    data/exports/model_results.json
    data/processed/predictions_lgbm.parquet
    data/processed/predictions_xgb.parquet
    data/processed/model_lgbm.joblib
    data/processed/model_xgb.joblib
    data/exports/figures/pr_curve_lgbm.png
    data/exports/figures/pr_curve_xgb.png
  </files>
  <action>
Create `src/models/lightgbm_xgboost.py` implementing both gradient boosting training pipelines.

**Module docstring and imports:**

```python
"""LightGBM and XGBoost models with Optuna hyperparameter tuning.

Trains Optuna-tuned LightGBM (primary, matching Alerta Escuela) and XGBoost
(comparison) models using the same temporal splits and evaluation patterns
established in baseline.py.

Usage::

    uv run python src/models/lightgbm_xgboost.py
"""

from __future__ import annotations

import json
import logging
import sys
from datetime import datetime, timezone
from pathlib import Path

import joblib
import numpy as np
import polars as pl
import optuna
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from lightgbm import LGBMClassifier, early_stopping, log_evaluation
from xgboost import XGBClassifier
from sklearn.metrics import (
    average_precision_score,
    precision_recall_curve,
    PrecisionRecallDisplay,
)

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from data.features import MODEL_FEATURES
from models.baseline import (
    create_temporal_splits,
    _df_to_numpy,
    compute_metrics,
    _threshold_analysis,
    TRAIN_YEARS,
    VALIDATE_YEAR,
    TEST_YEAR,
    FIXED_THRESHOLDS,
    ID_COLUMNS,
)
from utils import find_project_root

logger = logging.getLogger(__name__)
```

**Equity features set (for console highlighting):**

```python
_EQUITY_FEATURES = {
    "poverty_quintile", "rural", "lang_other_indigenous",
    "lang_quechua", "lang_aimara", "age", "es_mujer", "es_peruano",
}
```

**Function 1: `_compute_scale_pos_weight(y_train: np.ndarray) -> float`**

Compute scale_pos_weight = n_neg / n_pos from training labels:
```python
def _compute_scale_pos_weight(y_train: np.ndarray) -> float:
    n_pos = int(y_train.sum())
    n_neg = len(y_train) - n_pos
    spw = n_neg / n_pos
    print(f"Class imbalance: {n_neg:,} neg / {n_pos:,} pos = scale_pos_weight={spw:.4f}")
    return spw
```

**Function 2: `_lgbm_objective(trial, X_train, y_train, w_train, X_val, y_val, w_val, spw) -> float`**

Optuna objective for LightGBM. Returns weighted validation PR-AUC.

Search space (all from 06-RESEARCH.md):
- `learning_rate`: suggest_float, [0.01, 0.3], log=True
- `num_leaves`: suggest_int, [8, 128]
- `max_depth`: suggest_int, [3, 12]
- `min_child_samples`: suggest_int, [5, 100]
- `subsample`: suggest_float, [0.5, 1.0]
- `colsample_bytree`: suggest_float, [0.5, 1.0]
- `reg_alpha`: suggest_float, [1e-8, 10.0], log=True
- `reg_lambda`: suggest_float, [1e-8, 10.0], log=True

Fixed params: n_estimators=500, scale_pos_weight=spw, importance_type="gain", verbose=-1, random_state=42.

Training call:
```python
model = LGBMClassifier(n_estimators=500, **trial_params, scale_pos_weight=spw,
                        importance_type="gain", verbose=-1, random_state=42)
model.fit(
    X_train, y_train,
    sample_weight=w_train,
    eval_set=[(X_val, y_val)],
    eval_sample_weight=[w_val],
    eval_metric="average_precision",
    callbacks=[early_stopping(50), log_evaluation(0)],
)
y_prob = model.predict_proba(X_val)[:, 1]
return average_precision_score(y_val, y_prob, sample_weight=w_val)
```

**CRITICAL API NOTES for LightGBM:**
- Use `eval_sample_weight=[w_val]` (list wrapping is required)
- Use `callbacks=[early_stopping(50), log_evaluation(0)]` in `.fit()`, NOT in constructor
- Use `eval_metric="average_precision"` (NOT "aucpr")
- Use `verbose=-1` to suppress output
- DO NOT use `is_unbalance=True` -- use `scale_pos_weight` instead

**Function 3: `_xgb_objective(trial, X_train, y_train, w_train, X_val, y_val, w_val, spw) -> float`**

Optuna objective for XGBoost. Returns weighted validation PR-AUC.

Search space:
- `learning_rate`: suggest_float, [0.01, 0.3], log=True
- `max_depth`: suggest_int, [3, 10]
- `min_child_weight`: suggest_int, [1, 20]
- `gamma`: suggest_float, [1e-8, 5.0], log=True
- `subsample`: suggest_float, [0.5, 1.0]
- `colsample_bytree`: suggest_float, [0.5, 1.0]
- `reg_alpha`: suggest_float, [1e-8, 10.0], log=True
- `reg_lambda`: suggest_float, [1e-8, 10.0], log=True

Fixed params: n_estimators=500, early_stopping_rounds=50, scale_pos_weight=spw, eval_metric="aucpr", importance_type="gain", verbosity=0, random_state=42.

Training call:
```python
model = XGBClassifier(n_estimators=500, **trial_params, scale_pos_weight=spw,
                       eval_metric="aucpr", early_stopping_rounds=50,
                       importance_type="gain", verbosity=0, random_state=42)
model.fit(
    X_train, y_train,
    sample_weight=w_train,
    eval_set=[(X_val, y_val)],
    sample_weight_eval_set=[w_val],
    verbose=False,
)
y_prob = model.predict_proba(X_val)[:, 1]
return average_precision_score(y_val, y_prob, sample_weight=w_val)
```

**CRITICAL API NOTES for XGBoost (differences from LightGBM):**
- Use `sample_weight_eval_set=[w_val]` (NOT `eval_sample_weight`)
- Use `early_stopping_rounds=50` in **constructor** (NOT in .fit() and NOT as callback)
- Use `eval_metric="aucpr"` (NOT "average_precision")
- Use `verbosity=0` in constructor AND `verbose=False` in .fit()
- Best iteration is `model.best_iteration` (NO trailing underscore)
- Use `min_child_weight` (NOT `min_child_samples`)
- Has `gamma` parameter (minimum loss reduction for split)

**Function 4: `_extract_feature_importances(model, feature_names: list[str]) -> list[tuple[str, float]]`**

Extract and normalize feature importances:
```python
def _extract_feature_importances(model, feature_names: list[str]) -> list[tuple[str, float]]:
    raw_imp = model.feature_importances_
    norm_imp = raw_imp / raw_imp.sum()
    importance_pairs = list(zip(feature_names, norm_imp))
    sorted_imp = sorted(importance_pairs, key=lambda x: x[1], reverse=True)
    return sorted_imp
```

**Function 5: `_print_feature_importances(sorted_importances: list[tuple[str, float]], model_name: str) -> None`**

Print top-10 feature importances for human review:
```python
def _print_feature_importances(sorted_importances, model_name):
    print(f"\n=== TOP-10 {model_name.upper()} FEATURE IMPORTANCES (GAIN, NORMALIZED) ===")
    for rank, (feat, imp) in enumerate(sorted_importances[:10], 1):
        equity_mark = " ***" if feat in _EQUITY_FEATURES else ""
        print(f"  {rank:2d}. {feat:<40s} {imp:.4f}{equity_mark}")

    max_feat, max_imp = sorted_importances[0]
    print(f"\n  Max importance: {max_feat} = {max_imp:.4f} (must be < 0.50)")
    assert max_imp < 0.50, (
        f"Feature {max_feat} has {max_imp:.4f} normalized importance (>50%)"
    )
    print(f"  Importance concentration check: PASS")
```

**Function 6: `_save_predictions_gbm(df, y_prob, y_pred, optimal_threshold, split_name, model_name) -> pl.DataFrame`**

Save predictions parquet, following baseline pattern but parameterized by model_name:
```python
def _save_predictions_gbm(df, y_prob, y_pred, optimal_threshold, split_name, model_name):
    pred_df = df.select(ID_COLUMNS).with_columns([
        pl.Series("prob_dropout", y_prob),
        pl.Series("pred_dropout", y_pred),
        pl.lit(model_name).alias("model"),
        pl.lit(optimal_threshold).alias("threshold"),
        pl.lit(split_name).alias("split"),
    ])
    return pred_df
```

**Function 7: `_plot_pr_curve_gbm(y_true, y_prob, w, thresholds_data, optimal_threshold, model_name, output_path) -> None`**

Generate PR curve PNG, following baseline _plot_pr_curve pattern but with model_name in title/label:
- PrecisionRecallDisplay.from_predictions(y_true, y_prob, sample_weight=w, name=f"{model_name} (weighted)")
- Figure size: (10, 7)
- Horizontal dashed line at weighted base rate
- Markers at 5 fixed thresholds + star at optimal
- Title: f"Precision-Recall Curve: {model_name} (Validation {VALIDATE_YEAR})"
- plt.tight_layout(), savefig(dpi=150), plt.close()

**Function 8: `_train_and_evaluate_model(model_name, model, X_train, y_train, w_train, X_val, y_val, w_val, X_test, y_test, w_test, val_df, test_df, feature_names, study, spw, root) -> dict`**

Common evaluation and export logic for both models:

1. Get predictions: `y_prob_val = model.predict_proba(X_val)[:, 1]` and same for test
2. Threshold analysis on validation: `threshold_data = _threshold_analysis(y_val, y_prob_val, w_val)`
3. Get optimal_threshold from threshold_data
4. Apply threshold: `y_pred_val = (y_prob_val >= optimal_threshold).astype(int)`, same for test
5. Compute metrics (weighted and unweighted) for val and test:
   ```python
   w_val_metrics = compute_metrics(y_val, y_prob_val, y_pred_val, weights=w_val)
   uw_val_metrics = compute_metrics(y_val, y_prob_val, y_pred_val, weights=None)
   w_test_metrics = compute_metrics(y_test, y_prob_test, y_pred_test, weights=w_test)
   uw_test_metrics = compute_metrics(y_test, y_prob_test, y_pred_test, weights=None)
   ```
6. Print metrics comparison (val and test):
   ```python
   print(f"\n=== {model_name.upper()} METRICS: Validation {VALIDATE_YEAR} ===")
   print(f"  {'Metric':<15s} {'Weighted':>12s} {'Unweighted':>12s} {'Diff':>10s}")
   for key in w_val_metrics:
       w_v = w_val_metrics[key]; uw_v = uw_val_metrics[key]
       print(f"  {key:<15s} {w_v:>12.4f} {uw_v:>12.4f} {w_v - uw_v:>+10.4f}")
   ```
   Repeat for test.
7. Feature importances: `sorted_imp = _extract_feature_importances(model, feature_names)`
8. Print feature importances: `_print_feature_importances(sorted_imp, model_name)`
9. Save predictions (val + test combined):
   ```python
   val_preds = _save_predictions_gbm(val_df, y_prob_val, y_pred_val, optimal_threshold, f"validate_{VALIDATE_YEAR}", model_name)
   test_preds = _save_predictions_gbm(test_df, y_prob_test, y_pred_test, optimal_threshold, f"test_{TEST_YEAR}", model_name)
   combined = pl.concat([val_preds, test_preds])
   abbrev = {"lightgbm": "lgbm", "xgboost": "xgb"}[model_name]
   pred_path = root / "data" / "processed" / f"predictions_{abbrev}.parquet"
   combined.write_parquet(pred_path)
   print(f"Predictions saved: {combined.height:,} rows to {pred_path}")
   ```
10. Persist model:
    ```python
    model_path = root / "data" / "processed" / f"model_{abbrev}.joblib"
    joblib.dump(model, model_path)
    print(f"Model saved: {model_path}")
    ```
11. Generate PR curve:
    ```python
    pr_path = root / "data" / "exports" / "figures" / f"pr_curve_{abbrev}.png"
    _plot_pr_curve_gbm(y_val, y_prob_val, w_val, threshold_data, optimal_threshold, model_name, pr_path)
    ```
12. Determine best_iteration based on model type:
    ```python
    if model_name == "lightgbm":
        best_iter = model.best_iteration_   # trailing underscore for LightGBM
    else:
        best_iter = model.best_iteration    # no trailing underscore for XGBoost
    ```
13. Build and return the model entry dict (for model_results.json):
    ```python
    def _round_dict(d, decimals=6):
        return {k: round(v, decimals) if isinstance(v, float) else v for k, v in d.items()}

    entry = {
        "metadata": {
            "model_type": "LGBMClassifier" if model_name == "lightgbm" else "XGBClassifier",
            "train_years": TRAIN_YEARS,
            "validate_year": VALIDATE_YEAR,
            "test_year": TEST_YEAR,
            "n_train": int(X_train.shape[0]),
            "n_validate": int(X_val.shape[0]),
            "n_test": int(X_test.shape[0]),
            "n_features": len(feature_names),
            "feature_names": list(feature_names),
            "best_iteration": int(best_iter),
            "scale_pos_weight": round(float(spw), 4),  # spw passed as parameter
            "optuna_n_trials": study.trials[-1].number + 1,
            "optuna_best_trial": study.best_trial.number,
            "optuna_best_params": study.best_trial.params,
            "calibration_note": "scale_pos_weight distorts probability estimates; Phase 7 handles calibration",
            "generated_at": datetime.now(timezone.utc).isoformat(),
        },
        "metrics": {
            f"validate_{VALIDATE_YEAR}": {
                "weighted": _round_dict(w_val_metrics),
                "unweighted": _round_dict(uw_val_metrics),
            },
            f"test_{TEST_YEAR}": {
                "weighted": _round_dict(w_test_metrics),
                "unweighted": _round_dict(uw_test_metrics),
            },
        },
        "threshold_analysis": threshold_data,
        "feature_importances": [
            {"feature": feat, "importance": round(float(imp), 6)}
            for feat, imp in sorted_imp
        ],
    }
    return entry
    ```

**Function 9: `run_lgbm_xgb_pipeline() -> dict`**

Main orchestration function:

1. **Load data:**
   ```python
   root = find_project_root()
   parquet_path = root / "data" / "processed" / "enaho_with_features.parquet"
   results_path = root / "data" / "exports" / "model_results.json"

   print("Loading feature matrix...")
   df = pl.read_parquet(parquet_path)
   print(f"Loaded: {df.height:,} rows, {df.width} columns")
   ```

2. **Create temporal splits:**
   ```python
   print("\nCreating temporal splits...")
   train_df, val_df, test_df = create_temporal_splits(df)
   ```

3. **Convert to numpy:**
   ```python
   X_train, y_train, w_train = _df_to_numpy(train_df)
   X_val, y_val, w_val = _df_to_numpy(val_df)
   X_test, y_test, w_test = _df_to_numpy(test_df)
   print(f"\nFeature matrix shapes: train={X_train.shape}, val={X_val.shape}, test={X_test.shape}")
   ```

4. **Compute scale_pos_weight:**
   ```python
   spw = _compute_scale_pos_weight(y_train)
   ```

5. **LightGBM Optuna tuning (100 trials):**
   ```python
   print("\n" + "=" * 60)
   print("LIGHTGBM OPTUNA TUNING (100 trials)")
   print("=" * 60)
   optuna.logging.set_verbosity(optuna.logging.WARNING)
   lgbm_study = optuna.create_study(direction="maximize", study_name="lgbm_prauc")
   lgbm_study.optimize(
       lambda trial: _lgbm_objective(trial, X_train, y_train, w_train, X_val, y_val, w_val, spw),
       n_trials=100,
   )
   print(f"\nBest LightGBM trial: #{lgbm_study.best_trial.number}")
   print(f"Best LightGBM val PR-AUC: {lgbm_study.best_trial.value:.4f}")
   print(f"Best params: {lgbm_study.best_trial.params}")
   ```

6. **Retrain best LightGBM:**
   ```python
   print("\nRetraining best LightGBM model...")
   best_lgbm_params = lgbm_study.best_trial.params
   best_lgbm = LGBMClassifier(
       n_estimators=500,
       **best_lgbm_params,
       scale_pos_weight=spw,
       importance_type="gain",
       verbose=-1,
       random_state=42,
   )
   best_lgbm.fit(
       X_train, y_train,
       sample_weight=w_train,
       eval_set=[(X_val, y_val)],
       eval_sample_weight=[w_val],
       eval_metric="average_precision",
       callbacks=[early_stopping(50), log_evaluation(0)],
   )
   print(f"LightGBM best iteration: {best_lgbm.best_iteration_}")
   ```

7. **Evaluate LightGBM:**
   Call `_train_and_evaluate_model("lightgbm", best_lgbm, X_train, y_train, w_train, X_val, y_val, w_val, X_test, y_test, w_test, val_df, test_df, list(MODEL_FEATURES), lgbm_study, spw, root)` to get the model entry dict.

8. **XGBoost Optuna tuning (50 trials):**
   ```python
   print("\n" + "=" * 60)
   print("XGBOOST OPTUNA TUNING (50 trials)")
   print("=" * 60)
   xgb_study = optuna.create_study(direction="maximize", study_name="xgb_prauc")
   xgb_study.optimize(
       lambda trial: _xgb_objective(trial, X_train, y_train, w_train, X_val, y_val, w_val, spw),
       n_trials=50,
   )
   print(f"\nBest XGBoost trial: #{xgb_study.best_trial.number}")
   print(f"Best XGBoost val PR-AUC: {xgb_study.best_trial.value:.4f}")
   print(f"Best params: {xgb_study.best_trial.params}")
   ```

9. **Retrain best XGBoost:**
   ```python
   print("\nRetraining best XGBoost model...")
   best_xgb_params = xgb_study.best_trial.params
   best_xgb = XGBClassifier(
       n_estimators=500,
       **best_xgb_params,
       scale_pos_weight=spw,
       eval_metric="aucpr",
       early_stopping_rounds=50,
       importance_type="gain",
       verbosity=0,
       random_state=42,
   )
   best_xgb.fit(
       X_train, y_train,
       sample_weight=w_train,
       eval_set=[(X_val, y_val)],
       sample_weight_eval_set=[w_val],
       verbose=False,
   )
   print(f"XGBoost best iteration: {best_xgb.best_iteration}")
   ```

10. **Evaluate XGBoost:**
    Call `_train_and_evaluate_model("xgboost", best_xgb, X_train, y_train, w_train, X_val, y_val, w_val, X_test, y_test, w_test, val_df, test_df, list(MODEL_FEATURES), xgb_study, spw, root)` to get the model entry dict.

11. **Algorithm-independence check:**
    ```python
    lgbm_prauc = lgbm_entry["metrics"][f"validate_{VALIDATE_YEAR}"]["weighted"]["pr_auc"]
    xgb_prauc = xgb_entry["metrics"][f"validate_{VALIDATE_YEAR}"]["weighted"]["pr_auc"]
    ratio = xgb_prauc / lgbm_prauc

    print(f"\n=== ALGORITHM-INDEPENDENCE CHECK ===")
    print(f"  LightGBM val PR-AUC (weighted): {lgbm_prauc:.4f}")
    print(f"  XGBoost  val PR-AUC (weighted): {xgb_prauc:.4f}")
    print(f"  Ratio (XGB/LGBM):               {ratio:.4f}")
    print(f"  Threshold:                       >= 0.95 (within 5%)")

    if ratio >= 0.95:
        print(f"  Algorithm-independence:          PASS")
    else:
        print(f"  Algorithm-independence:          WARNING -- XGBoost is {(1-ratio)*100:.1f}% below LightGBM")
    ```

12. **LR baseline comparison:**
    ```python
    print(f"\n=== VS LOGISTIC REGRESSION BASELINE ===")
    print(f"  LR      val PR-AUC (weighted): 0.2103")
    print(f"  LightGBM val PR-AUC (weighted): {lgbm_prauc:.4f} ({'BEATS LR' if lgbm_prauc > 0.2103 else 'DOES NOT BEAT LR'})")
    print(f"  XGBoost  val PR-AUC (weighted): {xgb_prauc:.4f} ({'BEATS LR' if xgb_prauc > 0.2103 else 'DOES NOT BEAT LR'})")
    ```

13. **Merge into model_results.json:**
    ```python
    print(f"\nUpdating model_results.json...")
    with open(results_path, "r") as f:
        model_results = json.load(f)

    assert "logistic_regression" in model_results, "LR baseline entry missing from model_results.json!"

    model_results["lightgbm"] = lgbm_entry
    model_results["xgboost"] = xgb_entry

    with open(results_path, "w") as f:
        json.dump(model_results, f, indent=2)

    print(f"model_results.json updated with lightgbm and xgboost entries")
    print(f"  Keys: {list(model_results.keys())}")
    ```

14. **Final summary:**
    ```python
    print("\n" + "=" * 60)
    print("LIGHTGBM + XGBOOST PIPELINE COMPLETE")
    print("=" * 60)
    print(f"  LightGBM val PR-AUC (W):  {lgbm_prauc:.4f} (LR baseline: 0.2103)")
    print(f"  XGBoost  val PR-AUC (W):  {xgb_prauc:.4f}")
    print(f"  Algorithm-independence:    ratio={ratio:.4f} {'PASS' if ratio >= 0.95 else 'WARNING'}")
    print(f"  Optuna trials:            LGBM={lgbm_study.n_trials}, XGB={xgb_study.n_trials}")
    print(f"  LightGBM best iteration:  {best_lgbm.best_iteration_}")
    print(f"  XGBoost best iteration:   {best_xgb.best_iteration}")
    print(f"  Max feature importance:   {lgbm_entry['feature_importances'][0]['feature']} = {lgbm_entry['feature_importances'][0]['importance']:.4f}")
    print("=" * 60)
    ```

15. Return model_results dict.

**if __name__ == "__main__" block:**
```python
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(name)s - %(levelname)s - %(message)s")
    run_lgbm_xgb_pipeline()
```

**After creating the file, run the pipeline:**
```bash
cd PROJECT_ROOT && uv run python src/models/lightgbm_xgboost.py
```

Expected runtime: ~4-5 minutes (100 LGBM trials + 50 XGB trials + retraining).

**NOTE:** The `_round_dict` helper should be defined at module level or inside `_train_and_evaluate_model`. Reuse it consistently for all float values in JSON output.
  </action>
  <verify>
Run the pipeline:
```bash
uv run python src/models/lightgbm_xgboost.py
```

Then verify outputs:
```bash
ls -la data/exports/model_results.json
ls -la data/processed/predictions_lgbm.parquet
ls -la data/processed/predictions_xgb.parquet
ls -la data/processed/model_lgbm.joblib
ls -la data/processed/model_xgb.joblib
ls -la data/exports/figures/pr_curve_lgbm.png
ls -la data/exports/figures/pr_curve_xgb.png

uv run python -c "
import json
with open('data/exports/model_results.json') as f:
    d = json.load(f)
print('Keys:', list(d.keys()))
print('LR val PR-AUC:', d['logistic_regression']['metrics']['validate_2022']['weighted']['pr_auc'])
print('LGBM val PR-AUC:', d['lightgbm']['metrics']['validate_2022']['weighted']['pr_auc'])
print('XGB val PR-AUC:', d['xgboost']['metrics']['validate_2022']['weighted']['pr_auc'])
print('LGBM top-3 features:', [(f['feature'], f['importance']) for f in d['lightgbm']['feature_importances'][:3]])
print('LGBM optuna trials:', d['lightgbm']['metadata']['optuna_n_trials'])
print('XGB optuna trials:', d['xgboost']['metadata']['optuna_n_trials'])

import polars as pl
lgbm_pred = pl.read_parquet('data/processed/predictions_lgbm.parquet')
xgb_pred = pl.read_parquet('data/processed/predictions_xgb.parquet')
print(f'LGBM predictions: {lgbm_pred.shape}')
print(f'XGB predictions: {xgb_pred.shape}')
"
```

Verify:
- model_results.json has keys: logistic_regression, lightgbm, xgboost
- LightGBM val PR-AUC > 0.2103 (beats LR baseline)
- XGBoost val PR-AUC within 5% of LightGBM
- Top feature importance < 0.50
- Both prediction parquets exist with ~52,112 rows each
- Both model joblib files exist
- Both PR curve PNGs exist
  </verify>
  <done>
- `src/models/lightgbm_xgboost.py` exists with run_lgbm_xgb_pipeline()
- LightGBM Optuna-tuned with 100 trials, val PR-AUC > 0.2103
- XGBoost Optuna-tuned with 50 trials, val PR-AUC within 5% of LightGBM
- No single feature > 50% importance
- model_results.json updated with lightgbm and xgboost entries (LR entry preserved)
- predictions_lgbm.parquet and predictions_xgb.parquet saved
- model_lgbm.joblib and model_xgb.joblib persisted
- pr_curve_lgbm.png and pr_curve_xgb.png generated
- Console output shows feature importances, metrics, algorithm-independence check
  </done>
</task>

<task type="auto">
  <name>Task 2: Create gate test 2.2 for LightGBM/XGBoost validation</name>
  <files>tests/gates/test_gate_2_2.py</files>
  <action>
Create `tests/gates/test_gate_2_2.py` following the established gate test pattern from test_gate_2_1.py.

```python
"""Gate Test 2.2: LightGBM + XGBoost Validation.

Validates:
- LightGBM beats LR baseline (val PR-AUC > 0.2103)
- XGBoost within 5% of LightGBM (algorithm-independence)
- No single feature > 50% importance
- model_results.json has all three model entries with correct schema
- Feature importances normalized and sorted
- Predictions parquets exist with correct schema
- Model joblib files persist and have predict_proba
- PR curve PNGs exist
- Threshold analysis complete for both models
"""

import json
import sys
from pathlib import Path

import numpy as np
import polars as pl
import pytest

sys.path.insert(0, str(Path(__file__).resolve().parents[2] / "src"))

from data.features import MODEL_FEATURES
from utils import find_project_root

ROOT = find_project_root()
RESULTS_PATH = ROOT / "data" / "exports" / "model_results.json"
LGBM_PRED_PATH = ROOT / "data" / "processed" / "predictions_lgbm.parquet"
XGB_PRED_PATH = ROOT / "data" / "processed" / "predictions_xgb.parquet"
LGBM_MODEL_PATH = ROOT / "data" / "processed" / "model_lgbm.joblib"
XGB_MODEL_PATH = ROOT / "data" / "processed" / "model_xgb.joblib"
LGBM_PR_PATH = ROOT / "data" / "exports" / "figures" / "pr_curve_lgbm.png"
XGB_PR_PATH = ROOT / "data" / "exports" / "figures" / "pr_curve_xgb.png"
```

**Fixtures:**

```python
@pytest.fixture(scope="module")
def model_results() -> dict:
    """Load model_results.json once per test module."""
    with open(RESULTS_PATH) as f:
        return json.load(f)
```

**Test functions (11 tests):**

**test_model_results_has_all_models(model_results)**
- Assert "logistic_regression" in model_results (LR entry preserved)
- Assert "lightgbm" in model_results
- Assert "xgboost" in model_results
- Print: "model_results.json keys: {list(model_results.keys())}"

**test_lgbm_beats_lr_baseline(model_results)**
- Get LightGBM weighted val PR-AUC from model_results["lightgbm"]["metrics"]["validate_2022"]["weighted"]["pr_auc"]
- Assert lgbm_prauc > 0.2103 (hardcoded LR baseline)
- Print: "LightGBM val PR-AUC (weighted): {value:.4f} > 0.2103 (LR baseline) PASS"

**test_algorithm_independence(model_results)**
- Get LightGBM and XGBoost weighted val PR-AUC
- Compute ratio = xgb / lgbm
- Assert ratio >= 0.95
- Print both values, the ratio, and PASS/FAIL

**test_lgbm_no_feature_dominance(model_results)**
- Get lightgbm feature_importances (list of {feature, importance} dicts)
- Assert the first entry (highest importance) has importance < 0.50
- Print top-5 feature importances

**test_lgbm_importances_normalized(model_results)**
- Get lightgbm feature_importances
- Sum all importance values
- Assert abs(total - 1.0) < 0.01 (normalized to sum=1 within tolerance)
- Assert len(importances) == 25 (one per feature)
- Assert importances are sorted descending

**test_lgbm_metadata(model_results)**
- Check lightgbm metadata: model_type == "LGBMClassifier"
- n_features == 25
- train_years == [2018, 2019, 2020, 2021]
- validate_year == 2022
- test_year == 2023
- best_iteration > 0
- scale_pos_weight > 4.0 (should be ~4.80)
- optuna_n_trials >= 100
- optuna_best_trial >= 0
- optuna_best_params is a dict with at least "learning_rate" key
- Print: metadata summary

**test_xgb_metadata(model_results)**
- Check xgboost metadata: model_type == "XGBClassifier"
- n_features == 25
- optuna_n_trials >= 50
- optuna_best_params has "learning_rate" key

**test_threshold_analysis_both_models(model_results)**
- For each model in ["lightgbm", "xgboost"]:
  - Assert threshold_analysis exists
  - Assert optimal_threshold between 0.05 and 0.95
  - Assert optimization_target == "max_weighted_f1"
  - Assert thresholds list has entries for 0.3, 0.4, 0.5, 0.6, 0.7
  - Assert each entry has weighted_f1, weighted_precision, weighted_recall
- Print optimal thresholds for both

**test_predictions_parquets_exist()**
- For each path in [LGBM_PRED_PATH, XGB_PRED_PATH]:
  - Assert file exists
  - Load parquet
  - Check required columns: CONGLOME, VIVIENDA, HOGAR, CODPERSO, year, UBIGEO, FACTOR07, dropout, prob_dropout, pred_dropout, model, threshold, split
  - Assert prob_dropout in [0, 1]
  - Assert pred_dropout in {0, 1}
  - Assert row count near 52,112 (+/- 200)
- Check model column values: "lightgbm" for lgbm, "xgboost" for xgb
- Print row counts

**test_model_joblibs_exist()**
- For each path in [LGBM_MODEL_PATH, XGB_MODEL_PATH]:
  - Assert file exists
  - Load with joblib.load
  - Assert has predict_proba method
- Print: "Both models loaded, have predict_proba: True"

**test_pr_curve_figures_exist()**
- For each path in [LGBM_PR_PATH, XGB_PR_PATH]:
  - Assert file exists
  - Assert size > 10KB
- Print sizes

**Summary section at end of file:**

```python
if __name__ == "__main__":
    with open(RESULTS_PATH) as f:
        data = json.load(f)

    lgbm = data["lightgbm"]
    xgb = data["xgboost"]
    lgbm_prauc = lgbm["metrics"]["validate_2022"]["weighted"]["pr_auc"]
    xgb_prauc = xgb["metrics"]["validate_2022"]["weighted"]["pr_auc"]
    ratio = xgb_prauc / lgbm_prauc

    print("=== GATE TEST 2.2 SUMMARY ===")
    print(f"LightGBM val PR-AUC (weighted): {lgbm_prauc:.4f} > 0.2103 {'PASS' if lgbm_prauc > 0.2103 else 'FAIL'}")
    print(f"XGBoost  val PR-AUC (weighted): {xgb_prauc:.4f}")
    print(f"Algorithm-independence: ratio={ratio:.4f} {'PASS' if ratio >= 0.95 else 'FAIL'}")

    top5 = lgbm["feature_importances"][:5]
    print(f"\nTop-5 LightGBM features:")
    for i, f in enumerate(top5, 1):
        print(f"  {i}. {f['feature']:<40s} {f['importance']:.4f}")

    max_imp = lgbm["feature_importances"][0]["importance"]
    print(f"\nMax importance: {max_imp:.4f} < 0.50 {'PASS' if max_imp < 0.50 else 'FAIL'}")
    print(f"Optuna trials: LGBM={lgbm['metadata']['optuna_n_trials']}, XGB={xgb['metadata']['optuna_n_trials']}")
    print(f"Models persisted: model_lgbm.joblib, model_xgb.joblib")
    print(f"Predictions: predictions_lgbm.parquet, predictions_xgb.parquet")
    print(f"PR curves: pr_curve_lgbm.png, pr_curve_xgb.png")
```

**Run the gate test:**
```bash
uv run pytest tests/gates/test_gate_2_2.py -v -s
```
  </action>
  <verify>
Run gate test:
```bash
uv run pytest tests/gates/test_gate_2_2.py -v -s
```

Verify: all 11 tests pass, console output shows:
- LightGBM val PR-AUC > 0.2103
- XGBoost within 5% of LightGBM
- Max feature importance < 0.50
- Feature importances printed for human review
  </verify>
  <done>
- Gate test 2.2 has 11 test functions covering all success criteria
- All tests pass when run with pytest
- Console output prints feature importances and algorithm-independence check for human review
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete LightGBM + XGBoost training pipeline with:
- LightGBM Optuna-tuned with 100 trials, using scale_pos_weight for class imbalance and FACTOR07 survey weights
- XGBoost Optuna-tuned with 50 trials, same weighting strategy
- Feature importances extracted (gain-based, normalized to sum=1)
- model_results.json updated with lightgbm and xgboost entries (LR entry preserved)
- Predictions parquets for both models (val + test combined)
- Persisted models as joblib files
- PR curve PNGs for both models
- Gate test 2.2 passing all 11 assertions

Key items for human review:
- Top-10 LightGBM feature importances: age, poverty-related features, and rurality should be in top-5
- Algorithm-independence: XGBoost PR-AUC should be within 5% of LightGBM
- Both models beat LR baseline (0.2103)
  </what-built>
  <how-to-verify>
1. Review gate test output with feature importances:
   ```bash
   uv run pytest tests/gates/test_gate_2_2.py -v -s
   ```
   Check:
   - LightGBM val PR-AUC significantly above 0.2103
   - XGBoost val PR-AUC within 5% of LightGBM
   - Top-5 features include age, poverty-related features (poverty_quintile or poverty_index_z), and rurality (rural)
   - No single feature dominates (< 50%)

2. Review feature importance breakdown:
   ```bash
   uv run python -c "
   import json
   with open('data/exports/model_results.json') as f:
       d = json.load(f)
   lgbm = d['lightgbm']
   print('=== TOP-10 LIGHTGBM FEATURE IMPORTANCES ===')
   equity = {'poverty_quintile', 'rural', 'lang_other_indigenous', 'lang_quechua', 'lang_aimara', 'age', 'es_mujer', 'es_peruano'}
   for i, f in enumerate(lgbm['feature_importances'][:10], 1):
       mark = ' ***' if f['feature'] in equity else ''
       print(f\"  {i:2d}. {f['feature']:<40s} {f['importance']:.4f}{mark}\")
   print()
   print('=== TOP-5 XGBOOST FEATURE IMPORTANCES ===')
   for i, f in enumerate(d['xgboost']['feature_importances'][:5], 1):
       print(f\"  {i:2d}. {f['feature']:<40s} {f['importance']:.4f}\")
   "
   ```

3. Review PR curves:
   Open `data/exports/figures/pr_curve_lgbm.png` and `data/exports/figures/pr_curve_xgb.png`:
   - Curves should be well above the chance level line
   - Threshold markers visible
   - LightGBM curve should be above or near XGBoost curve

4. Rejection criteria:
   - REJECT if LightGBM val PR-AUC <= 0.2103 (does not beat LR baseline)
   - REJECT if age is NOT in top-10 features (unexpected model behavior)
   - REJECT if a single feature > 50% importance (model not using diverse signal)
   - ACCEPT if top-5 includes age and at least one poverty/rurality feature, even if exact ranking differs from expectations
  </how-to-verify>
  <resume-signal>Type "approved" if feature importances show age, poverty, and rurality in top-5 and algorithm-independence holds. Describe any issues if adjustments needed.</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. `src/models/lightgbm_xgboost.py` exists with run_lgbm_xgb_pipeline() that trains both models
2. LightGBM achieves val PR-AUC > 0.2103 (beats LR baseline)
3. XGBoost val PR-AUC within 5% of LightGBM (algorithm-independence)
4. No single feature > 50% normalized gain importance
5. `data/exports/model_results.json` has three keys: logistic_regression, lightgbm, xgboost -- each with metadata, metrics, threshold_analysis
6. lightgbm and xgboost entries additionally have feature_importances (normalized, sorted descending)
7. `data/processed/predictions_lgbm.parquet` and `predictions_xgb.parquet` each have ~52,112 rows
8. `data/processed/model_lgbm.joblib` and `model_xgb.joblib` load and have predict_proba
9. `data/exports/figures/pr_curve_lgbm.png` and `pr_curve_xgb.png` exist and are > 10KB
10. Gate test 2.2 passes: `uv run pytest tests/gates/test_gate_2_2.py -v`
11. Human reviews and approves feature importances
</verification>

<success_criteria>
- LightGBM val PR-AUC > 0.2103 (beats LR baseline)
- XGBoost val PR-AUC within 5% of LightGBM
- No single feature > 50% importance
- model_results.json, prediction parquets, model joblibs, PR curves all exist with correct content
- Gate test 2.2 passes all assertions
- Human reviews and approves feature importances (age, poverty, rurality in top-5)
</success_criteria>

<output>
After completion, create `.planning/phases/06-lightgbm-xgboost/06-01-SUMMARY.md`
</output>
