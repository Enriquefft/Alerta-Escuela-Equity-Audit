---
phase: 15-paper-expansion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - paper/references.bib
  - paper/main.tex
autonomous: true

must_haves:
  truths:
    - "Introduction frames 3 explicit research questions"
    - "Related Work has 4 subsections positioning paper against 10+ prior works"
    - "Bibliography contains 30+ entries"
  artifacts:
    - path: "paper/references.bib"
      provides: "Complete bibliography with 43 entries"
      contains: "crenshaw1989demarginalizing"
    - path: "paper/main.tex"
      provides: "Expanded Introduction and Related Work sections"
      contains: "\\subsection{Early Warning Systems in Education}"
  key_links:
    - from: "paper/main.tex"
      to: "paper/references.bib"
      via: "\\cite commands"
      pattern: "\\\\cite\\{(buolamwini|crenshaw|perdomo|baker2022)"
---

<objective>
Expand bibliography to 43 entries and rewrite Introduction + Related Work sections to full academic depth.

Purpose: The bibliography and front matter (Intro + Related Work) are foundational — every subsequent section cites these works. Doing them first ensures all citations are available and the paper's framing (RQs, positioning) is established before expanding analytical sections.

Output: Updated references.bib (43 entries), expanded Introduction (~2 pages with 3 RQs and contributions list), expanded Related Work (~3-4 pages with 4 subsections citing 10+ prior works).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-paper-expansion/15-CONTEXT.md
@.planning/phases/15-paper-expansion/15-RESEARCH.md
@paper/main.tex
@paper/references.bib
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add 25 new BibTeX entries to references.bib</name>
  <files>paper/references.bib</files>
  <action>
Add all 25 new BibTeX entries from 15-RESEARCH.md to paper/references.bib, organized under appropriate section comment headers. The entries to add are:

**EWS in Education (8):** bowers2010grades, knowles2015needles, lakkaraju2015machine, adelman2018predicting, baker2020predicting, perdomo2023difficult, lee2019machine, mcmahon2020reenvisioning

**Algorithmic Fairness in Education (7):** baker2022algorithmic, loukina2019many, mehrabi2021survey, pan2024examining, karimihag2021predicting, saleiro2018aequitas, gardner2024debiasing

**Fairness in Latin American Educational AI (4):** cueto2009explaining, cueto2016education, villegas2023supporting, zawacki2022artificial

**Intersectionality in ML Fairness (4):** crenshaw1989demarginalizing, buolamwini2018gender, kearns2018preventing, hebertjohnson2018multicalibration

**Technical (2):** niculescumizil2005predicting, macnell2023implementing

Copy each BibTeX entry exactly as provided in 15-RESEARCH.md. Maintain the existing section comment structure (% === headers) and add new section headers for new categories. Keep existing 18 entries unchanged.
  </action>
  <verify>Count entries in references.bib: grep -c '@' paper/references.bib should show 43 (18 existing + 25 new). Verify no duplicate keys.</verify>
  <done>references.bib contains all 43 BibTeX entries with no duplicates and valid BibTeX syntax.</done>
</task>

<task type="auto">
  <name>Task 2: Expand Introduction with 3 RQs, motivation, and contributions</name>
  <files>paper/main.tex</files>
  <action>
Replace the current 2-paragraph Introduction (lines 48-50) with a full academic introduction (~2 pages). Structure:

**Paragraph 1 — Opening hook:** Educational early warning systems are proliferating across Latin America. Peru's Alerta Escuela flags students at risk of dropping out using administrative data. Such systems promise efficiency but their fairness properties remain unaudited. Cite barocas2016big, mitchell2021algorithmic.

**Paragraph 2 — The gap:** Despite growing algorithmic fairness literature, few studies audit deployed educational prediction systems in developing countries. Most fairness work focuses on US/European contexts, race/gender dimensions, and does not use survey weights or intersectional analysis. Cite kizilcec2020algorithmic, baker2022algorithmic, gardner2024debiasing.

**Paragraph 3 — Peru context:** Peru is a multilingual country where indigenous-language speakers face persistent educational inequities. ENAHO provides nationally representative data spanning 150,135 student-year observations across 6 years. The Alerta Escuela system operates on administrative data we cannot access directly, so we construct a proxy replication using ENAHO survey data. Cite inei2023enaho, cueto2009explaining, minedu2023alerta.

**Paragraph 4 — Research questions:** Frame as: "This paper addresses three research questions:"
\begin{enumerate}
  \item \textbf{RQ1:} What disparities exist in dropout prediction accuracy across demographic groups (language, geography, poverty, sex)?
  \item \textbf{RQ2:} How does the model encode these disparities---through identity features directly or through structural proxies?
  \item \textbf{RQ3:} How do intersections of demographic dimensions (e.g., language $\times$ geography) amplify prediction errors beyond what single-axis analysis reveals?
\end{enumerate}

**Paragraph 5 — Approach summary:** Brief description of methodology: LightGBM model with Platt calibration, fairness evaluation across 7 dimensions and 3 intersections using fairlearn, SHAP interpretability analysis. Temporal train/val/test split. Survey-weighted metrics throughout. Cite ke2017lightgbm, bird2020fairlearn, lundberg2017unified.

**Paragraph 6 — Key findings preview:** Surveillance-invisibility axis (indigenous FNR=0.22/FPR=0.52 vs Spanish FNR=0.63/FPR=0.18). Urban indigenous students FNR=0.753. SHAP shows model predicts through spatial-structural features, not identity features directly. Algorithm-independence confirmed (LightGBM vs XGBoost ratio 1.0006).

**Paragraph 7 — Contributions list:** "Our contributions are:" (use itemize)
- One of few comprehensive fairness audits of an educational EWS in a developing country
- Demonstration of survey-weighted fairness evaluation methodology
- Evidence that intersectional analysis reveals disparities hidden by single-axis evaluation
- Open-source replicable audit framework (code, data pipeline, and analysis publicly available)

Cite buolamwini2018gender, crenshaw1989demarginalizing in the contributions about intersectional analysis.

**CRITICAL:** Maintain "Alerta Escuela-style model" proxy replication framing. Do NOT claim to audit Alerta Escuela directly. Use phrasing like "an Alerta Escuela-style dropout prediction model" or "a proxy replication of Peru's Alerta Escuela approach."
  </action>
  <verify>Compile paper: cd paper && latexmk -pdf main.tex (or pdflatex + bibtex cycle). Introduction should span ~2 pages. All \cite commands must resolve.</verify>
  <done>Introduction contains 3 numbered RQs, motivation paragraphs, contributions list, and all citations resolve.</done>
</task>

<task type="auto">
  <name>Task 3: Expand Related Work into 4 subsections</name>
  <files>paper/main.tex</files>
  <action>
Replace the current 2-paragraph Related Work (lines 56-58) with a full section containing 4 subsections (~3-4 pages total). Follow the narrative arcs from 15-RESEARCH.md exactly.

**\subsection{Early Warning Systems in Education}** (~1 page)
Arc: EWS evolved from simple indicator models (bowers2010grades) to ML systems (lakkaraju2015machine, knowles2015needles, baker2020predicting), deployed at scale in the US and developing countries (adelman2018predicting in Guatemala/Honduras). Lee (2019) established temporal train-test splits for EWS evaluation. McMahon et al. (2020) argue EWS should shift from identification to meaningful intervention. Recent critical work questions whether individual risk scores add value beyond structural predictors (perdomo2023difficult). Our paper audits an EWS-style model in Peru, where deployment occurs but fairness evaluation does not.

**\subsection{Algorithmic Fairness in Education}** (~1 page)
Arc: Open with Kizilcec and Lee (2020) as anchor — they identified the gap that fairness audits of deployed educational systems are rare. Baker and Hawn (2022) catalogued known biases and introduced slice analysis. Mehrabi et al. (2021) provide comprehensive taxonomy of fairness definitions. Specific studies examine fairness in automated scoring (loukina2019many), dropout prediction (pan2024examining in US context, karimihag2021predicting combining calibration and fairness), and propose debiasing strategies (gardner2024debiasing). Saleiro et al. (2018) provide operational audit frameworks (Aequitas). However, most focus on gender/race in US/European contexts. Our paper fills the gap with a developing-country, multilingual context using survey-weighted analysis across 7 dimensions.

**\subsection{Fairness in Latin American Educational AI}** (~0.5-1 page)
Arc: Latin American education faces structural inequality (unesco2022lac). In Peru, indigenous-language speakers face persistent achievement gaps — Cueto et al. (2009) documented language-based marginalization, and Cueto, Miranda, and Leon (2016) found only 37% of indigenous students attend bilingual schools. ML is increasingly applied to dropout prediction in the region (villegas2023supporting, adelman2018predicting), and AI in education is growing (zawacki2022artificial), but fairness audits remain absent. Our paper provides the first such audit for a Peruvian educational prediction system.

**\subsection{Intersectionality in ML Fairness}** (~0.5-1 page)
Arc: Crenshaw (1989) established that single-axis analysis misses compound disadvantage. Buolamwini and Gebru (2018) demonstrated this computationally with Gender Shades (darker-skinned females: 34.7% error vs lighter-skinned males: 0.8%). Kearns et al. (2018) formalized subgroup fairness, showing classifiers can appear fair on groups but not intersections. Hebert-Johnson et al. (2018) extended this to calibration (multicalibration). Our intersectional analysis (language x geography revealing urban indigenous FNR=0.753) directly operationalizes these frameworks in an educational context where no prior intersectional audit exists.

**For each cited work, explicitly state what they did NOT do that we do.** For example: "Pan and Zhang (2024) examined fairness in dropout prediction but in a US context without survey weights or intersectional analysis."

**CRITICAL:** Do NOT just list papers. Each paragraph should build a narrative that positions our work. Lead with empirical studies, use surveys as framing references. Maintain "Alerta Escuela-style model" proxy framing when referencing our work.
  </action>
  <verify>Compile paper: cd paper && latexmk -pdf main.tex. Related Work should have 4 visible subsection headings. Count unique \cite commands in Related Work — should reference at least 10 distinct works.</verify>
  <done>Related Work contains 4 subsections, cites 10+ distinct prior works with explicit positioning, and all citations resolve.</done>
</task>

</tasks>

<verification>
1. Paper compiles without errors: cd paper && pdflatex main.tex && bibtex main && pdflatex main.tex && pdflatex main.tex
2. Bibliography count: grep -c '@' paper/references.bib >= 43
3. No undefined citation warnings in LaTeX log
4. Introduction contains \begin{enumerate} with 3 RQ items
5. Related Work contains 4 \subsection commands
</verification>

<success_criteria>
- references.bib has 43 entries (18 existing + 25 new)
- Introduction spans ~2 pages with 3 numbered RQs, motivation, and contributions list
- Related Work spans ~3-4 pages with 4 subsections citing 10+ prior works
- Paper compiles cleanly with all citations resolved
- All numerical results and metric values unchanged
- Proxy replication framing maintained throughout
</success_criteria>

<output>
After completion, create `.planning/phases/15-paper-expansion/15-01-SUMMARY.md`
</output>
