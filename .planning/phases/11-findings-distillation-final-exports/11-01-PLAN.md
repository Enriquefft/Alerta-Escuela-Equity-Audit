# Plan 11-01: Findings Distillation + Final Export Validation + Gate Test 3.4

## Goal
Synthesize all analysis into 5-7 bilingual media-ready findings ordered by impact, validate all 7 JSON export files against M4 site contracts, create export README, and pass gate test 3.4.

## Context
- All 10 prior phases complete — 6 of 7 export files already exist in `data/exports/`
- Missing: `findings.json` (new) and `README.md` (new)
- Existing exports: fairness_metrics.json, shap_values.json, choropleth.json, model_results.json, descriptive_tables.json, onnx/lightgbm_dropout.onnx
- 11-CONTEXT.md defines: narrative arc ordering, bilingual tone, headline format, explanation depth, README structure

## Tasks

### Task 1: Create `src/fairness/findings.py` — Findings Distillation Script

**What:** Python script that loads all existing exports, assembles 5-7 findings with bilingual content, validates metric_source paths resolve, and writes `data/exports/findings.json`.

**Findings (ordered by narrative arc from 11-CONTEXT.md):**

1. **"The System Misses 6 in 10"** — FNR finding
   - stat: ~63% FNR overall (castellano baseline) vs ~23% for other_indigenous
   - metric_source: `fairness_metrics.json#dimensions.language.groups.castellano.fnr`
   - visualization_type: `bar_chart`
   - severity: `critical`
   - Headline pattern: "6 de cada 10 estudiantes en riesgo no son detectados por Alerta Escuela"

2. **"Indigenous Language = Surveillance Bias"** — FNR-FPR trade-off
   - stat: other_indigenous FPR=0.537 vs castellano FPR=0.160; but FNR=0.227 vs 0.639
   - metric_source: `fairness_metrics.json#dimensions.language.groups.other_indigenous.fpr`
   - visualization_type: `grouped_bar`
   - severity: `critical`
   - Headline: model over-flags indigenous students (surveillance) while under-detecting Spanish-speakers (invisibility)

3. **"Urban Indigenous Students Most Invisible"** — Intersection finding
   - stat: other_indigenous_urban FNR=0.753 (n=89)
   - metric_source: `fairness_metrics.json#intersections.language_x_rural.groups.other_indigenous_urban.fnr`
   - visualization_type: `heatmap`
   - severity: `high`
   - Headline: 3 of 4 urban indigenous students at risk are completely missed

4. **"The Model Sees Poverty, Not Identity"** — SHAP finding
   - stat: top-5 SHAP features are structural (age, nightlights, literacy, poverty), 0/5 overlap with LR identity features
   - metric_source: `shap_values.json#top_5_shap`
   - visualization_type: `bar_chart`
   - severity: `medium`
   - Headline: the model predicts through geography and economics, not who you are

5. **"Selva's False Negative Crisis"** — Regional FNR disparity finding
   - stat: Selva FNR (~0.59) — model misses nearly 6 in 10 at-risk students in the Amazon basin
   - metric_source: `fairness_metrics.json#dimensions.region.groups.selva.fnr`
   - visualization_type: `choropleth`
   - severity: `high`
   - Headline: Alerta Escuela misses nearly 6 of 10 at-risk students in the Selva region
   - Note: calibration_high_risk fields are null (calibrated probs max 0.431, never reach 0.7 threshold), so regional story is told through FNR instead

6. **"District Predictions Don't Match Reality"** — Cross-validation finding
   - stat: Pearson r = -0.071 between model predictions and admin dropout rates
   - metric_source: `choropleth.json#correlation.pearson_r`
   - visualization_type: `choropleth`
   - severity: `medium`
   - Headline: district-level predictions weakly correlate with official dropout statistics

7. **"Sex Equity Gap Is Minimal"** — Positive finding (contrast)
   - stat: FNR gap only 0.026 between sexes
   - metric_source: `fairness_metrics.json#dimensions.sex.gaps.max_fnr_gap`
   - visualization_type: `bar_chart`
   - severity: `low`
   - Headline: unlike language and geography, the model treats boys and girls nearly equally

**Implementation details:**
- Load each of the 5 existing JSON exports
- For each finding, resolve `metric_source` path to verify it exists and extract actual value
- Write bilingual headlines/explanations following 11-CONTEXT.md tone guidelines
- Spanish: journalistic-accessible, stat-forward headlines, Peruvian context natural
- English: same stat, adds geographic/institutional context for international audience
- Explanations: 2-3 sentences max per finding
- Output schema per finding: `{id, stat, headline_es, headline_en, explanation_es, explanation_en, metric_source: {path, label}, visualization_type, data_key, severity}`
- `data_key` maps to the export file key the M4 site should render for that finding
- **All stat values extracted at runtime from exports** — plan-text numbers are approximate guides; the script reads live values from JSON files, not hardcoded numbers
- Write to `data/exports/findings.json`

**File:** `src/fairness/findings.py` (~200 lines)

### Task 2: Create `data/exports/README.md` — Export Documentation

**What:** Developer-facing documentation for M4 site integration.

**Structure (from 11-CONTEXT.md):**
1. Overview table — 7 files mapped to site sections
2. Per-file sections (7 sections), each with:
   - Purpose (1 sentence)
   - Schema (field names, types, example values)
   - Which M4 site component consumes it
   - File size / record count
   - Data provenance: source, pipeline step, producing script

**Files documented:**
| File | Size | Producing Script | Phase |
|------|------|-----------------|-------|
| findings.json | ~15 KB | src/fairness/findings.py | 11 |
| fairness_metrics.json | 28 KB | src/fairness/metrics.py | 8 |
| shap_values.json | 29 KB | src/fairness/shap_analysis.py | 9 |
| choropleth.json | 402 KB | src/fairness/cross_validation.py | 10 |
| model_results.json | 30 KB | src/models/*.py | 5-7 |
| descriptive_tables.json | 195 KB | src/data/descriptive.py | 4 |
| onnx/lightgbm_dropout.onnx | 0.10 MB | src/models/calibration.py | 7 |

**File:** `data/exports/README.md` (~200 lines)

### Task 3: Create `tests/gates/test_gate_3_4.py` — Gate Test

**What:** Validates all Phase 11 success criteria.

**Tests (~12 assertions + 1 human review):**

1. `test_findings_json_exists_and_valid` — file exists, valid JSON
2. `test_findings_count` — 5-7 findings
3. `test_findings_required_fields` — each finding has all 10 required fields (id, stat, headline_es, headline_en, explanation_es, explanation_en, metric_source, visualization_type, data_key, severity)
4. `test_findings_ids_unique` — no duplicate finding IDs
5. `test_findings_severity_values` — severity in {critical, high, medium, low}
6. `test_metric_source_paths_resolve` — KEY TEST: for each finding, parse metric_source.path (format: `filename.json#json.path`), load the referenced file, navigate the path, assert value exists and is non-null
7. `test_all_seven_exports_present` — verify all 7 files exist in data/exports/
8. `test_readme_exists` — data/exports/README.md exists
9. `test_readme_documents_all_files` — README mentions all 7 export filenames
10. `test_headlines_not_empty` — no empty headline strings
11. `test_explanations_length` — explanations are 2-3 sentences (50-500 chars)
12. `test_human_review_findings` — prints all headlines (es + en), stats, and severity for human review; always passes

**Fixtures:**
- `findings_data` (scope=module) — loads findings.json
- `all_exports` (scope=module) — dict mapping filename to loaded JSON for path resolution

**Pattern:** follows test_gate_3_3.py conventions exactly (docstring, imports, sys.path, find_project_root, assertion style)

**File:** `tests/gates/test_gate_3_4.py` (~250 lines)

## Execution Order

1. Task 3 first (gate test) — defines acceptance criteria
2. Task 1 (findings.py) — produces findings.json
3. Task 2 (README.md) — documents all exports
4. Run gate test: `uv run pytest tests/gates/test_gate_3_4.py -v -s`
5. Fix any failures, iterate
6. Atomic commit

## Commit Message
```
feat(11-01): findings distillation + final exports + gate test 3.4

- 7 bilingual media-ready findings ordered by narrative impact
- All metric_source paths validated against export files
- Export README for M4 site developers
- Gate test 3.4: 12 assertions + human review
```

## Success Criteria Verification
| Criterion | Verified By |
|-----------|------------|
| findings.json has 5-7 findings with all required fields | test_findings_count, test_findings_required_fields |
| Every metric_source path resolves | test_metric_source_paths_resolve |
| All 7 exports present | test_all_seven_exports_present |
| README documents each file | test_readme_exists, test_readme_documents_all_files |
| Gate test 3.4 passes + headlines printed | test_human_review_findings |
